<!DOCTYPE html>
<!-- saved from url=(0074)https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://openreview.net/favicon.ico"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><script async="" src="./ICLR2022_poster_files/js"></script><script>window.dataLayer = window.dataLayer || [];
function gtag() { dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-108703919-1', {
  page_path: window.location.pathname + window.location.search,
  transport_type: 'beacon'
});</script><title>ICLR 2022 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta property="og:title" content="ICLR 2022 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta name="next-head-count" content="14"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preload" href="./ICLR2022_poster_files/ed513f7ce02040ba.css" as="style"><link rel="stylesheet" href="./ICLR2022_poster_files/ed513f7ce02040ba.css" data-n-g=""><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./ICLR2022_poster_files/polyfills-5cd94c89d3acac5f.js.download"></script><script src="./ICLR2022_poster_files/webpack-363f7b452897525e.js.download" defer=""></script><script src="./ICLR2022_poster_files/framework-79bce4a3a540b080.js.download" defer=""></script><script src="./ICLR2022_poster_files/main-aaf30a81beffaaa5.js.download" defer=""></script><script src="./ICLR2022_poster_files/_app-dd140e4fb7437f25.js.download" defer=""></script><script src="./ICLR2022_poster_files/6253-86ab2843b0544962.js.download" defer=""></script><script src="./ICLR2022_poster_files/group-18de1916b352abe5.js.download" defer=""></script><script src="./ICLR2022_poster_files/_buildManifest.js.download" defer=""></script><script src="./ICLR2022_poster_files/_ssgManifest.js.download" defer=""></script><script src="./ICLR2022_poster_files/_middlewareManifest.js.download" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap">@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4DRG.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZtyH.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNb4Q.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFlYA.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARPQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARGQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARDQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4AROQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARBQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARNQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARMQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARCQ_mu72Bi.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyOzW1IPriezag.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyHzW1IPriezag.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyCzW1IPriezag.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyPzW1IPriezag.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyAzW1IPriezag.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyMzW1IPriezag.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyNzW1IPriezag.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyDzW1IPrie.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr4TRASf6M7VBj.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6TRASf6M7VBj.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6zRASf6M7VBj.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5TRASf6M7Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVadyBx2pqPIif.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVYNyBx2pqPIif.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZdyBx2pqPIif.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVaNyBx2pqPIif.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZ9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVa9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVatyBx2pqPIif.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZNyBx2pqPA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><script src="./ICLR2022_poster_files/tex-chtml-full.js.download" async="" crossorigin="anonymous"></script><link as="script" rel="prefetch" href="./ICLR2022_poster_files/login-6d19e702f9bd1dcc.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/privacy-276f217f8a2a3840.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/terms-e2f16e0dce69665f.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/sponsors-987fb223230996d5.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/contact-2c775b351d0a5421.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/venues-458c67c21955226a.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/about-77a61c48f23a1315.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_files/index-55b7d6149a41ddec.js.download"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-stretchy-v.mjx-c28 mjx-beg mjx-c::before {
  content: "\239B";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c28 mjx-ext mjx-c::before {
  content: "\239C";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c28 mjx-end mjx-c::before {
  content: "\239D";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c28 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c28 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-stretchy-v.mjx-c29 mjx-beg mjx-c::before {
  content: "\239E";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c29 mjx-ext mjx-c::before {
  content: "\239F";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c29 mjx-end mjx-c::before {
  content: "\23A0";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c29 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c29 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c1D44C.TEX-I::before {
  padding: 0.683em 0.763em 0 0;
  content: "Y";
}

mjx-c.mjx-c1D44B.TEX-I::before {
  padding: 0.683em 0.852em 0 0;
  content: "X";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c2218::before {
  padding: 0.444em 0.5em 0 0;
  content: "\2218";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D435.TEX-I::before {
  padding: 0.683em 0.759em 0 0;
  content: "B";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c1D70C.TEX-I::before {
  padding: 0.442em 0.517em 0.216em 0;
  content: "\3C1";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c5E::before {
  padding: 0.694em 0.5em 0 0;
  content: "^";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c2225::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D41B.TEX-B::before {
  padding: 0.694em 0.639em 0.006em 0;
  content: "b";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c1D714.TEX-I::before {
  padding: 0.443em 0.622em 0.011em 0;
  content: "\3C9";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c7E::before {
  padding: 0.318em 0.5em 0 0;
  content: "~";
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c1D45E.TEX-I::before {
  padding: 0.442em 0.46em 0.194em 0;
  content: "q";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c3F.TEX-MI::before {
  padding: 0.716em 0.551em 0 0;
  content: "?";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c398::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "\398";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c1D715::before {
  padding: 0.715em 0.566em 0.022em 0;
  content: "\2202";
}

mjx-c.mjx-c2F.TEX-S1::before {
  padding: 0.85em 0.578em 0.349em 0;
  content: "/";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-c2229::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2229";
}

mjx-c.mjx-c41::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c4E::before {
  padding: 0.683em 0.75em 0 0;
  content: "N";
}

mjx-c.mjx-c44::before {
  padding: 0.683em 0.764em 0 0;
  content: "D";
}

mjx-c.mjx-c49::before {
  padding: 0.683em 0.361em 0 0;
  content: "I";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c2DC.TEX-S1::before {
  padding: 0.722em 0.556em 0 0;
  content: "\2DC";
}

mjx-c.mjx-c28.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: "(";
}

mjx-c.mjx-c29.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: ")";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c53::before {
  padding: 0.705em 0.556em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c45::before {
  padding: 0.68em 0.681em 0 0;
  content: "E";
}

mjx-c.mjx-c1D499.TEX-BI::before {
  padding: 0.452em 0.659em 0.008em 0;
  content: "x";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c22C6::before {
  padding: 0.486em 0.5em 0 0;
  content: "\22C6";
}
</style></head><body data-new-gr-c-s-check-loaded="14.1068.0" data-gr-ext-installed=""><div id="__next" data-reactroot=""><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off" autocorrect="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login?redirect=%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%23poster-submissions&amp;noprompt=true">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2022"><img class="icon" src="./ICLR2022_poster_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2022</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>The Tenth International Conference on Learning Representations </h1>
<h3>ICLR 2022</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Virtual
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> Apr 25 2022
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/Conferences/2022" title="The Tenth International Conference on Learning Representations  Homepage" target="_blank">https://iclr.cc/Conferences/2022</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2022pc@gmail.com" target="_blank">iclr2022pc@gmail.com</a>
      </span>
  </h4>

<div class="description">
    <p class="no-margin">Please see the venue website for more information.</p>
  <p>Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0</p>
</div>
</div><div id="invitation" style="display: block;"></div><div id="notes">
<div class="tabs-container " style=""><div class="mobile-full-width">
  <ul class="nav nav-tabs" role="tablist">
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
          Your Consoles
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oral-submissions" aria-controls="oral-submissions" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true" aria-expanded="false">
          Oral Presentations
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#spotlight-submissions" aria-controls="spotlight-submissions" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true" aria-expanded="false">
          Spotlight Presentations
        </a>
      </li>
      <li role="presentation" class="active">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions" aria-controls="poster-submissions" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true" aria-expanded="true">
          Poster Presentations
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#submitted-submissions" aria-controls="submitted-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
          Rejected Submissions
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#desk-rejected-withdrawn-submissions" aria-controls="desk-rejected-withdrawn-submissions" role="tab" data-toggle="tab" data-tab-index="5" data-modify-history="true">
          Desk Rejected/Withdrawn Submissions
        </a>
      </li>
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#recent-activity" aria-controls="recent-activity" role="tab" data-toggle="tab" data-tab-index="6" data-modify-history="true">
          Recent Activity
        </a>
      </li>
  </ul>
</div>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade" id="oral-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="FPCMqjI0jXN" data-number="4597">
        <h4>
          <a href="https://openreview.net/forum?id=FPCMqjI0jXN">
              Domino: Discovering Systematic Errors with Cross-Modal Embeddings
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FPCMqjI0jXN" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sabri_Eyuboglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sabri_Eyuboglu1">Sabri Eyuboglu</a>, <a href="https://openreview.net/profile?id=~Maya_Varma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maya_Varma1">Maya Varma</a>, <a href="https://openreview.net/profile?id=~Khaled_Kamal_Saab1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Khaled_Kamal_Saab1">Khaled Kamal Saab</a>, <a href="https://openreview.net/profile?id=~Jean-Benoit_Delbrouck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jean-Benoit_Delbrouck1">Jean-Benoit Delbrouck</a>, <a href="https://openreview.net/profile?id=~Christopher_Lee-Messer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Lee-Messer1">Christopher Lee-Messer</a>, <a href="https://openreview.net/profile?id=~Jared_Dunnmon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jared_Dunnmon1">Jared Dunnmon</a>, <a href="https://openreview.net/profile?id=~James_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Zou1">James Zou</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Re1">Christopher Re</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FPCMqjI0jXN-details-599" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FPCMqjI0jXN-details-599"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robustness, subgroup analysis, error analysis, multimodal, slice discovery</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data).
        Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NudBMY-tzDr" data-number="4559">
        <h4>
          <a href="https://openreview.net/forum?id=NudBMY-tzDr">
              Natural Language Descriptions of Deep Visual Features
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NudBMY-tzDr" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Evan_Hernandez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Evan_Hernandez1">Evan Hernandez</a>, <a href="https://openreview.net/profile?id=~Sarah_Schwettmann2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sarah_Schwettmann2">Sarah Schwettmann</a>, <a href="https://openreview.net/profile?id=~David_Bau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Bau1">David Bau</a>, <a href="https://openreview.net/profile?id=~Teona_Bagashvili1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Teona_Bagashvili1">Teona Bagashvili</a>, <a href="https://openreview.net/profile?id=~Antonio_Torralba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Antonio_Torralba1">Antonio Torralba</a>, <a href="https://openreview.net/profile?id=~Jacob_Andreas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_Andreas1">Jacob Andreas</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NudBMY-tzDr-details-165" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NudBMY-tzDr-details-165"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="tYRrOdSnVUy" data-number="4491">
        <h4>
          <a href="https://openreview.net/forum?id=tYRrOdSnVUy">
              Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=tYRrOdSnVUy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lixu_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lixu_Wang1">Lixu Wang</a>, <a href="https://openreview.net/profile?id=~Shichao_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shichao_Xu1">Shichao Xu</a>, <a href="https://openreview.net/profile?id=~Ruiqi_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruiqi_Xu1">Ruiqi Xu</a>, <a href="https://openreview.net/profile?id=~Xiao_Wang11" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiao_Wang11">Xiao Wang</a>, <a href="https://openreview.net/profile?id=~Qi_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qi_Zhu2">Qi Zhu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#tYRrOdSnVUy-details-313" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tYRrOdSnVUy-details-313"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 &amp; STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel Non-Transferable Learning (NTL) method to restrict the model generalization ability to certain domains for model ownership verification and applicability authorization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YWNAX0caEjI" data-number="4412">
        <h4>
          <a href="https://openreview.net/forum?id=YWNAX0caEjI">
              Neural Structured Prediction for Inductive Node Classification
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YWNAX0caEjI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Meng_Qu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Meng_Qu2">Meng Qu</a>, <a href="https://openreview.net/profile?id=~Huiyu_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huiyu_Cai1">Huiyu Cai</a>, <a href="https://openreview.net/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jian_Tang1">Jian Tang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YWNAX0caEjI-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YWNAX0caEjI-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uxgg9o7bI_3" data-number="4320">
        <h4>
          <a href="https://openreview.net/forum?id=uxgg9o7bI_3">
              A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uxgg9o7bI_3" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Asiri_Wijesinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Asiri_Wijesinghe1">Asiri Wijesinghe</a>, <a href="https://openreview.net/profile?id=~Qing_Wang14" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qing_Wang14">Qing Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uxgg9o7bI_3-details-567" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uxgg9o7bI_3-details-567"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=uxgg9o7bI_3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LdlwbBP2mlq" data-number="4253">
        <h4>
          <a href="https://openreview.net/forum?id=LdlwbBP2mlq">
              Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LdlwbBP2mlq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chulhee_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chulhee_Yun1">Chulhee Yun</a>, <a href="https://openreview.net/profile?id=~Shashank_Rajput1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shashank_Rajput1">Shashank Rajput</a>, <a href="https://openreview.net/profile?id=~Suvrit_Sra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Suvrit_Sra1">Suvrit Sra</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LdlwbBP2mlq-details-526" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LdlwbBP2mlq-details-526"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-Åojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide tight upper and lower bounds on convergence rates of shuffling-based minibatch SGD and local SGD, and propose an algorithmic modification that improves convergence rates beyond our lower bounds.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LdlwbBP2mlq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Z7Lk2cQEG8a" data-number="4247">
        <h4>
          <a href="https://openreview.net/forum?id=Z7Lk2cQEG8a">
              The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Z7Lk2cQEG8a" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yifei_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yifei_Wang2">Yifei Wang</a>, <a href="https://openreview.net/profile?id=~Jonathan_Lacotte1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Lacotte1">Jonathan Lacotte</a>, <a href="https://openreview.net/profile?id=~Mert_Pilanci3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mert_Pilanci3">Mert Pilanci</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Z7Lk2cQEG8a-details-557" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Z7Lk2cQEG8a-details-557"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural networks, global optimization, convex optimization, convex analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys.
        Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Z7Lk2cQEG8a&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RQLLzMCefQu" data-number="4212">
        <h4>
          <a href="https://openreview.net/forum?id=RQLLzMCefQu">
              Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RQLLzMCefQu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yonathan_Efroni2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonathan_Efroni2">Yonathan Efroni</a>, <a href="https://openreview.net/profile?id=~Dipendra_Misra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dipendra_Misra1">Dipendra Misra</a>, <a href="https://openreview.net/profile?id=~Akshay_Krishnamurthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Akshay_Krishnamurthy1">Akshay Krishnamurthy</a>, <a href="https://openreview.net/profile?id=~Alekh_Agarwal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alekh_Agarwal2">Alekh Agarwal</a>, <a href="https://openreview.net/profile?id=~John_Langford1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~John_Langford1">John Langford</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RQLLzMCefQu-details-107" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RQLLzMCefQu-details-107"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=RQLLzMCefQu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="b-ny3x071E5" data-number="4127">
        <h4>
          <a href="https://openreview.net/forum?id=b-ny3x071E5">
              Bootstrapped Meta-Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=b-ny3x071E5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sebastian_Flennerhag1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sebastian_Flennerhag1">Sebastian Flennerhag</a>, <a href="https://openreview.net/profile?id=~Yannick_Schroecker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yannick_Schroecker1">Yannick Schroecker</a>, <a href="https://openreview.net/profile?id=~Tom_Zahavy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tom_Zahavy2">Tom Zahavy</a>, <a href="https://openreview.net/profile?id=~Hado_van_Hasselt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hado_van_Hasselt1">Hado van Hasselt</a>, <a href="https://openreview.net/profile?id=~David_Silver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Silver1">David Silver</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satinder_Singh2">Satinder Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#b-ny3x071E5-details-285" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b-ny3x071E5-details-285"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-learning, meta-gradients, meta-reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an algorithm for meta-learning with gradients that bootstraps the meta-learner from itself or another update rule. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XzTtHjgPDsT" data-number="4032">
        <h4>
          <a href="https://openreview.net/forum?id=XzTtHjgPDsT">
              Coordination Among Neural Modules Through a Shared Global Workspace
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XzTtHjgPDsT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Aniket_Rajiv_Didolkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aniket_Rajiv_Didolkar1">Aniket Rajiv Didolkar</a>, <a href="https://openreview.net/profile?id=~Alex_Lamb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Lamb1">Alex Lamb</a>, <a href="https://openreview.net/profile?id=~Kartikeya_Badola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kartikeya_Badola1">Kartikeya Badola</a>, <a href="https://openreview.net/profile?id=~Nan_Rosemary_Ke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Rosemary_Ke1">Nan Rosemary Ke</a>, <a href="https://openreview.net/profile?id=~Nasim_Rahaman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nasim_Rahaman1">Nasim Rahaman</a>, <a href="https://openreview.net/profile?id=~Jonathan_Binas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Binas1">Jonathan Binas</a>, <a href="https://openreview.net/profile?id=~Charles_Blundell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Charles_Blundell1">Charles Blundell</a>, <a href="https://openreview.net/profile?id=~Michael_Curtis_Mozer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Curtis_Mozer1">Michael Curtis Mozer</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yoshua_Bengio1">Yoshua Bengio</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XzTtHjgPDsT-details-534" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XzTtHjgPDsT-details-534"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">slot based recurrent architectures, attention, transformers, latent bottleneck.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">communication among different specialist using a shared workspace allowing higher order interactions </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XzTtHjgPDsT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="l4IHywGq6a" data-number="3972">
        <h4>
          <a href="https://openreview.net/forum?id=l4IHywGq6a">
              Data-Efficient Graph Grammar Learning for Molecular Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=l4IHywGq6a" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minghao_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghao_Guo1">Minghao Guo</a>, <a href="https://openreview.net/profile?id=~Veronika_Thost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Veronika_Thost1">Veronika Thost</a>, <a href="https://openreview.net/profile?id=~Beichen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Beichen_Li1">Beichen Li</a>, <a href="https://openreview.net/profile?id=~Payel_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payel_Das1">Payel Das</a>, <a href="https://openreview.net/profile?id=~Jie_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jie_Chen1">Jie Chen</a>, <a href="https://openreview.net/profile?id=~Wojciech_Matusik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wojciech_Matusik2">Wojciech Matusik</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#l4IHywGq6a-details-560" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l4IHywGq6a-details-560"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">molecular generation, graph grammar, data efficient generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mo>âˆ¼</mo></mrow><mn>20</mn></math></mjx-assistive-mml></mjx-container> samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>o</mi><mi>n</mi><mi>l</mi><mi>y</mi></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>117</mn></math></mjx-assistive-mml></mjx-container> training samples and is competitive against existing methods using <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>81</mn></math></mjx-assistive-mml></mjx-container>k data points.
        </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="iC4UHbQ01Mp" data-number="3949">
        <h4>
          <a href="https://openreview.net/forum?id=iC4UHbQ01Mp">
              Poisoning and Backdooring Contrastive Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=iC4UHbQ01Mp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nicholas_Carlini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Carlini1">Nicholas Carlini</a>, <a href="https://openreview.net/profile?id=~Andreas_Terzis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Terzis1">Andreas Terzis</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#iC4UHbQ01Mp-details-322" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iC4UHbQ01Mp-details-322"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Contrastive Learning, Poisoning attack, Backdoor attack, CLIP</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We argue poisoning and backdooring attacks are a serious threat to multimodal contrastive classifiers, because they are explicitly designed to be trained on uncurated datasets from the Internet.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="w1UbdvWH_R3" data-number="3870">
        <h4>
          <a href="https://openreview.net/forum?id=w1UbdvWH_R3">
              Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path
          </a>
        
          
            <a href="https://openreview.net/pdf?id=w1UbdvWH_R3" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~X.Y._Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~X.Y._Han1">X.Y. Han</a>, <a href="https://openreview.net/profile?id=~Vardan_Papyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vardan_Papyan1">Vardan Papyan</a>, <a href="https://openreview.net/profile?id=~David_L._Donoho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_L._Donoho1">David L. Donoho</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#w1UbdvWH_R3-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w1UbdvWH_R3-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Neural Collapse occurs empirically on deep nets trained with MSE loss and studying this setting leads to insightful closed-form dynamics.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ltM1RMZntpu" data-number="3861">
        <h4>
          <a href="https://openreview.net/forum?id=ltM1RMZntpu">
              Weighted Training for Cross-Task Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ltM1RMZntpu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shuxiao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shuxiao_Chen1">Shuxiao Chen</a>, <a href="https://openreview.net/profile?id=~Koby_Crammer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Koby_Crammer1">Koby Crammer</a>, <a href="https://openreview.net/profile?id=~Hangfeng_He3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hangfeng_He3">Hangfeng He</a>, <a href="https://openreview.net/profile?id=~Dan_Roth3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Roth3">Dan Roth</a>, <a href="https://openreview.net/profile?id=~Weijie_J_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weijie_J_Su1">Weijie J Su</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 01 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ltM1RMZntpu-details-855" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ltM1RMZntpu-details-855"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Cross-task learning, Natural language processing, Representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ltM1RMZntpu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wRODLDHaAiW" data-number="3763">
        <h4>
          <a href="https://openreview.net/forum?id=wRODLDHaAiW">
              iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wRODLDHaAiW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marine_Schimel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marine_Schimel1">Marine Schimel</a>, <a href="https://openreview.net/profile?id=~Ta-Chu_Kao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ta-Chu_Kao1">Ta-Chu Kao</a>, <a href="https://openreview.net/profile?id=~Kristopher_T_Jensen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kristopher_T_Jensen1">Kristopher T Jensen</a>, <a href="https://openreview.net/profile?id=~Guillaume_Hennequin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guillaume_Hennequin1">Guillaume Hennequin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wRODLDHaAiW-details-90" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wRODLDHaAiW-details-90"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuroscience, latent variable models, RNN, VAE, motor control, control theory, dynamical systems</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a novel autoencoder that uses iLQR as an inference model and apply it to synthetic data as well as neural recordings from primate motor cortex.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wRODLDHaAiW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="z7p2V6KROOV" data-number="3653">
        <h4>
          <a href="https://openreview.net/forum?id=z7p2V6KROOV">
              Extending the WILDS Benchmark for Unsupervised Adaptation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=z7p2V6KROOV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shiori_Sagawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shiori_Sagawa1">Shiori Sagawa</a>, <a href="https://openreview.net/profile?id=~Pang_Wei_Koh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pang_Wei_Koh1">Pang Wei Koh</a>, <a href="https://openreview.net/profile?email=tonyhlee%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="tonyhlee@stanford.edu">Tony Lee</a>, <a href="https://openreview.net/profile?id=~Irena_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Irena_Gao1">Irena Gao</a>, <a href="https://openreview.net/profile?id=~Sang_Michael_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sang_Michael_Xie1">Sang Michael Xie</a>, <a href="https://openreview.net/profile?id=~Kendrick_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kendrick_Shen1">Kendrick Shen</a>, <a href="https://openreview.net/profile?id=~Ananya_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ananya_Kumar1">Ananya Kumar</a>, <a href="https://openreview.net/profile?id=~Weihua_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weihua_Hu1">Weihua Hu</a>, <a href="https://openreview.net/profile?id=~Michihiro_Yasunaga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michihiro_Yasunaga1">Michihiro Yasunaga</a>, <a href="https://openreview.net/profile?id=~Henrik_Marklund2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Henrik_Marklund2">Henrik Marklund</a>, <a href="https://openreview.net/profile?id=~Sara_Beery1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sara_Beery1">Sara Beery</a>, <a href="https://openreview.net/profile?email=etienne.david%40inrae.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="etienne.david@inrae.fr">Etienne David</a>, <a href="https://openreview.net/profile?id=~Ian_Stavness1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ian_Stavness1">Ian Stavness</a>, <a href="https://openreview.net/profile?email=guowei%40g.ecc.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="guowei@g.ecc.u-tokyo.ac.jp">Wei Guo</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jure_Leskovec1">Jure Leskovec</a>, <a href="https://openreview.net/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kate_Saenko1">Kate Saenko</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chelsea_Finn1">Chelsea Finn</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Percy_Liang1">Percy Liang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#z7p2V6KROOV-details-485" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="z7p2V6KROOV-details-485"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">distribution shifts, adaptation, unlabeled data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce U-WILDS, which augments the WILDS distribution shift benchmark with realistic unlabeled data, and benchmark existing methods for unlabeled data on these in-the-wild distribution shifts.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="avgclFZ221l" data-number="3526">
        <h4>
          <a href="https://openreview.net/forum?id=avgclFZ221l">
              Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=avgclFZ221l" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~S_Chandra_Mouli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~S_Chandra_Mouli1">S Chandra Mouli</a>, <a href="https://openreview.net/profile?id=~Bruno_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bruno_Ribeiro1">Bruno Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#avgclFZ221l-details-292" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="avgclFZ221l-details-292"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="4" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44C TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi></math></mjx-assistive-mml></mjx-container> from input <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="5" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container> when the change in environment is due a (stochastic) input transformation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="6" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.056em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c></mjx-mtext></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2218"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.051em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>T</mi><mtext>te</mtext></msup><mo>âˆ˜</mo><msup><mi>X</mi><mo data-mjx-alternate="1">â€²</mo></msup></math></mjx-assistive-mml></mjx-container> not observed in training, as in training we observe <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="7" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.056em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c72"></mjx-c></mjx-mtext></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2218"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.051em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>T</mi><mtext>tr</mtext></msup><mo>âˆ˜</mo><msup><mi>X</mi><mo data-mjx-alternate="1">â€²</mo></msup></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="8" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.051em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>X</mi><mo data-mjx-alternate="1">â€²</mo></msup></math></mjx-assistive-mml></mjx-container> is a hidden variable. This work argues that when the transformations in train <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="9" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.056em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c72"></mjx-c></mjx-mtext></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>T</mi><mtext>tr</mtext></msup></math></mjx-assistive-mml></mjx-container> and test <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="10" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.056em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c></mjx-mtext></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>T</mi><mtext>te</mtext></msup></math></mjx-assistive-mml></mjx-container> are (arbitrary) symmetry transformations induced by a collection of known <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="11" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="12" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44C TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi></math></mjx-assistive-mml></mjx-container> in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Counterfactual-invariant representations for symmetry transformations</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KB5onONJIAU" data-number="3521">
        <h4>
          <a href="https://openreview.net/forum?id=KB5onONJIAU">
              Comparing Distributions by Measuring Differences that Affect Decision Making
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KB5onONJIAU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shengjia_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shengjia_Zhao1">Shengjia Zhao</a>, <a href="https://openreview.net/profile?id=~Abhishek_Sinha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Abhishek_Sinha1">Abhishek Sinha</a>, <a href="https://openreview.net/profile?id=~Yutong_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yutong_He1">Yutong He</a>, <a href="https://openreview.net/profile?id=~Aidan_Perreault1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aidan_Perreault1">Aidan Perreault</a>, <a href="https://openreview.net/profile?id=~Jiaming_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiaming_Song1">Jiaming Song</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefano_Ermon1">Stefano Ermon</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KB5onONJIAU-details-93" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KB5onONJIAU-details-93"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">probability divergence, two sample test, generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UseMOjWENv" data-number="3403">
        <h4>
          <a href="https://openreview.net/forum?id=UseMOjWENv">
              MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UseMOjWENv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yusong_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yusong_Wu1">Yusong Wu</a>, <a href="https://openreview.net/profile?id=~Ethan_Manilow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ethan_Manilow1">Ethan Manilow</a>, <a href="https://openreview.net/profile?id=~Yi_Deng4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Deng4">Yi Deng</a>, <a href="https://openreview.net/profile?email=rigeljs%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="rigeljs@google.com">Rigel Swavely</a>, <a href="https://openreview.net/profile?id=~Kyle_Kastner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyle_Kastner1">Kyle Kastner</a>, <a href="https://openreview.net/profile?id=~Tim_Cooijmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tim_Cooijmans1">Tim Cooijmans</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aaron_Courville3">Aaron Courville</a>, <a href="https://openreview.net/profile?id=~Cheng-Zhi_Anna_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng-Zhi_Anna_Huang1">Cheng-Zhi Anna Huang</a>, <a href="https://openreview.net/profile?id=~Jesse_Engel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jesse_Engel1">Jesse Engel</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UseMOjWENv-details-820" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UseMOjWENv-details-820"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Audio Synthesis, Generative Model, Hierarchical, DDSP, Music, Audio, Structured Models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance,  and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Controlling musical performance and synthesis with a structured hierarchical generative model</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="N0n_QyQ5lBF" data-number="3278">
        <h4>
          <a href="https://openreview.net/forum?id=N0n_QyQ5lBF">
              Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=N0n_QyQ5lBF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Bo_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Wan1">Bo Wan</a>, <a href="https://openreview.net/profile?id=~Wenjuan_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wenjuan_Han1">Wenjuan Han</a>, <a href="https://openreview.net/profile?id=~Zilong_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zilong_Zheng1">Zilong Zheng</a>, <a href="https://openreview.net/profile?id=~Tinne_Tuytelaars1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tinne_Tuytelaars1">Tinne Tuytelaars</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#N0n_QyQ5lBF-details-828" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N0n_QyQ5lBF-details-828"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Grammar Induction, Vision-Language Matching, Unsupervised Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new unsupervised vision-language grammar induction task to explore the multimodal information and induce a shared hierarchical structure for both image and language simultaneously.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=N0n_QyQ5lBF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EhYjZy6e1gJ" data-number="3113">
        <h4>
          <a href="https://openreview.net/forum?id=EhYjZy6e1gJ">
              PiCO: Contrastive Label Disambiguation for Partial Label Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EhYjZy6e1gJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Haobo_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Haobo_Wang1">Haobo Wang</a>, <a href="https://openreview.net/profile?id=~Ruixuan_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruixuan_Xiao1">Ruixuan Xiao</a>, <a href="https://openreview.net/profile?id=~Yixuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yixuan_Li1">Yixuan Li</a>, <a href="https://openreview.net/profile?id=~Lei_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lei_Feng1">Lei Feng</a>, <a href="https://openreview.net/profile?id=~Gang_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Niu1">Gang Niu</a>, <a href="https://openreview.net/profile?id=~Gang_Chen6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Chen6">Gang Chen</a>, <a href="https://openreview.net/profile?id=~Junbo_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junbo_Zhao1">Junbo Zhao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EhYjZy6e1gJ-details-846" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EhYjZy6e1gJ-details-846"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Partial Label Learning, Contrastive Learning, Prototype-based Disambiguation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A synergistic PLL framework that leverages contrastive learning for enhanced representation and improved label disambiguation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=EhYjZy6e1gJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0EXmFzUn5I" data-number="3064">
        <h4>
          <a href="https://openreview.net/forum?id=0EXmFzUn5I">
              Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0EXmFzUn5I" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shizhan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shizhan_Liu1">Shizhan Liu</a>, <a href="https://openreview.net/profile?id=~Hang_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hang_Yu1">Hang Yu</a>, <a href="https://openreview.net/profile?id=~Cong_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cong_Liao1">Cong Liao</a>, <a href="https://openreview.net/profile?id=~Jianguo_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianguo_Li2">Jianguo Li</a>, <a href="https://openreview.net/profile?id=~Weiyao_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weiyao_Lin1">Weiyao Lin</a>, <a href="https://openreview.net/profile?id=~Alex_X._Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_X._Liu1">Alex X. Liu</a>, <a href="https://openreview.net/profile?email=dustdar%40dsg.tuwien.ac.at" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="dustdar@dsg.tuwien.ac.at">Schahram Dustdar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0EXmFzUn5I-details-18" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0EXmFzUn5I-details-18"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>) with regard to the sequence length <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="14" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>, while its time and space complexity scale linearly with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="15" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a multiresolution pyramidal attention mechanism for long-range dependence modeling and time series forecasting, successfully reducing the maximum length of the signal traversing path to O(1) while achieving linear time and space complexity</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wIzUeM3TAU" data-number="2877">
        <h4>
          <a href="https://openreview.net/forum?id=wIzUeM3TAU">
              Expressiveness and Approximation Properties of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wIzUeM3TAU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Floris_Geerts1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Floris_Geerts1">Floris Geerts</a>, <a href="https://openreview.net/profile?id=~Juan_L_Reutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_L_Reutter1">Juan L Reutter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">29 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wIzUeM3TAU-details-550" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wIzUeM3TAU-details-550"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A general methodology for assessing the expressive and approximation power of GNNs is presented.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="1L0C5ROtFp" data-number="2819">
        <h4>
          <a href="https://openreview.net/forum?id=1L0C5ROtFp">
              Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space
          </a>
        
          
            <a href="https://openreview.net/pdf?id=1L0C5ROtFp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Steeven_JANNY2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Steeven_JANNY2">Steeven JANNY</a>, <a href="https://openreview.net/profile?id=~Fabien_Baradel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fabien_Baradel1">Fabien Baradel</a>, <a href="https://openreview.net/profile?id=~Natalia_Neverova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Natalia_Neverova1">Natalia Neverova</a>, <a href="https://openreview.net/profile?email=madiha.nadri-wolf%40univ-lyon1.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="madiha.nadri-wolf@univ-lyon1.fr">Madiha Nadri</a>, <a href="https://openreview.net/profile?id=~Greg_Mori2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Greg_Mori2">Greg Mori</a>, <a href="https://openreview.net/profile?id=~Christian_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christian_Wolf1">Christian Wolf</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 01 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#1L0C5ROtFp-details-595" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1L0C5ROtFp-details-595"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="p-BhZSz59o4" data-number="2678">
        <h4>
          <a href="https://openreview.net/forum?id=p-BhZSz59o4">
              BEiT: BERT Pre-Training of Image Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=p-BhZSz59o4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hangbo_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hangbo_Bao1">Hangbo Bao</a>, <a href="https://openreview.net/profile?id=~Li_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Li_Dong1">Li Dong</a>, <a href="https://openreview.net/profile?id=~Songhao_Piao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Songhao_Piao1">Songhao Piao</a>, <a href="https://openreview.net/profile?id=~Furu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Furu_Wei1">Furu Wei</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#p-BhZSz59o4-details-87" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="p-BhZSz59o4-details-87"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, pre-training, vision Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a masked image modeling task to pretrain vision Transformers.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UYneFzXSJWh" data-number="2660">
        <h4>
          <a href="https://openreview.net/forum?id=UYneFzXSJWh">
              Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UYneFzXSJWh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ananya_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ananya_Kumar1">Ananya Kumar</a>, <a href="https://openreview.net/profile?id=~Aditi_Raghunathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditi_Raghunathan1">Aditi Raghunathan</a>, <a href="https://openreview.net/profile?id=~Robbie_Matthew_Jones1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robbie_Matthew_Jones1">Robbie Matthew Jones</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tengyu_Ma1">Tengyu Ma</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Percy_Liang1">Percy Liang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UYneFzXSJWh-details-280" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UYneFzXSJWh-details-280"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="16" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo accent="false" stretchy="false">â†’</mo></math></mjx-assistive-mml></mjx-container> STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Fine-tuning does better than linear probing (training a linear classifier on pretrained features) in-distribution, but worse out-of-distribution (OOD)---we analyze why this happens and propose a way to get the benefits of both.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UYneFzXSJWh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Qg2vi4ZbHM9" data-number="2443">
        <h4>
          <a href="https://openreview.net/forum?id=Qg2vi4ZbHM9">
              StyleAlign: Analysis and Applications of Aligned StyleGAN Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Qg2vi4ZbHM9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zongze_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zongze_Wu2">Zongze Wu</a>, <a href="https://openreview.net/profile?id=~Yotam_Nitzan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yotam_Nitzan1">Yotam Nitzan</a>, <a href="https://openreview.net/profile?id=~Eli_Shechtman3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eli_Shechtman3">Eli Shechtman</a>, <a href="https://openreview.net/profile?id=~Dani_Lischinski2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dani_Lischinski2">Dani Lischinski</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Qg2vi4ZbHM9-details-844" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qg2vi4ZbHM9-details-844"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we perform an in-depth study of the properties and applications of aligned generative models.
        We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Analysis and applications of aligned generative models</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Qg2vi4ZbHM9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qnQN4yr6FJz" data-number="2336">
        <h4>
          <a href="https://openreview.net/forum?id=qnQN4yr6FJz">
              Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qnQN4yr6FJz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kohei_Miyaguchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kohei_Miyaguchi1">Kohei Miyaguchi</a>, <a href="https://openreview.net/profile?id=~Takayuki_Katsuki2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Takayuki_Katsuki2">Takayuki Katsuki</a>, <a href="https://openreview.net/profile?id=~Akira_Koseki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Akira_Koseki1">Akira Koseki</a>, <a href="https://openreview.net/profile?id=~Toshiya_Iwamori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Toshiya_Iwamori1">Toshiya Iwamori</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qnQN4yr6FJz-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qnQN4yr6FJz-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Black-box variational inference, missing values, evidence upper bound</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. 
        In this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new variational approximation and algorithm are proposed for discriminative inference with missing features.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uYLFoz1vlAC" data-number="2218">
        <h4>
          <a href="https://openreview.net/forum?id=uYLFoz1vlAC">
              Efficiently Modeling Long Sequences with Structured State Spaces
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uYLFoz1vlAC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Albert_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Albert_Gu1">Albert Gu</a>, <a href="https://openreview.net/profile?id=~Karan_Goel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karan_Goel1">Karan Goel</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Re1">Christopher Re</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uYLFoz1vlAC-details-250" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uYLFoz1vlAC-details-250"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequence models, state space, RNN, CNN, Long Range Arena</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>10000</mn></math></mjx-assistive-mml></mjx-container> or more steps.  A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="18" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>x</mi><mo data-mjx-alternate="1">â€²</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>B</mi><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>D</mi><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, and showed that for appropriate choices of the state matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="19" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>, this system could handle long-range dependencies mathematically and empirically.  However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution.  We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.  Our technique involves conditioning <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="20" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.  S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="21" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>60</mn><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce the S3 model based on new algorithms for state spaces that is particularly effective on long-range dependencies.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=uYLFoz1vlAC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bVuP3ltATMz" data-number="2196">
        <h4>
          <a href="https://openreview.net/forum?id=bVuP3ltATMz">
              Large Language Models Can Be Strong Differentially Private Learners
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bVuP3ltATMz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xuechen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xuechen_Li1">Xuechen Li</a>, <a href="https://openreview.net/profile?id=~Florian_Tramer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florian_Tramer1">Florian Tramer</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Percy_Liang1">Percy Liang</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bVuP3ltATMz-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bVuP3ltATMz-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">language model, differential privacy, language generation, fine-tuning, NLP</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.
        We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.
        With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. 
        To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. 
        The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. 
        Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.
        Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show how to build highly performant differentially private NLP models by fine-tuning large pretrained models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PzcvxEMzvQC" data-number="2181">
        <h4>
          <a href="https://openreview.net/forum?id=PzcvxEMzvQC">
              GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PzcvxEMzvQC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minkai_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minkai_Xu1">Minkai Xu</a>, <a href="https://openreview.net/profile?id=~Lantao_Yu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lantao_Yu2">Lantao Yu</a>, <a href="https://openreview.net/profile?id=~Yang_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Song1">Yang Song</a>, <a href="https://openreview.net/profile?id=~Chence_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chence_Shi1">Chence Shi</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefano_Ermon1">Stefano Ermon</a>, <a href="https://openreview.net/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jian_Tang1">Jian Tang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PzcvxEMzvQC-details-896" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PzcvxEMzvQC-details-896"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">molecular conformation generation, deep generative models, diffusion probabilistic models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel probabilistic diffusion framework to generate accurate and diverse molecular conformations, achieving state-of-the-art results on conformation generation and property prediction</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PzcvxEMzvQC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zIUyj55nXR" data-number="2028">
        <h4>
          <a href="https://openreview.net/forum?id=zIUyj55nXR">
              Frame Averaging for Invariant and Equivariant Network Design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zIUyj55nXR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Omri_Puny1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Omri_Puny1">Omri Puny</a>, <a href="https://openreview.net/profile?id=~Matan_Atzmon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matan_Atzmon1">Matan Atzmon</a>, <a href="https://openreview.net/profile?id=~Edward_J._Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Edward_J._Smith1">Edward J. Smith</a>, <a href="https://openreview.net/profile?id=~Ishan_Misra2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ishan_Misra2">Ishan Misra</a>, <a href="https://openreview.net/profile?id=~Aditya_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Grover1">Aditya Grover</a>, <a href="https://openreview.net/profile?id=~Heli_Ben-Hamu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Heli_Ben-Hamu1">Heli Ben-Hamu</a>, <a href="https://openreview.net/profile?id=~Yaron_Lipman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yaron_Lipman1">Yaron Lipman</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zIUyj55nXR-details-618" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zIUyj55nXR-details-618"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Invariant and equivariant neural network, expressive power</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. 
        We introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="22" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn></math></mjx-assistive-mml></mjx-container>-WL graph separation, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="23" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Introducing a general methodology for building expressive and efficient invariant and equivariant networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oapKSVM2bcj" data-number="1830">
        <h4>
          <a href="https://openreview.net/forum?id=oapKSVM2bcj">
              Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oapKSVM2bcj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alex_Rogozhnikov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Rogozhnikov1">Alex Rogozhnikov</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oapKSVM2bcj-details-696" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oapKSVM2bcj-details-696"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">tensor manipulations, tensor transformation, einops, einstein notation, einsum</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Tensor computations underlie modern scientific computing and deep learning.
        A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc.
        However, tensor operations in all frameworks follow the same paradigm.
        Recent neural network architectures demonstrate demand for higher expressiveness of tensor operations.
        The current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. 
        Moreover, some commonly used operations do not provide sufficient checks and can break a tensor structure.
        These mistakes are elusive as no tools or tests can detect them.
        Independently, API discrepancies complicate code transfer between frameworks.
        We propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors.
        We implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a notation for clear and reliable tensor manipulations; we implented notation in Python package to handle multiple frameworks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dl4LetuLdyK" data-number="1663">
        <h4>
          <a href="https://openreview.net/forum?id=Dl4LetuLdyK">
              A Fine-Grained Analysis on Distribution Shift
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dl4LetuLdyK" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Olivia_Wiles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Olivia_Wiles1">Olivia Wiles</a>, <a href="https://openreview.net/profile?id=~Sven_Gowal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sven_Gowal2">Sven Gowal</a>, <a href="https://openreview.net/profile?id=~Florian_Stimberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florian_Stimberg1">Florian Stimberg</a>, <a href="https://openreview.net/profile?id=~Sylvestre-Alvise_Rebuffi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sylvestre-Alvise_Rebuffi1">Sylvestre-Alvise Rebuffi</a>, <a href="https://openreview.net/profile?email=iraktena%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="iraktena@google.com">Ira Ktena</a>, <a href="https://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krishnamurthy_Dj_Dvijotham1">Krishnamurthy Dj Dvijotham</a>, <a href="https://openreview.net/profile?id=~Ali_Taylan_Cemgil2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ali_Taylan_Cemgil2">Ali Taylan Cemgil</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dl4LetuLdyK-details-353" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dl4LetuLdyK-details-353"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robustness, distribution shifts</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets.  Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani &amp; Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. 
        Code is available at github.com/deepmind/distribution_shift_framework.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate and analyse the robustness of a variety of methods under distribution shifts using our flexible experimental framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Dl4LetuLdyK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5hLP5JY9S2d" data-number="1647">
        <h4>
          <a href="https://openreview.net/forum?id=5hLP5JY9S2d">
              Open-Set Recognition: A Good Closed-Set Classifier is All You Need
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5hLP5JY9S2d" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sagar_Vaze1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sagar_Vaze1">Sagar Vaze</a>, <a href="https://openreview.net/profile?id=~Kai_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kai_Han1">Kai Han</a>, <a href="https://openreview.net/profile?id=~Andrea_Vedaldi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrea_Vedaldi1">Andrea Vedaldi</a>, <a href="https://openreview.net/profile?id=~Andrew_Zisserman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Zisserman1">Andrew Zisserman</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5hLP5JY9S2d-details-625" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5hLP5JY9S2d-details-625"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">open set recognition, image recognition, computer vision</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr_closed_set_all_you_need.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that the baseline method for open-set recognition can achieve state-of-the-art performance and introduce new benchmark settings</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5hLP5JY9S2d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="M752z9FKJP" data-number="1591">
        <h4>
          <a href="https://openreview.net/forum?id=M752z9FKJP">
              Learning Strides in Convolutional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=M752z9FKJP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rachid_Riad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rachid_Riad1">Rachid Riad</a>, <a href="https://openreview.net/profile?id=~Olivier_Teboul2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Olivier_Teboul2">Olivier Teboul</a>, <a href="https://openreview.net/profile?id=~David_Grangier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Grangier1">David Grangier</a>, <a href="https://openreview.net/profile?id=~Neil_Zeghidour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Neil_Zeghidour1">Neil Zeghidour</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#M752z9FKJP-details-936" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="M752z9FKJP-details-936"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce DiffStride, the first downsampling layer with learnable strides for convolutional neural networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7UmjRGzp-A" data-number="1302">
        <h4>
          <a href="https://openreview.net/forum?id=7UmjRGzp-A">
              Understanding over-squashing and bottlenecks on graphs via curvature
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7UmjRGzp-A" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jake_Topping1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jake_Topping1">Jake Topping</a>, <a href="https://openreview.net/profile?id=~Francesco_Di_Giovanni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Francesco_Di_Giovanni1">Francesco Di Giovanni</a>, <a href="https://openreview.net/profile?id=~Benjamin_Paul_Chamberlain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Benjamin_Paul_Chamberlain1">Benjamin Paul Chamberlain</a>, <a href="https://openreview.net/profile?id=~Xiaowen_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaowen_Dong1">Xiaowen Dong</a>, <a href="https://openreview.net/profile?id=~Michael_M._Bronstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_M._Bronstein1">Michael M. Bronstein</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7UmjRGzp-A-details-28" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7UmjRGzp-A-details-28"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="24" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-hop neighbors grows rapidly with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="25" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a  curvature-based graph rewiring method to alleviate the over-squashing.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="8c50f-DoWAu" data-number="1253">
        <h4>
          <a href="https://openreview.net/forum?id=8c50f-DoWAu">
              Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme
          </a>
        
          
            <a href="https://openreview.net/pdf?id=8c50f-DoWAu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vadim_Popov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vadim_Popov1">Vadim Popov</a>, <a href="https://openreview.net/profile?id=~Ivan_Vovk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_Vovk1">Ivan Vovk</a>, <a href="https://openreview.net/profile?id=~Vladimir_Gogoryan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vladimir_Gogoryan1">Vladimir Gogoryan</a>, <a href="https://openreview.net/profile?id=~Tasnima_Sadekova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tasnima_Sadekova1">Tasnima Sadekova</a>, <a href="https://openreview.net/profile?id=~Mikhail_Sergeevich_Kudinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mikhail_Sergeevich_Kudinov1">Mikhail Sergeevich Kudinov</a>, <a href="https://openreview.net/profile?id=~Jiansheng_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiansheng_Wei1">Jiansheng Wei</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 27 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#8c50f-DoWAu-details-111" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8c50f-DoWAu-details-111"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">speech, voice conversion, diffusion models, stochastic differential equations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EskfH0bwNVn" data-number="938">
        <h4>
          <a href="https://openreview.net/forum?id=EskfH0bwNVn">
              Resolving Training Biases via Influence-based Data Relabeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EskfH0bwNVn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shuming_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shuming_Kong1">Shuming Kong</a>, <a href="https://openreview.net/profile?id=~Yanyan_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanyan_Shen1">Yanyan Shen</a>, <a href="https://openreview.net/profile?id=~Linpeng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Linpeng_Huang1">Linpeng Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EskfH0bwNVn-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EskfH0bwNVn-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Training bias, influence functions, data relabeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that estimates the impacts of a training sample on the modelâ€™s predictions. Recent studies on \emph{data resampling} have employed influence functions to identify \emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an influence-based relabeling framework for solving training bias with a theoretical guarantee</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=EskfH0bwNVn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="9Hrka5PA7LW" data-number="934">
        <h4>
          <a href="https://openreview.net/forum?id=9Hrka5PA7LW">
              Representational Continuity for Unsupervised Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=9Hrka5PA7LW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Divyam_Madaan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Divyam_Madaan1">Divyam Madaan</a>, <a href="https://openreview.net/profile?id=~Jaehong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jaehong_Yoon1">Jaehong Yoon</a>, <a href="https://openreview.net/profile?id=~Yuanchun_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuanchun_Li1">Yuanchun Li</a>, <a href="https://openreview.net/profile?id=~Yunxin_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yunxin_Liu2">Yunxin Liu</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#9Hrka5PA7LW-details-151" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9Hrka5PA7LW-details-151"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Continual Learning, Representational Learning, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We attempt to bridge the gap between continual learning &amp; representation learning and show that unsupervised continual learning achieves better performance and learns perceptual features with a smoother loss landscape than SCL.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RJkAHKp7kNZ" data-number="870">
        <h4>
          <a href="https://openreview.net/forum?id=RJkAHKp7kNZ">
              Vision-Based Manipulators Need to Also See from Their Hands
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RJkAHKp7kNZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kyle_Hsu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyle_Hsu1">Kyle Hsu</a>, <a href="https://openreview.net/profile?id=~Moo_Jin_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Moo_Jin_Kim1">Moo Jin Kim</a>, <a href="https://openreview.net/profile?id=~Rafael_Rafailov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rafael_Rafailov1">Rafael Rafailov</a>, <a href="https://openreview.net/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiajun_Wu1">Jiajun Wu</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chelsea_Finn1">Chelsea Finn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RJkAHKp7kNZ-details-643" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RJkAHKp7kNZ-details-643"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Appropriately designing the observation space of a vision-based manipulator and regularizing its representations leads to clear gains in learning stability and out-of-distribution generalization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ajXWF7bVR8d" data-number="868">
        <h4>
          <a href="https://openreview.net/forum?id=ajXWF7bVR8d">
              Meta-Learning with Fewer Tasks through Task Interpolation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ajXWF7bVR8d" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Huaxiu_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huaxiu_Yao1">Huaxiu Yao</a>, <a href="https://openreview.net/profile?id=~Linjun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Linjun_Zhang1">Linjun Zhang</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chelsea_Finn1">Chelsea Finn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ajXWF7bVR8d-details-942" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ajXWF7bVR8d-details-942"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-learning, task interpolation, meta-regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new framework to densify the task distribution via task interpolation</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="iRCUlgmdfHJ" data-number="822">
        <h4>
          <a href="https://openreview.net/forum?id=iRCUlgmdfHJ">
              DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS
          </a>
        
          
            <a href="https://openreview.net/pdf?id=iRCUlgmdfHJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Huiqi_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huiqi_Deng1">Huiqi Deng</a>, <a href="https://openreview.net/profile?id=~Qihan_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qihan_Ren1">Qihan Ren</a>, <a href="https://openreview.net/profile?id=~Hao_Zhang22" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Zhang22">Hao Zhang</a>, <a href="https://openreview.net/profile?id=~Quanshi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanshi_Zhang1">Quanshi Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#iRCUlgmdfHJ-details-675" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iRCUlgmdfHJ-details-675"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation bottleneck, representation ability, interaction, explanation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WAid50QschI" data-number="689">
        <h4>
          <a href="https://openreview.net/forum?id=WAid50QschI">
              Sparse Communication via Mixed Distributions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WAid50QschI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ant%C3%B3nio_Farinhas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~AntÃ³nio_Farinhas1">AntÃ³nio Farinhas</a>, <a href="https://openreview.net/profile?id=~Wilker_Aziz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wilker_Aziz1">Wilker Aziz</a>, <a href="https://openreview.net/profile?id=~Vlad_Niculae2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vlad_Niculae2">Vlad Niculae</a>, <a href="https://openreview.net/profile?id=~Andre_Martins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andre_Martins1">Andre Martins</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 28 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WAid50QschI-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WAid50QschI-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call "mixed random variables.'' Our starting point is a new "direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic ("sample-and-project'â€™) and an intrinsic one (based on face stratification). We experiment with both approaches on an  emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WAid50QschI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gEZrGCozdqR" data-number="598">
        <h4>
          <a href="https://openreview.net/forum?id=gEZrGCozdqR">
              Finetuned Language Models are Zero-Shot Learners
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gEZrGCozdqR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jason_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_Wei1">Jason Wei</a>, <a href="https://openreview.net/profile?email=bosma%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="bosma@google.com">Maarten Bosma</a>, <a href="https://openreview.net/profile?email=vzhao%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="vzhao@google.com">Vincent Zhao</a>, <a href="https://openreview.net/profile?id=~Kelvin_Guu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kelvin_Guu1">Kelvin Guu</a>, <a href="https://openreview.net/profile?id=~Adams_Wei_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adams_Wei_Yu1">Adams Wei Yu</a>, <a href="https://openreview.net/profile?id=~Brian_Lester1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Brian_Lester1">Brian Lester</a>, <a href="https://openreview.net/profile?id=~Nan_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Du1">Nan Du</a>, <a href="https://openreview.net/profile?id=~Andrew_M._Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_M._Dai1">Andrew M. Dai</a>, <a href="https://openreview.net/profile?id=~Quoc_V_Le1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quoc_V_Le1">Quoc V Le</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gEZrGCozdqR-details-483" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gEZrGCozdqR-details-483"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">natural language processing, zero-shot learning, language models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuningâ€”finetuning language models on a collection of datasets described via instructionsâ€”substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">"Instruction tuning", which finetunes language models on a collection of tasks described via instructions, substantially boosts zero-shot performance on unseen tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=gEZrGCozdqR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_CfpJazzXT2" data-number="570">
        <h4>
          <a href="https://openreview.net/forum?id=_CfpJazzXT2">
              F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_CfpJazzXT2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qing_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qing_Jin1">Qing Jin</a>, <a href="https://openreview.net/profile?id=~Jian_Ren2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jian_Ren2">Jian Ren</a>, <a href="https://openreview.net/profile?email=rzhuang%40snapchat.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="rzhuang@snapchat.com">Richard Zhuang</a>, <a href="https://openreview.net/profile?email=shanumante%40snapchat.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shanumante@snapchat.com">Sumant Hanumante</a>, <a href="https://openreview.net/profile?id=~Zhengang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhengang_Li2">Zhengang Li</a>, <a href="https://openreview.net/profile?id=~Zhiyu_Chen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhiyu_Chen3">Zhiyu Chen</a>, <a href="https://openreview.net/profile?id=~Yanzhi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanzhi_Wang3">Yanzhi Wang</a>, <a href="https://openreview.net/profile?id=~Kaiyuan_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaiyuan_Yang1">Kaiyuan Yang</a>, <a href="https://openreview.net/profile?id=~Sergey_Tulyakov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Tulyakov1">Sergey Tulyakov</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 25 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">30 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_CfpJazzXT2-details-804" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_CfpJazzXT2-details-804"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Network Quantization, Fixed-Point Arithmetic</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only ï¬xed-point 8-bit multiplication. To derive our method, we ï¬rst discuss the advantages of ï¬xed-point multiplication with different formats of ï¬xed-point numbers and study the statistical behavior of the associated ï¬xed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different ï¬xed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithmâ€”parameterized clipping activation (PACT)â€”and reformulate it using ï¬xed-point arithmetic. Finally, we unify the recently proposed method for quantization ï¬ne-tuning and our ï¬xed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or ï¬‚oating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method for neural network quantization with only 8-bit fixed-point multiplication.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UcDUxjPYWSr" data-number="565">
        <h4>
          <a href="https://openreview.net/forum?id=UcDUxjPYWSr">
              Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UcDUxjPYWSr" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ye_Yuan5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ye_Yuan5">Ye Yuan</a>, <a href="https://openreview.net/profile?id=~Yuda_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuda_Song2">Yuda Song</a>, <a href="https://openreview.net/profile?id=~Zhengyi_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhengyi_Luo1">Zhengyi Luo</a>, <a href="https://openreview.net/profile?id=~Wen_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wen_Sun1">Wen Sun</a>, <a href="https://openreview.net/profile?id=~Kris_M._Kitani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kris_M._Kitani1">Kris M. Kitani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UcDUxjPYWSr-details-529" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UcDUxjPYWSr-details-529"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Agent Design, Morphology Optimization, Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We learn a transform-and-control policy to both design and control an agent.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="s03AQxehtd_" data-number="549">
        <h4>
          <a href="https://openreview.net/forum?id=s03AQxehtd_">
              ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics
          </a>
        
          
            <a href="https://openreview.net/pdf?id=s03AQxehtd_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Boris_N._Oreshkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Boris_N._Oreshkin1">Boris N. Oreshkin</a>, <a href="https://openreview.net/profile?id=~Florent_Bocquelet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florent_Bocquelet1">Florent Bocquelet</a>, <a href="https://openreview.net/profile?id=~Felix_G._Harvey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Felix_G._Harvey1">Felix G. Harvey</a>, <a href="https://openreview.net/profile?id=~Bay_Raitt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bay_Raitt1">Bay Raitt</a>, <a href="https://openreview.net/profile?id=~Dominic_Laflamme1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dominic_Laflamme1">Dominic Laflamme</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">25 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#s03AQxehtd_-details-750" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="s03AQxehtd_-details-750"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">inverse kinematics, deep learning, pose modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=s03AQxehtd_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-70L8lpp9DF" data-number="287">
        <h4>
          <a href="https://openreview.net/forum?id=-70L8lpp9DF">
              Hyperparameter Tuning with Renyi Differential Privacy
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-70L8lpp9DF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nicolas_Papernot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicolas_Papernot1">Nicolas Papernot</a>, <a href="https://openreview.net/profile?id=~Thomas_Steinke2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Steinke2">Thomas Steinke</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-70L8lpp9DF-details-621" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-70L8lpp9DF-details-621"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">differential privacy, hyperparameter tuning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithmâ€™s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide privacy guarantees for hyperparameter search procedures, showing that tuning hyperparameters leaks private information, but that, under certain assumptions, this leakage is modest.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qj1IZ-6TInc" data-number="284">
        <h4>
          <a href="https://openreview.net/forum?id=qj1IZ-6TInc">
              Real-Time Neural Voice Camouflage
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qj1IZ-6TInc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mia_Chiquier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mia_Chiquier1">Mia Chiquier</a>, <a href="https://openreview.net/profile?id=~Chengzhi_Mao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chengzhi_Mao2">Chengzhi Mao</a>, <a href="https://openreview.net/profile?id=~Carl_Vondrick2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Carl_Vondrick2">Carl Vondrick</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Feb 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qj1IZ-6TInc-details-0" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qj1IZ-6TInc-details-0"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">automatic speech recognition, predictive models, privacy</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce predictive attacks, which achieve real-time performance in breaking automatic speech recognition models by forecasting the attack vector that will be the most effective in the future. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=qj1IZ-6TInc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade" id="spotlight-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="g1SzIRLQXMM" data-number="4724">
        <h4>
          <a href="https://openreview.net/forum?id=g1SzIRLQXMM">
              Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream
          </a>
        
          
            <a href="https://openreview.net/pdf?id=g1SzIRLQXMM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Franziska_Geiger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Franziska_Geiger1">Franziska Geiger</a>, <a href="https://openreview.net/profile?id=~Martin_Schrimpf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martin_Schrimpf1">Martin Schrimpf</a>, <a href="https://openreview.net/profile?id=~Tiago_Marques2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tiago_Marques2">Tiago Marques</a>, <a href="https://openreview.net/profile?id=~James_J._DiCarlo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_J._DiCarlo1">James J. DiCarlo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#g1SzIRLQXMM-details-217" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g1SzIRLQXMM-details-217"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">computational neuroscience, primate visual ventral stream, convolutional neural networks, biologically plausible learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image.  While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to standard model training on labeled images in ImageNet, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve 80% of the match to adult ventral stream. Specifically, training benefits predictions of higher visual cortex the most whereas early visual cortex predictions only improve marginally over the course of training. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved â€œat birth" (i.e. no training at all). Third, we find that, by training only 5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. This approach further improves on ImageNet performance over previous attempts in computer vision of minimizing trained components without substantially increasing the relative number of trained parameters. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be "wired up" by evolution (a model's "birth" state) and by developmental learning (a model's updates based on visual experience).</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop biologically-motivated initialization and training procedures to train models with 200x fewer synaptic updates (epochs x labeled images x weights) while maintaining 80% of brain predictivity on a set of neural and behavioral benchmarks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CALFyKVs87" data-number="4616">
        <h4>
          <a href="https://openreview.net/forum?id=CALFyKVs87">
              Dynamics-Aware Comparison of Learned Reward Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CALFyKVs87" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Blake_Wulfe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Blake_Wulfe1">Blake Wulfe</a>, <a href="https://openreview.net/profile?email=logan.ellis%40tri.global" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="logan.ellis@tri.global">Logan Michael Ellis</a>, <a href="https://openreview.net/profile?email=jean.mercat%40tri.global" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="jean.mercat@tri.global">Jean Mercat</a>, <a href="https://openreview.net/profile?id=~Rowan_Thomas_McAllister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rowan_Thomas_McAllister1">Rowan Thomas McAllister</a>, <a href="https://openreview.net/profile?id=~Adrien_Gaidon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adrien_Gaidon1">Adrien Gaidon</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CALFyKVs87-details-615" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CALFyKVs87-details-615"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reward Learning, Inverse Reinforcement Learning, Reinforcement Learning, Comparing Reward Functions</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="26" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">comparing</mtext></math></mjx-assistive-mml></mjx-container> reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method for quantifying the similarity of learned reward functions without performing policy learning and evaluation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=CALFyKVs87&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5LXw_QplBiF" data-number="4554">
        <h4>
          <a href="https://openreview.net/forum?id=5LXw_QplBiF">
              Learning Hierarchical Structures with Differentiable Nondeterministic Stacks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5LXw_QplBiF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Brian_DuSell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Brian_DuSell1">Brian DuSell</a>, <a href="https://openreview.net/profile?id=~David_Chiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Chiang1">David Chiang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5LXw_QplBiF-details-426" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5LXw_QplBiF-details-426"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RNN, pushdown automata, nondeterminism, formal languages, language modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a new stack-augmented RNN with strong results on CFL language modeling tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5LXw_QplBiF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eMudnJsb1T5" data-number="4500">
        <h4>
          <a href="https://openreview.net/forum?id=eMudnJsb1T5">
              Sampling with Mirrored Stein Operators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eMudnJsb1T5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiaxin_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiaxin_Shi1">Jiaxin Shi</a>, <a href="https://openreview.net/profile?id=~Chang_Liu10" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chang_Liu10">Chang Liu</a>, <a href="https://openreview.net/profile?id=~Lester_Mackey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lester_Mackey1">Lester Mackey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eMudnJsb1T5-details-234" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eMudnJsb1T5-details-234"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stein's method, Sampling, Mirror descent, Natural gradient descent, Probabilistic inference, Bayesian inference, Post-selection inference, Stein operators</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efficiently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under verifiable conditions on the target distribution.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce multi-particle generalization of mirror descent for sampling in constrained domains and non-Euclidean geometries.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="X6D9bAHhBQ1" data-number="4498">
        <h4>
          <a href="https://openreview.net/forum?id=X6D9bAHhBQ1">
              Planning in Stochastic Environments with a Learned Model
          </a>
        
          
            <a href="https://openreview.net/pdf?id=X6D9bAHhBQ1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ioannis_Antonoglou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ioannis_Antonoglou1">Ioannis Antonoglou</a>, <a href="https://openreview.net/profile?id=~Julian_Schrittwieser1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julian_Schrittwieser1">Julian Schrittwieser</a>, <a href="https://openreview.net/profile?id=~Sherjil_Ozair1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sherjil_Ozair1">Sherjil Ozair</a>, <a href="https://openreview.net/profile?id=~Thomas_K_Hubert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_K_Hubert1">Thomas K Hubert</a>, <a href="https://openreview.net/profile?id=~David_Silver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Silver1">David Silver</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#X6D9bAHhBQ1-details-445" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="X6D9bAHhBQ1-details-445"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, deep reinforcement learning, tree based search, MCTS</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the same performance as standard MuZero in the game of Go.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="T8wHz4rnuGL" data-number="4402">
        <h4>
          <a href="https://openreview.net/forum?id=T8wHz4rnuGL">
              RotoGrad: Gradient Homogenization in Multitask Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=T8wHz4rnuGL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Adri%C3%A1n_Javaloy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~AdriÃ¡n_Javaloy1">AdriÃ¡n Javaloy</a>, <a href="https://openreview.net/profile?id=~Isabel_Valera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Isabel_Valera1">Isabel Valera</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#T8wHz4rnuGL-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T8wHz4rnuGL-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multitask learning, conflicting gradients, negative transfer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks, when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short as they only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an algorithm to simultaneously homogenize gradient magnitudes and directions across tasks in MTL.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=T8wHz4rnuGL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="D6nH3719vZy" data-number="4395">
        <h4>
          <a href="https://openreview.net/forum?id=D6nH3719vZy">
              On Improving Adversarial Transferability of Vision Transformers 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=D6nH3719vZy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Muzammal_Naseer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muzammal_Naseer1">Muzammal Naseer</a>, <a href="https://openreview.net/profile?id=~Kanchana_Ranasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kanchana_Ranasinghe1">Kanchana Ranasinghe</a>, <a href="https://openreview.net/profile?id=~Salman_Khan4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Salman_Khan4">Salman Khan</a>, <a href="https://openreview.net/profile?id=~Fahad_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fahad_Khan1">Fahad Khan</a>, <a href="https://openreview.net/profile?id=~Fatih_Porikli2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fatih_Porikli2">Fatih Porikli</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#D6nH3719vZy-details-477" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D6nH3719vZy-details-477"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Adversarial Perturbations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs).  This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs.Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models.  \emph{(i) Self-Ensemble:} We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. \emph{(ii) Token Refinement:} We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT.Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such refined tokens within the ensemble of classifiers found in a single vision transformer has significantly higher transferability and thereby brings out the true generalization potential of the ViT's adversarial space. Code: https://t.ly/hBbW.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Novel approach to improve transferability of adversarial perturbations found in vision transformers via self-ensemble and token refinement.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eW5R4Cek6y6" data-number="4393">
        <h4>
          <a href="https://openreview.net/forum?id=eW5R4Cek6y6">
              On Predicting Generalization using GANs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eW5R4Cek6y6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Zhang1">Yi Zhang</a>, <a href="https://openreview.net/profile?id=~Arushi_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arushi_Gupta1">Arushi Gupta</a>, <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sanjeev_Arora1">Sanjeev Arora</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eW5R4Cek6y6-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eW5R4Cek6y6-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalization, generative adversarial network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting Generalization in Deep Learning competition aims to encourage discovery of methods to better predict test error. The current paper investigates a simple idea: can test error be predicted using {\em synthetic data,} produced using a Generative Adversarial Network (GAN) that was trained on the same training dataset? Upon investigating several GAN models and architectures, we find that this turns out to be the case. 
        
        In fact, using GANs pre-trained on standard datasets, the test error can be predicted without requiring any additional hyper-parameter tuning. This result is surprising because GANs have well-known limitations (e.g. mode collapse) and are known to not learn the data distribution accurately. Yet the generated samples are good enough to substitute for test data. Several additional experiments are presented to explore reasons why GANs do well at this task. In addition to a new approach for predicting generalization, the counter-intuitive phenomena presented in our work may also call for a better understanding of GANs' strengths and limitations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="L3_SsSNMmy" data-number="4356">
        <h4>
          <a href="https://openreview.net/forum?id=L3_SsSNMmy">
              On the Connection between Local Attention and Dynamic Depth-wise Convolution
          </a>
        
          
            <a href="https://openreview.net/pdf?id=L3_SsSNMmy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qi_Han3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qi_Han3">Qi Han</a>, <a href="https://openreview.net/profile?id=~Zejia_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zejia_Fan1">Zejia Fan</a>, <a href="https://openreview.net/profile?id=~Qi_Dai4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qi_Dai4">Qi Dai</a>, <a href="https://openreview.net/profile?id=~Lei_Sun4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lei_Sun4">Lei Sun</a>, <a href="https://openreview.net/profile?id=~Ming-Ming_Cheng3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming-Ming_Cheng3">Ming-Ming Cheng</a>, <a href="https://openreview.net/profile?id=~Jiaying_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiaying_Liu1">Jiaying Liu</a>, <a href="https://openreview.net/profile?id=~Jingdong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingdong_Wang1">Jingdong Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#L3_SsSNMmy-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="L3_SsSNMmy-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">local attention, depth-wise convolution, dynamic depth-wise convolution, weight sharing, dynamic weight</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution. We empirically observe that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the connection between local attention and dynamic depth-wise convolution in terms of sparse connectivity, weight sharing, and dynamic weight</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uorVGbWV5sw" data-number="4355">
        <h4>
          <a href="https://openreview.net/forum?id=uorVGbWV5sw">
              Strength of Minibatch Noise in SGD
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uorVGbWV5sw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Ziyin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Ziyin1">Liu Ziyin</a>, <a href="https://openreview.net/profile?id=~Kangqiao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kangqiao_Liu1">Kangqiao Liu</a>, <a href="https://openreview.net/profile?id=~Takashi_Mori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Takashi_Mori1">Takashi Mori</a>, <a href="https://openreview.net/profile?id=~Masahito_Ueda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Masahito_Ueda1">Masahito Ueda</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 08 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uorVGbWV5sw-details-692" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uorVGbWV5sw-details-692"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stochastic gradient descent, minibatch noise, discrete-time SGD, noise and fluctuation, exact solvable models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We solve the strength and shape of the minibatch noise in SGD exactly. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="cU8rknuhxc" data-number="4352">
        <h4>
          <a href="https://openreview.net/forum?id=cU8rknuhxc">
              Learning more skills through optimistic exploration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=cU8rknuhxc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~DJ_Strouse1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~DJ_Strouse1">DJ Strouse</a>, <a href="https://openreview.net/profile?id=~Kate_Baumli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kate_Baumli1">Kate Baumli</a>, <a href="https://openreview.net/profile?id=~David_Warde-Farley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Warde-Farley1">David Warde-Farley</a>, <a href="https://openreview.net/profile?id=~Volodymyr_Mnih1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Volodymyr_Mnih1">Volodymyr Mnih</a>, <a href="https://openreview.net/profile?id=~Steven_Stenberg_Hansen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Steven_Stenberg_Hansen1">Steven Stenberg Hansen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#cU8rknuhxc-details-249" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cU8rknuhxc-details-249"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">intrinsic control, skill discovery, unsupervised skill learning, uncertainty estimation, optimistic exploration, variational information maximization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learn more skills by adding an information gain exploration bonus based on discriminator ensemble disagreement.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PLDOnFoVm4" data-number="4323">
        <h4>
          <a href="https://openreview.net/forum?id=PLDOnFoVm4">
              Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PLDOnFoVm4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhi_Zhang1">Zhi Zhang</a>, <a href="https://openreview.net/profile?id=~Zhuoran_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuoran_Yang1">Zhuoran Yang</a>, <a href="https://openreview.net/profile?id=~Han_Liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Han_Liu4">Han Liu</a>, <a href="https://openreview.net/profile?id=~Pratap_Tokekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pratap_Tokekar1">Pratap Tokekar</a>, <a href="https://openreview.net/profile?id=~Furong_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Furong_Huang1">Furong Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PLDOnFoVm4-details-884" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PLDOnFoVm4-details-884"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent Reinforcement Learning, Predictive State Representation, Dynamic Interaction Graph</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. 
        Based on GAPSR, we propose an end-to-end  MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new algorithm for MARL under a multi-agent predictive state representation model, where we incorporate a dynamic interaction graph; we provide the theoretical guarantees of our model and run various experiments to support our algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PLDOnFoVm4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="26gKg6x-ie" data-number="4292">
        <h4>
          <a href="https://openreview.net/forum?id=26gKg6x-ie">
              Adversarial Support Alignment
          </a>
        
          
            <a href="https://openreview.net/pdf?id=26gKg6x-ie" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shangyuan_Tong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shangyuan_Tong1">Shangyuan Tong</a>, <a href="https://openreview.net/profile?id=~Timur_Garipov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Timur_Garipov1">Timur Garipov</a>, <a href="https://openreview.net/profile?id=~Yang_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Zhang3">Yang Zhang</a>, <a href="https://openreview.net/profile?id=~Shiyu_Chang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shiyu_Chang2">Shiyu Chang</a>, <a href="https://openreview.net/profile?id=~Tommi_S._Jaakkola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tommi_S._Jaakkola1">Tommi S. Jaakkola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#26gKg6x-ie-details-998" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="26gKg6x-ie-details-998"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">support alignment, distribution alignment, optimal transport, domain adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the problem of aligning the supports of distributions.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="41e9o6cQPj" data-number="4280">
        <h4>
          <a href="https://openreview.net/forum?id=41e9o6cQPj">
              GreaseLM: Graph REASoning Enhanced Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=41e9o6cQPj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xikun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xikun_Zhang1">Xikun Zhang</a>, <a href="https://openreview.net/profile?id=~Antoine_Bosselut1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Antoine_Bosselut1">Antoine Bosselut</a>, <a href="https://openreview.net/profile?id=~Michihiro_Yasunaga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michihiro_Yasunaga1">Michihiro Yasunaga</a>, <a href="https://openreview.net/profile?id=~Hongyu_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hongyu_Ren1">Hongyu Ren</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Percy_Liang1">Percy Liang</a>, <a href="https://openreview.net/profile?id=~Christopher_D_Manning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_D_Manning1">Christopher D Manning</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jure_Leskovec1">Jure Leskovec</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#41e9o6cQPj-details-253" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="41e9o6cQPj-details-253"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">language models, commonsense, question answering, knowledge graphs, KG augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and GNNs over multiple layers of modality interaction operations, allowing both modalities to bidirectionally inform the representation of the other.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DTkEfj0Ygb8" data-number="4263">
        <h4>
          <a href="https://openreview.net/forum?id=DTkEfj0Ygb8">
              Learning meta-features for AutoML
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DTkEfj0Ygb8" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Herilalaina_Rakotoarison1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Herilalaina_Rakotoarison1">Herilalaina Rakotoarison</a>, <a href="https://openreview.net/profile?id=~Louisot_Milijaona1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Louisot_Milijaona1">Louisot Milijaona</a>, <a href="https://openreview.net/profile?id=~Andry_RASOANAIVO1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andry_RASOANAIVO1">Andry RASOANAIVO</a>, <a href="https://openreview.net/profile?id=~Michele_Sebag1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michele_Sebag1">Michele Sebag</a>, <a href="https://openreview.net/profile?id=~Marc_Schoenauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marc_Schoenauer1">Marc Schoenauer</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DTkEfj0Ygb8-details-249" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DTkEfj0Ygb8-details-249"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AutoML, Meta-features, Hyper-parameter Optimization, Optimal Transport</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed \mf s with the space of distributions on the hyper-parameter configurations. MetaBu meta-features, learned once and for all, induce a topology on the set of datasets that is exploited to define a distribution of promising hyper-parameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta-features boosts the performance of state of the art AutoML systems, AutoSklearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu meta-features gives some hints into when an ML algorithm does well. Finally, the topology based on MetaBu meta-features enables to estimate the intrinsic dimensionality of the OpenML benchmark w.r.t. a given ML algorithm or pipeline. The source code is available at https://github.com/luxusg1/metabu.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel approach to learn dataset meta-features for AutoML.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dup_dDqkZC5" data-number="4221">
        <h4>
          <a href="https://openreview.net/forum?id=Dup_dDqkZC5">
              Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dup_dDqkZC5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Roger_Girgis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roger_Girgis1">Roger Girgis</a>, <a href="https://openreview.net/profile?id=~Florian_Golemo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florian_Golemo1">Florian Golemo</a>, <a href="https://openreview.net/profile?id=~Felipe_Codevilla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Felipe_Codevilla1">Felipe Codevilla</a>, <a href="https://openreview.net/profile?id=~Martin_Weiss4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martin_Weiss4">Martin Weiss</a>, <a href="https://openreview.net/profile?id=~Jim_Aldon_D%26%23x27%3BSouza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jim_Aldon_D&#39;Souza1">Jim Aldon D'Souza</a>, <a href="https://openreview.net/profile?id=~Samira_Ebrahimi_Kahou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samira_Ebrahimi_Kahou1">Samira Ebrahimi Kahou</a>, <a href="https://openreview.net/profile?id=~Felix_Heide2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Felix_Heide2">Felix Heide</a>, <a href="https://openreview.net/profile?id=~Christopher_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Pal1">Christopher Pal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dup_dDqkZC5-details-672" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dup_dDqkZC5-details-672"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">trajectory prediction, motion forecasting, transformers, latent variable models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as â€œAutoBotsâ€. The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the
        entire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the modelâ€™s socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a
        single desktop GPU (1080 Ti) in under 48h.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">New Transformer-based architecture for socially consistent motion forecasting. Achieves SotA performance on NuScenes at a fraction of the compute of competing methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Dup_dDqkZC5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5FUq05QRc5b" data-number="4176">
        <h4>
          <a href="https://openreview.net/forum?id=5FUq05QRc5b">
              Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5FUq05QRc5b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qi_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qi_Lyu2">Qi Lyu</a>, <a href="https://openreview.net/profile?id=~Xiao_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiao_Fu1">Xiao Fu</a>, <a href="https://openreview.net/profile?id=~Weiran_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weiran_Wang1">Weiran Wang</a>, <a href="https://openreview.net/profile?id=~Songtao_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Songtao_Lu1">Songtao Lu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5FUq05QRc5b-details-140" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5FUq05QRc5b-details-140"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated. 
        This work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5FUq05QRc5b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EDeVYpT42oS" data-number="4137">
        <h4>
          <a href="https://openreview.net/forum?id=EDeVYpT42oS">
              Deconstructing the Inductive Biases of Hamiltonian Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EDeVYpT42oS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nate_Gruver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nate_Gruver1">Nate Gruver</a>, <a href="https://openreview.net/profile?id=~Marc_Anton_Finzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marc_Anton_Finzi1">Marc Anton Finzi</a>, <a href="https://openreview.net/profile?id=~Samuel_Don_Stanton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samuel_Don_Stanton1">Samuel Don Stanton</a>, <a href="https://openreview.net/profile?id=~Andrew_Gordon_Wilson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Gordon_Wilson1">Andrew Gordon Wilson</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EDeVYpT42oS-details-689" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EDeVYpT42oS-details-689"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that donâ€™t conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TrjbxzRcnf-" data-number="4131">
        <h4>
          <a href="https://openreview.net/forum?id=TrjbxzRcnf-">
              Memorizing Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TrjbxzRcnf-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuhuai_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuhuai_Wu1">Yuhuai Wu</a>, <a href="https://openreview.net/profile?id=~Markus_Norman_Rabe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Markus_Norman_Rabe1">Markus Norman Rabe</a>, <a href="https://openreview.net/profile?id=~DeLesley_Hutchins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~DeLesley_Hutchins1">DeLesley Hutchins</a>, <a href="https://openreview.net/profile?id=~Christian_Szegedy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christian_Szegedy1">Christian Szegedy</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TrjbxzRcnf--details-895" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TrjbxzRcnf--details-895"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transformer, architecture, memorization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights.  
        We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="27" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. 
        On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose to use an external memory module to allow instant utilization of newly acquired knowledge.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="X8cLTHexYyY" data-number="4103">
        <h4>
          <a href="https://openreview.net/forum?id=X8cLTHexYyY">
              Learning-Augmented <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="28" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-means Clustering
          </a>
        
          
            <a href="https://openreview.net/pdf?id=X8cLTHexYyY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jon_Ergun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jon_Ergun1">Jon C. Ergun</a>, <a href="https://openreview.net/profile?id=~Zhili_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhili_Feng1">Zhili Feng</a>, <a href="https://openreview.net/profile?id=~Sandeep_Silwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandeep_Silwal1">Sandeep Silwal</a>, <a href="https://openreview.net/profile?id=~David_Woodruff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Woodruff1">David Woodruff</a>, <a href="https://openreview.net/profile?id=~Samson_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samson_Zhou1">Samson Zhou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#X8cLTHexYyY-details-765" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="X8cLTHexYyY-details-765"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">clustering, learning-augmented algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="29" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="30" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where ``advice'' is provided to help perform clustering. Specifically, we consider the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="31" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though na\"{i}vely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="32" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-means problem augmented with a learning-based predictor that gives noisy information about true labels.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=X8cLTHexYyY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SsPCtEY6yCl" data-number="4060">
        <h4>
          <a href="https://openreview.net/forum?id=SsPCtEY6yCl">
              On the Uncomputability of Partition Functions in Energy-Based Sequence Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SsPCtEY6yCl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chu-Cheng_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chu-Cheng_Lin1">Chu-Cheng Lin</a>, <a href="https://openreview.net/profile?id=~Arya_D._McCarthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arya_D._McCarthy1">Arya D. McCarthy</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SsPCtEY6yCl-details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SsPCtEY6yCl-details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based models, turing completeness, model capacity, sequence models, autoregressive models, partition function, parameter estimation, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection--and therefore learning model parameters--not only difficult, but generally _undecidable_. The reason is that there are no good deterministic or randomized estimates of partition functions. Specifically, we exhibit a pathological example where under common assumptions, _no_ useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, we consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but finite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">EBMs over sequences have several theoretical limitations as learnable probabilistic sequence models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fILj7WpI-g" data-number="4039">
        <h4>
          <a href="https://openreview.net/forum?id=fILj7WpI-g">
              Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fILj7WpI-g" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrew_Jaegle2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Jaegle2">Andrew Jaegle</a>, <a href="https://openreview.net/profile?id=~Sebastian_Borgeaud1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sebastian_Borgeaud1">Sebastian Borgeaud</a>, <a href="https://openreview.net/profile?id=~Jean-Baptiste_Alayrac2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jean-Baptiste_Alayrac2">Jean-Baptiste Alayrac</a>, <a href="https://openreview.net/profile?id=~Carl_Doersch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Carl_Doersch1">Carl Doersch</a>, <a href="https://openreview.net/profile?id=~Catalin_Ionescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Catalin_Ionescu1">Catalin Ionescu</a>, <a href="https://openreview.net/profile?id=~David_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Ding2">David Ding</a>, <a href="https://openreview.net/profile?id=~Skanda_Koppula1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Skanda_Koppula1">Skanda Koppula</a>, <a href="https://openreview.net/profile?id=~Daniel_Zoran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Zoran1">Daniel Zoran</a>, <a href="https://openreview.net/profile?id=~Andrew_Brock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Brock1">Andrew Brock</a>, <a href="https://openreview.net/profile?id=~Evan_Shelhamer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Evan_Shelhamer2">Evan Shelhamer</a>, <a href="https://openreview.net/profile?id=~Olivier_J_Henaff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Olivier_J_Henaff1">Olivier J Henaff</a>, <a href="https://openreview.net/profile?id=~Matthew_Botvinick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matthew_Botvinick1">Matthew Botvinick</a>, <a href="https://openreview.net/profile?id=~Andrew_Zisserman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Zisserman1">Andrew Zisserman</a>, <a href="https://openreview.net/profile?id=~Oriol_Vinyals1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oriol_Vinyals1">Oriol Vinyals</a>, <a href="https://openreview.net/profile?id=~Joao_Carreira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joao_Carreira1">Joao Carreira</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fILj7WpI-g-details-292" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fILj7WpI-g-details-292"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Perceiver, BERT, natural language processing, optical flow, computer vision, multimodal, GLUE, ImageNet, StarCraft</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain &amp; task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=fILj7WpI-g&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="POvMvLi91f" data-number="4011">
        <h4>
          <a href="https://openreview.net/forum?id=POvMvLi91f">
              DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=POvMvLi91f" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aviral_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aviral_Kumar2">Aviral Kumar</a>, <a href="https://openreview.net/profile?id=~Rishabh_Agarwal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rishabh_Agarwal2">Rishabh Agarwal</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tengyu_Ma1">Tengyu Ma</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aaron_Courville3">Aaron Courville</a>, <a href="https://openreview.net/profile?id=~George_Tucker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Tucker1">George Tucker</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#POvMvLi91f-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="POvMvLi91f-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Q-learning, offline RL, regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive aliasing, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that implicit regularization effects can lead to poor performance in value-based offline RL and propose an explicit regularizer to mitigate these effects.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="iMSjopcOn0p" data-number="3989">
        <h4>
          <a href="https://openreview.net/forum?id=iMSjopcOn0p">
              MT3: Multi-Task Multitrack Music Transcription
          </a>
        
          
            <a href="https://openreview.net/pdf?id=iMSjopcOn0p" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Joshua_P_Gardner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_P_Gardner1">Joshua P Gardner</a>, <a href="https://openreview.net/profile?id=~Ian_Simon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ian_Simon1">Ian Simon</a>, <a href="https://openreview.net/profile?id=~Ethan_Manilow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ethan_Manilow1">Ethan Manilow</a>, <a href="https://openreview.net/profile?id=~Curtis_Hawthorne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Curtis_Hawthorne1">Curtis Hawthorne</a>, <a href="https://openreview.net/profile?id=~Jesse_Engel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jesse_Engel1">Jesse Engel</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#iMSjopcOn0p-details-869" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iMSjopcOn0p-details-869"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">music transcription, transformer, multi-task learning, low resource learning, music understanding, music information retrieval</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information. Further, many AMT datasets are ``low-resource'', as even expert musicians find music transcription difficult and time-consuming. Thus, prior work has focused on task-specific architectures, tailored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Unified framework for music transcription, jointly training a single model on six multi-instrument datasets and establishing a new SOTA for low-resource music transcription.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="nHpzE7DqAnG" data-number="3935">
        <h4>
          <a href="https://openreview.net/forum?id=nHpzE7DqAnG">
              Does your graph need a confidence boost?  Convergent boosted smoothing on graphs with tabular node features
          </a>
        
          
            <a href="https://openreview.net/pdf?id=nHpzE7DqAnG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiuhai_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiuhai_Chen1">Jiuhai Chen</a>, <a href="https://openreview.net/profile?id=~Jonas_Mueller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonas_Mueller1">Jonas Mueller</a>, <a href="https://openreview.net/profile?id=~Vassilis_N._Ioannidis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vassilis_N._Ioannidis1">Vassilis N. Ioannidis</a>, <a href="https://openreview.net/profile?id=~Soji_Adeshina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Soji_Adeshina1">Soji Adeshina</a>, <a href="https://openreview.net/profile?id=~Yangkun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yangkun_Wang1">Yangkun Wang</a>, <a href="https://openreview.net/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tom_Goldstein1">Tom Goldstein</a>, <a href="https://openreview.net/profile?id=~David_Wipf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Wipf1">David Wipf</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Apr 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#nHpzE7DqAnG-details-700" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nHpzE7DqAnG-details-700"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Network, Boosting, Node classification, Tabular Data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.).  In this setting boosted decision trees and related ensembling techniques generally dominate real-world applications involving iid training/test sets.  However, when there are relations between samples and the iid assumption is no longer reasonable, it remains unclear how to incorporate these dependencies within existing boosting pipelines.  To this end, we propose a generalized framework for combining boosted trees and more general model ensembling techniques, with graph propagation layers that share  node/sample information across edges connecting related samples.  And unlike previous efforts to integrate graph-based models with boosting, our approach is anchored to a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of benchmarks involving non-iid graph data with tabular node features, our framework achieves comparable or superior performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a convergent method for combining boosting and graph propagation layers. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_xwr8gOBeV1" data-number="3929">
        <h4>
          <a href="https://openreview.net/forum?id=_xwr8gOBeV1">
              Geometric and Physical Quantities improve E(3) Equivariant Message Passing
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_xwr8gOBeV1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Johannes_Brandstetter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johannes_Brandstetter1">Johannes Brandstetter</a>, <a href="https://openreview.net/profile?id=~Rob_Hesselink1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rob_Hesselink1">Rob Hesselink</a>, <a href="https://openreview.net/profile?id=~Elise_van_der_Pol1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elise_van_der_Pol1">Elise van der Pol</a>, <a href="https://openreview.net/profile?id=~Erik_J_Bekkers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Erik_J_Bekkers1">Erik J Bekkers</a>, <a href="https://openreview.net/profile?id=~Max_Welling1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Max_Welling1">Max Welling</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_xwr8gOBeV1-details-942" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_xwr8gOBeV1-details-942"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">equivariant graph neural networks, steerable message passing, non-linear convolutions, molecular modeling, covariant information</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="33" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></mjx-assistive-mml></mjx-container>) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. Our model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions.
        Through the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We generalise equivariant graph networks such that node and edge updates are able to leverage covariant information.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_xwr8gOBeV1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="l3SDgUh7qZO" data-number="3925">
        <h4>
          <a href="https://openreview.net/forum?id=l3SDgUh7qZO">
              SphereFace2: Binary Classification is All You Need for Deep Face Recognition
          </a>
        
          
            <a href="https://openreview.net/pdf?id=l3SDgUh7qZO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yandong_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yandong_Wen1">Yandong Wen</a>, <a href="https://openreview.net/profile?id=~Weiyang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weiyang_Liu1">Weiyang Liu</a>, <a href="https://openreview.net/profile?id=~Adrian_Weller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adrian_Weller1">Adrian Weller</a>, <a href="https://openreview.net/profile?id=~Bhiksha_Raj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhiksha_Raj1">Bhiksha Raj</a>, <a href="https://openreview.net/profile?id=~Rita_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rita_Singh1">Rita Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#l3SDgUh7qZO-details-636" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l3SDgUh7qZO-details-636"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we start by identifying the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the "competitive" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this "one-vs-all" binary classification framework so that it can outperform current competitive methods. Our experiments on popular benchmarks demonstrate that SphereFace2 can consistently outperform state-of-the-art deep face recognition methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel deep face recognition framework</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="mHu2vIds_-b" data-number="3885">
        <h4>
          <a href="https://openreview.net/forum?id=mHu2vIds_-b">
              Boosting Randomized Smoothing with Variance Reduced Classifiers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=mHu2vIds_-b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mikl%C3%B3s_Z._Horv%C3%A1th1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~MiklÃ³s_Z._HorvÃ¡th1">MiklÃ³s Z. HorvÃ¡th</a>, <a href="https://openreview.net/profile?id=~Mark_Niklas_Mueller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mark_Niklas_Mueller2">Mark Niklas Mueller</a>, <a href="https://openreview.net/profile?id=~Marc_Fischer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marc_Fischer1">Marc Fischer</a>, <a href="https://openreview.net/profile?id=~Martin_Vechev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martin_Vechev1">Martin Vechev</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#mHu2vIds_-b-details-255" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mHu2vIds_-b-details-255"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial robustness, certified robustness, randomized smoothing</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Randomized Smoothing (RS) is a promising method for obtaining robustness certiï¬cates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically conï¬rm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to signiï¬cantly more consistent classiï¬cations for a given input. This, in turn, leads to substantially increased certiï¬able radii for samples close to the decision boundary. Additionally, we introduce key optimizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classiï¬ers consistently improve on their strongest constituting model with respect to their average certiï¬ed radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show -- theoretically and empirically -- that ensembles reduce variance under randomized smoothing, yielding higher certified accuracy, leading to a new state-of-the-art on CIFAR-10 and ImageNet.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=mHu2vIds_-b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="t5EmXZ3ZLR" data-number="3848">
        <h4>
          <a href="https://openreview.net/forum?id=t5EmXZ3ZLR">
              SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=t5EmXZ3ZLR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Manuel_Nonnenmacher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Manuel_Nonnenmacher1">Manuel Nonnenmacher</a>, <a href="https://openreview.net/profile?id=~Thomas_Pfeil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Pfeil1">Thomas Pfeil</a>, <a href="https://openreview.net/profile?id=~Ingo_Steinwart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingo_Steinwart1">Ingo Steinwart</a>, <a href="https://openreview.net/profile?id=~David_Reeb2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Reeb2">David Reeb</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#t5EmXZ3ZLR-details-173" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="t5EmXZ3ZLR-details-173"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Structured Pruning, Saliency-based Pruning, Network Compression, Hessian Approximation, Neural Architecture Search, Deep Learning, Computer Vision</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a second-order structured pruning method which efficiently captures global correlations among structures of deep neural networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="8Py-W8lSUgy" data-number="3837">
        <h4>
          <a href="https://openreview.net/forum?id=8Py-W8lSUgy">
              Relational Multi-Task Learning: Modeling Relations between Data and Tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=8Py-W8lSUgy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kaidi_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaidi_Cao1">Kaidi Cao</a>, <a href="https://openreview.net/profile?id=~Jiaxuan_You2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiaxuan_You2">Jiaxuan You</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jure_Leskovec1">Jure Leskovec</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#8Py-W8lSUgy-details-101" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8Py-W8lSUgy-details-101"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Relational Representation Learning, Multi-task Learning, Meta Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data pointâ€™s labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data pointâ€™s labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a novel relational multi-task learning setting where we leverage data point labels from auxiliary tasks to make more accurate predictions on the new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage labels from auxiliary tasks. The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layerâ€™s weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge label captures the label of a data point on a particular task. Under MetaLink, we reformulate the new task as a link label prediction problem between a data node and a task node. The MetaLink framework provides flexibility to model knowledge transfer from auxiliary task labels to the task of interest. We evaluate MetaLink on 6 benchmark datasets in both biochemical and vision domains. Experiments demonstrate that MetaLink can successfully utilize the relations among different tasks, outperforming the state-of-the-art methods under the proposed relational multi-task learning setting, with up to 27% improvement in ROC AUC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose MetaLink to solve a variety of multi-task learning settings, by constructing a knowledge graph over data points and tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sRZ3GhmegS" data-number="3836">
        <h4>
          <a href="https://openreview.net/forum?id=sRZ3GhmegS">
              CoBERL: Contrastive BERT for Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sRZ3GhmegS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrea_Banino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrea_Banino1">Andrea Banino</a>, <a href="https://openreview.net/profile?id=~Adria_Puigdomenech_Badia2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adria_Puigdomenech_Badia2">Adria Puigdomenech Badia</a>, <a href="https://openreview.net/profile?id=~Jacob_C_Walker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_C_Walker1">Jacob C Walker</a>, <a href="https://openreview.net/profile?id=~Tim_Scholtes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tim_Scholtes1">Tim Scholtes</a>, <a href="https://openreview.net/profile?id=~Jovana_Mitrovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jovana_Mitrovic1">Jovana Mitrovic</a>, <a href="https://openreview.net/profile?id=~Charles_Blundell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Charles_Blundell1">Charles Blundell</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sRZ3GhmegS-details-737" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sRZ3GhmegS-details-737"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Contrastive Learning, Representation Learning, Transformer, Deep Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new loss and an improved architecture to efficiently train attentional models in reinforcement learning. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=sRZ3GhmegS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qwBK94cP1y" data-number="3826">
        <h4>
          <a href="https://openreview.net/forum?id=qwBK94cP1y">
              Optimal Transport for Causal Discovery
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qwBK94cP1y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ruibo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruibo_Tu1">Ruibo Tu</a>, <a href="https://openreview.net/profile?id=~Kun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kun_Zhang1">Kun Zhang</a>, <a href="https://openreview.net/profile?id=~Hedvig_Kjellstrom1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hedvig_Kjellstrom1">Hedvig Kjellstrom</a>, <a href="https://openreview.net/profile?id=~Cheng_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng_Zhang1">Cheng Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qwBK94cP1y-details-623" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qwBK94cP1y-details-623"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">causal discovery, optimal transport, functional causal model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To determine causal relationships between two variables, approaches based on Functional Causal Models (FCMs) have been proposed by properly restricting model classes; however, the performance is sensitive to the model assumptions, which makes it difficult to use. In this paper, we provide a novel dynamical-system view of FCMs and propose a new framework for identifying causal direction in the bivariate case. We first show the connection between FCMs and optimal transport, and then study optimal transport under the constraints of FCMs. Furthermore, by exploiting the dynamical interpretation of optimal transport under the FCM constraints, we determine the corresponding underlying dynamical process of the static cause-effect pair data. It provides a new dimension for describing static causal discovery tasks while enjoying more freedom for modeling the quantitative causal influences. In particular, we show that Additive Noise Models (ANMs) correspond to volume-preserving pressureless flows. Consequently, based on their velocity field divergence, we introduce a criterion for determining causal direction. With this criterion, we propose a novel optimal transport-based algorithm for ANMs which is robust to the choice of models and extend it to post-nonlinear models. Our method demonstrated state-of-the-art results on both synthetic and causal discovery benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=qwBK94cP1y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="I1hQbx10Kxn" data-number="3797">
        <h4>
          <a href="https://openreview.net/forum?id=I1hQbx10Kxn">
              On Bridging Generic and Personalized Federated Learning for Image Classification
          </a>
        
          
            <a href="https://openreview.net/pdf?id=I1hQbx10Kxn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hong-You_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hong-You_Chen1">Hong-You Chen</a>, <a href="https://openreview.net/profile?id=~Wei-Lun_Chao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei-Lun_Chao1">Wei-Lun Chao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#I1hQbx10Kxn-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I1hQbx10Kxn-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, personalization, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from each other. This divergence further leads to a dilemma: "Should we prioritize the learned model's generic performance (for future use at the server) or its personalized performance (for each client)?" These two, seemingly competing goals have divided the community to focus on one or the other, yet in this paper we show that it is possible to approach both at the same time. Concretely, we propose a novel federated learning framework that explicitly decouples a model's dual duties with two prediction tasks. On the one hand, we introduce a family of losses that are robust to non-identical class distributions, enabling clients to train a generic predictor with a consistent objective across them. On the other hand, we formulate the personalized predictor as a lightweight adaptive module that is learned to minimize each client's empirical risk on top of the generic predictor. With this two-loss, two-predictor framework which we name Federated Robust Decoupling (Fed-RoD), the learned model can simultaneously achieve state-of-the-art generic and personalized performance, essentially bridging the two tasks. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4-D6CZkRXxI" data-number="3741">
        <h4>
          <a href="https://openreview.net/forum?id=4-D6CZkRXxI">
              Value Gradient weighted Model-Based Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4-D6CZkRXxI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Claas_A_Voelcker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Claas_A_Voelcker1">Claas A Voelcker</a>, <a href="https://openreview.net/profile?id=~Victor_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Liao1">Victor Liao</a>, <a href="https://openreview.net/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Animesh_Garg1">Animesh Garg</a>, <a href="https://openreview.net/profile?id=~Amir-massoud_Farahmand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir-massoud_Farahmand1">Amir-massoud Farahmand</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4-D6CZkRXxI-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4-D6CZkRXxI-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, reinforcment learning, objective mismatch, value function, sensitivity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the Value-gradient weighted Model loss, a method for value-aware model learning in challenging settings, such as small model capacity and the presence of distracting state dimensions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=4-D6CZkRXxI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-llS6TiOew" data-number="3684">
        <h4>
          <a href="https://openreview.net/forum?id=-llS6TiOew">
              Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-llS6TiOew" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ada_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ada_Wan1">Ada Wan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">58 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-llS6TiOew-details-22" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-llS6TiOew-details-22"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, evaluation, multilingual NLP / multilinguality, representation learning for language data, statistical comparisons, Double Descent, conditional language modeling, data-centric approach, diversity in AI, morphology, Transformer, meta evaluation, visualization or interpretation of learned representations, character encoding, internationalization and localization, robustness, statistical science for NLP, science in the era of AI/DL (AIxScience), transdisciplinarity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate  the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United Nations Parallel Corpus across 5 data sizes on 3 representation levels --- character, byte, and word. Results show that performance is relative to the representation granularity of each of the languages, not to the language as a whole. On the character and byte levels, we are able to eliminate statistically significant performance disparity, hence demonstrating that a language cannot be intrinsically hard. The disparity that mirrors the morphological complexity hierarchy is shown to be a byproduct of word segmentation. Evidence from data statistics, along with the fact that word segmentation is qualitatively indeterminate, renders a decades-long debate on morphological complexity (unless it is being intentionally modeled in a word-based, meaning-driven context) irrelevant in the context of computing. The intent of our work is to help effect more objectivity and adequacy in evaluation as well as fairness and inclusivity in experimental setup in the area of language and computing so to uphold diversity in Machine Learning and Artificial Intelligence research. Multilinguality is real and relevant in computing not due to canonical, structural linguistic concepts such as morphology or "words" in our minds, but rather standards related to internationalization and localization, such as character encoding --- something which has thus far been sorely overlooked in our discourse and curricula. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate performance disparity in multilingual NLP with Transformer conditional LMs, and find, in the context of computing, morphological complexity to be a byproduct of word segmentation and disparity arising therefrom unwarranted. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YJ1WzgMVsMt" data-number="3625">
        <h4>
          <a href="https://openreview.net/forum?id=YJ1WzgMVsMt">
              Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YJ1WzgMVsMt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Desik_Rengarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Desik_Rengarajan1">Desik Rengarajan</a>, <a href="https://openreview.net/profile?email=gargivaidya%40tamu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="gargivaidya@tamu.edu">Gargi Vaidya</a>, <a href="https://openreview.net/profile?id=~Akshay_Sarvesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Akshay_Sarvesh1">Akshay Sarvesh</a>, <a href="https://openreview.net/profile?id=~Dileep_Kalathil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dileep_Kalathil1">Dileep Kalathil</a>, <a href="https://openreview.net/profile?id=~Srinivas_Shakkottai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Srinivas_Shakkottai1">Srinivas Shakkottai</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YJ1WzgMVsMt-details-433" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YJ1WzgMVsMt-details-433"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Sparse Rewards, Learning from Demonstrations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback.  Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully.  However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame.  This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from.  In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy} for faster and efficient online RL in such sparse reward settings.  The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data.  The key idea is that by obtaining guidance from - not imitating - the offline {data}, LOGO orients its policy in the manner of the sub-optimal {policy}, while yet being able to learn beyond and approach optimality.  We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode.  We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation.  We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of  benchmark environments with sparse rewards {and censored state}.  Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Reinforcement learning in sparse reward environments  using offline guidance. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=YJ1WzgMVsMt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="49A1Y6tRhaq" data-number="3613">
        <h4>
          <a href="https://openreview.net/forum?id=49A1Y6tRhaq">
              Linking Emergent and Natural Languages via Corpus Transfer
          </a>
        
          
            <a href="https://openreview.net/pdf?id=49A1Y6tRhaq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shunyu_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shunyu_Yao1">Shunyu Yao</a>, <a href="https://openreview.net/profile?id=~Mo_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Yu1">Mo Yu</a>, <a href="https://openreview.net/profile?id=~Yang_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Zhang3">Yang Zhang</a>, <a href="https://openreview.net/profile?id=~Karthik_R_Narasimhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_R_Narasimhan1">Karthik R Narasimhan</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chuang_Gan1">Chuang Gan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#49A1Y6tRhaq-details-116" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="49A1Y6tRhaq-details-116"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Emergent Language, Emergent Communication, Transfer Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game framework. As a result, it remains unclear how the emergent languages from these settings connect to natural languages or provide benefits in real-world language processing tasks, where statistical models trained on large text corpora dominate. In this work, we propose a novel way to establish such a link by corpus transfer, i.e. pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters. Our approach showcases non-trivial transfer benefits for two different tasks â€“ language modeling and image captioning. For example, in a low-resource setup (modeling 2 million natural language tokens), pre-training on an emergent language corpus with just 2 million tokens reduces model perplexity by 24.6% on average across ten natural languages. We also introduce a novel metric to predict the transferability of an emergent language by translating emergent messages to natural language captions grounded on the same images. We find that our translation-based metric highly correlates with the downstream performance on modeling natural languages (for instance <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="34" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70C TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï</mi><mo>=</mo><mn>0.83</mn></math></mjx-assistive-mml></mjx-container> on Hebrew), while topographic similarity, a popular metric in previous works, shows surprisingly low correlation (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="35" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70C TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï</mi><mo>=</mo><mn>0.003</mn></math></mjx-assistive-mml></mjx-container>), hinting that simple properties like attribute disentanglement from synthetic domains might not capture the full complexities of natural language. Our findings also indicate potential benefits of moving language emergence forward with natural language resources and models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We find that pre-training on an emergent language corpus improves natural language tasks in a low resource setup, and propose a metric to predict such a transferability.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=49A1Y6tRhaq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wv6g8fWLX2q" data-number="3597">
        <h4>
          <a href="https://openreview.net/forum?id=wv6g8fWLX2q">
              TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wv6g8fWLX2q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuzhou_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuzhou_Chen1">Yuzhou Chen</a>, <a href="https://openreview.net/profile?id=~Ignacio_Segovia-Dominguez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ignacio_Segovia-Dominguez1">Ignacio Segovia-Dominguez</a>, <a href="https://openreview.net/profile?id=~Baris_Coskunuzer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Baris_Coskunuzer1">Baris Coskunuzer</a>, <a href="https://openreview.net/profile?id=~Yulia_Gel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yulia_Gel1">Yulia Gel</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wv6g8fWLX2q-details-263" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wv6g8fWLX2q-details-263"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">topological data analysis, multipersistence, graph convolutional networks, supragraph diffusion, multivariate time series forecasting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GNNs have inherently static architectures, and as a result, do not explicitly account for time dependencies of the encoded knowledge and are limited in their ability to simultaneously infer latent time-conditioned relations among entities. We postulate that such hidden time-conditioned properties may be captured by the tools of multipersistence, i.e, a emerging machinery in topological data analysis which allows us to quantify dynamics of the data shape along multiple geometric dimensions. 
         We make the first step toward integrating the two rising research directions, that is, time-aware deep learning and multipersistence, and propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convolutional Network (TAMP-S2GCNets). We summarize inherent time-conditioned topological properties of the data as time-aware multipersistence Euler-Poincar\'e surface and prove its stability. We then construct a supragraph convolution module which simultaneously accounts for the extracted intra- and inter- spatio-temporal dependencies in the data. Our extensive experiments on highway traffic flow, Ethereum token prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets outperforms the state-of-the-art tools in multivariate time series forecasting tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We make the first step toward integrating two emerging directions, time-aware deep learning and multi-parameter persistence, allowing us to infer latent time-conditioned relations among entities in multivariate time series forecasting tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wv6g8fWLX2q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="K0E_F0gFDgA" data-number="3572">
        <h4>
          <a href="https://openreview.net/forum?id=K0E_F0gFDgA">
              The MultiBERTs: BERT Reproductions for Robustness Analysis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=K0E_F0gFDgA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Thibault_Sellam2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thibault_Sellam2">Thibault Sellam</a>, <a href="https://openreview.net/profile?id=~Steve_Yadlowsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Steve_Yadlowsky1">Steve Yadlowsky</a>, <a href="https://openreview.net/profile?id=~Ian_Tenney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ian_Tenney1">Ian Tenney</a>, <a href="https://openreview.net/profile?id=~Jason_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_Wei1">Jason Wei</a>, <a href="https://openreview.net/profile?id=~Naomi_Saphra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Naomi_Saphra1">Naomi Saphra</a>, <a href="https://openreview.net/profile?id=~Alexander_D%26%23x27%3BAmour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_D&#39;Amour1">Alexander D'Amour</a>, <a href="https://openreview.net/profile?id=~Tal_Linzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tal_Linzen1">Tal Linzen</a>, <a href="https://openreview.net/profile?id=~Jasmijn_Bastings1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jasmijn_Bastings1">Jasmijn Bastings</a>, <a href="https://openreview.net/profile?id=~Iulia_Raluca_Turc1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Iulia_Raluca_Turc1">Iulia Raluca Turc</a>, <a href="https://openreview.net/profile?id=~Jacob_Eisenstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_Eisenstein1">Jacob Eisenstein</a>, <a href="https://openreview.net/profile?id=~Dipanjan_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dipanjan_Das1">Dipanjan Das</a>, <a href="https://openreview.net/profile?id=~Ellie_Pavlick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ellie_Pavlick1">Ellie Pavlick</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#K0E_F0gFDgA-details-311" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="K0E_F0gFDgA-details-311"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pre-trained models, BERT, bootstrapping, hypothesis testing, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce MultiBERTs, 25 BERT checkpoints trained with similar hyper-parameters but different random seeds, and the Multi-Bootstrap, a bootstrapping method for experimental settings that involve multiple models and limited test data.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vSix3HPYKSU" data-number="3519">
        <h4>
          <a href="https://openreview.net/forum?id=vSix3HPYKSU">
              Message Passing Neural PDE Solvers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vSix3HPYKSU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Johannes_Brandstetter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johannes_Brandstetter1">Johannes Brandstetter</a>, <a href="https://openreview.net/profile?id=~Daniel_E._Worrall1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_E._Worrall1">Daniel E. Worrall</a>, <a href="https://openreview.net/profile?id=~Max_Welling1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Max_Welling1">Max Welling</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vSix3HPYKSU-details-817" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vSix3HPYKSU-details-817"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural PDE solvers, message passing, autoregressive models, zero-stability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed, and accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper introduces a message passing neural PDE solver that replaces all heuristically designed components in numerical PDE solvers with backprop-optimized neural function approximators. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ek7PSN7Y77z" data-number="3426">
        <h4>
          <a href="https://openreview.net/forum?id=Ek7PSN7Y77z">
              Multi-Stage Episodic Control for Strategic Exploration in Text Games
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ek7PSN7Y77z" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jens_Tuyls1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jens_Tuyls1">Jens Tuyls</a>, <a href="https://openreview.net/profile?id=~Shunyu_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shunyu_Yao1">Shunyu Yao</a>, <a href="https://openreview.net/profile?id=~Sham_M._Kakade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sham_M._Kakade1">Sham M. Kakade</a>, <a href="https://openreview.net/profile?id=~Karthik_R_Narasimhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_R_Narasimhan1">Karthik R Narasimhan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ek7PSN7Y77z-details-426" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ek7PSN7Y77z-details-426"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, language understanding, text-based games</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a multi-stage approach to playing text games that improves the score on Zork1 from around 40 to 103. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ek7PSN7Y77z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="V3C8p78sDa" data-number="3397">
        <h4>
          <a href="https://openreview.net/forum?id=V3C8p78sDa">
              Exploring the Limits of Large Scale Pre-training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=V3C8p78sDa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Samira_Abnar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samira_Abnar1">Samira Abnar</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Behnam_Neyshabur1">Behnam Neyshabur</a>, <a href="https://openreview.net/profile?id=~Hanie_Sedghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanie_Sedghi1">Hanie Sedghi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#V3C8p78sDa-details-404" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V3C8p78sDa-details-404"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Scaling law, Pre-training, Transfer learning, Large Scale, Vision Transformer, Few Shot, Empirical Investigation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might  observe that improvements in pre-training would transfer favorably to  most downstream tasks. In this work we systematically study this phenomena and establish that, as we increase the upstream accuracy, performance of downstream tasks \emph{saturates}. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance  that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the observed saturation behavior is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, in order to have a better downstream performance, we need to hurt upstream accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We perform a systematic investigation of limits of  large scale pre-training for few-shot and transfer learning in image recognition with a wide range of downstream tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JGO8CvG5S9" data-number="3390">
        <h4>
          <a href="https://openreview.net/forum?id=JGO8CvG5S9">
              Universal Approximation Under Constraints is Possible with Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JGO8CvG5S9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anastasis_Kratsios1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anastasis_Kratsios1">Anastasis Kratsios</a>, <a href="https://openreview.net/profile?id=~Behnoosh_Zamanlooy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Behnoosh_Zamanlooy2">Behnoosh Zamanlooy</a>, <a href="https://openreview.net/profile?id=~Tianlin_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianlin_Liu2">Tianlin Liu</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_DokmaniÄ‡1">Ivan DokmaniÄ‡</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JGO8CvG5S9-details-395" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JGO8CvG5S9-details-395"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Constrained Universal Approximation, Probabilistic Attention, Transformer Networks, Geometric Deep Learning, Measurable Maximum Theorem, Non-Affine Random Projections, Optimal Transport.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many practical problems need the output of a machine learning model to satisfy a set of constraints, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="36" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>.  Nevertheless, there is no known guarantee that classical neural network architectures can exactly encode constraints while simultaneously achieving universality.  We provide a quantitative constrained universal approximation theorem which guarantees that for any non-convex compact set <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="37" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> and any continuous function <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="38" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>:</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup><mo stretchy="false">â†’</mo><mi>K</mi></math></mjx-assistive-mml></mjx-container>, there is a probabilistic transformer <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="39" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.484em; margin-bottom: -0.531em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c5E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>F</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> whose randomized outputs all lie in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="40" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> and whose expected output uniformly approximates <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="41" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>.  Our second main result is a ``deep neural version'' of Berge's Maximum Theorem (1963).  The result guarantees that given an objective function <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="42" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>, a constraint set <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="43" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, and a family of soft constraint sets, there is a probabilistic transformer <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="44" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.484em; margin-bottom: -0.531em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c5E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>F</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> that approximately minimizes <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="45" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container> and whose outputs belong to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="46" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>; moreover, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="47" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.484em; margin-bottom: -0.531em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c5E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>F</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> approximately satisfies the soft constraints.  Our results imply the first universal approximation theorem for classical transformers with exact convex constraint satisfaction.  They also yield that a chart-free universal approximation theorem for Riemannian manifold-valued functions subject to suitable geodesically convex constraints.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide the first universal approximation theorem with exact non-convex constraint satisfaction, and we introduce probabilistic transformer networks to do so.  </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JGO8CvG5S9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hR_SMu8cxCV" data-number="3358">
        <h4>
          <a href="https://openreview.net/forum?id=hR_SMu8cxCV">
              Scaling Laws for Neural Machine Translation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hR_SMu8cxCV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Behrooz_Ghorbani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Behrooz_Ghorbani1">Behrooz Ghorbani</a>, <a href="https://openreview.net/profile?id=~Orhan_Firat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Orhan_Firat1">Orhan Firat</a>, <a href="https://openreview.net/profile?id=~Markus_Freitag2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Markus_Freitag2">Markus Freitag</a>, <a href="https://openreview.net/profile?id=~Ankur_Bapna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ankur_Bapna1">Ankur Bapna</a>, <a href="https://openreview.net/profile?id=~Maxim_Krikun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maxim_Krikun1">Maxim Krikun</a>, <a href="https://openreview.net/profile?id=~Xavier_Garcia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xavier_Garcia1">Xavier Garcia</a>, <a href="https://openreview.net/profile?id=~Ciprian_Chelba2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ciprian_Chelba2">Ciprian Chelba</a>, <a href="https://openreview.net/profile?id=~Colin_Cherry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Colin_Cherry1">Colin Cherry</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hR_SMu8cxCV-details-223" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hR_SMu8cxCV-details-223"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Scaling Laws, Neural Machine Translation, NMT, Model Scaling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide (model) scaling laws for neural machine translation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="8H5bpVwvt5" data-number="3341">
        <h4>
          <a href="https://openreview.net/forum?id=8H5bpVwvt5">
              AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=8H5bpVwvt5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Biwei_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Biwei_Huang1">Biwei Huang</a>, <a href="https://openreview.net/profile?id=~Fan_Feng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fan_Feng2">Fan Feng</a>, <a href="https://openreview.net/profile?id=~Chaochao_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chaochao_Lu1">Chaochao Lu</a>, <a href="https://openreview.net/profile?id=~Sara_Magliacane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sara_Magliacane1">Sara Magliacane</a>, <a href="https://openreview.net/profile?id=~Kun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kun_Zhang1">Kun Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#8H5bpVwvt5-details-303" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8H5bpVwvt5-details-303"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transfer RL, Graphical models, Efficient adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments. Specifically, we leverage a parsimonious graphical representation that characterizes structural relationships over variables in the RL system. Such graphical representations provide a compact way to encode what and where the changes across domains are, and furthermore inform us with a minimal set of changes that one has to consider for the purpose of policy adaptation. We show that by explicitly leveraging this compact representation to encode changes, we can efficiently adapt the policy to the target domain, in which only a few samples are needed and further policy optimization is avoided. We illustrate the efficacy of AdaRL through a series of experiments that vary factors in the observation, transition and reward functions for Cartpole and Atari games.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Efficient policy adaptation across domains by learning a parsimonious graphical representation that encodes changes in a compact way.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GQjaI9mLet" data-number="3282">
        <h4>
          <a href="https://openreview.net/forum?id=GQjaI9mLet">
              Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GQjaI9mLet" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Octavian-Eugen_Ganea1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Octavian-Eugen_Ganea1">Octavian-Eugen Ganea</a>, <a href="https://openreview.net/profile?id=~Xinyuan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xinyuan_Huang1">Xinyuan Huang</a>, <a href="https://openreview.net/profile?id=~Charlotte_Bunne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Charlotte_Bunne1">Charlotte Bunne</a>, <a href="https://openreview.net/profile?id=~Yatao_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yatao_Bian1">Yatao Bian</a>, <a href="https://openreview.net/profile?id=~Regina_Barzilay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Regina_Barzilay1">Regina Barzilay</a>, <a href="https://openreview.net/profile?id=~Tommi_S._Jaakkola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tommi_S._Jaakkola1">Tommi S. Jaakkola</a>, <a href="https://openreview.net/profile?id=~Andreas_Krause1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Krause1">Andreas Krause</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GQjaI9mLet-details-431" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GQjaI9mLet-details-431"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">protein complexes, protein structure, rigid body docking, SE(3) equivariance, graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing  docking software despite not relying on heavy candidate sampling, structure refinement, or templates.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We perform rigid protein docking using a novel independent SE(3)-equivariant message passing mechanism that guarantees the same resulting protein complex independent of the initial placement of the two 3D structures.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0RDcd5Axok" data-number="3260">
        <h4>
          <a href="https://openreview.net/forum?id=0RDcd5Axok">
              Towards a Unified View of Parameter-Efficient Transfer Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0RDcd5Axok" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Junxian_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junxian_He1">Junxian He</a>, <a href="https://openreview.net/profile?id=~Chunting_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chunting_Zhou1">Chunting Zhou</a>, <a href="https://openreview.net/profile?id=~Xuezhe_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xuezhe_Ma1">Xuezhe Ma</a>, <a href="https://openreview.net/profile?id=~Taylor_Berg-Kirkpatrick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Taylor_Berg-Kirkpatrick1">Taylor Berg-Kirkpatrick</a>, <a href="https://openreview.net/profile?id=~Graham_Neubig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Graham_Neubig1">Graham Neubig</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0RDcd5Axok-details-394" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0RDcd5Axok-details-394"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">parameter-efficient transfer learning, unified view, natural language processing</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a unified framework for several state-of-the-art parameter-efficient tuning methods, </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0RDcd5Axok&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="BS49l-B5Bql" data-number="3173">
        <h4>
          <a href="https://openreview.net/forum?id=BS49l-B5Bql">
              GNN-LM: Language Modeling based on Global Contexts via GNN
          </a>
        
          
            <a href="https://openreview.net/pdf?id=BS49l-B5Bql" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuxian_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuxian_Meng1">Yuxian Meng</a>, <a href="https://openreview.net/profile?id=~Shi_Zong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shi_Zong1">Shi Zong</a>, <a href="https://openreview.net/profile?id=~Xiaoya_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaoya_Li2">Xiaoya Li</a>, <a href="https://openreview.net/profile?id=~Xiaofei_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaofei_Sun1">Xiaofei Sun</a>, <a href="https://openreview.net/profile?id=~Tianwei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianwei_Zhang1">Tianwei Zhang</a>, <a href="https://openreview.net/profile?id=~Fei_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fei_Wu1">Fei Wu</a>, <a href="https://openreview.net/profile?id=~Jiwei_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiwei_Li1">Jiwei Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#BS49l-B5Bql-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BS49l-B5Bql-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla  LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="metRpM4Zrcb" data-number="3077">
        <h4>
          <a href="https://openreview.net/forum?id=metRpM4Zrcb">
              Continual Learning with Filter Atom Swapping
          </a>
        
          
            <a href="https://openreview.net/pdf?id=metRpM4Zrcb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zichen_Miao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zichen_Miao1">Zichen Miao</a>, <a href="https://openreview.net/profile?id=~Ze_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ze_Wang3">Ze Wang</a>, <a href="https://openreview.net/profile?id=~Wei_Chen26" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Chen26">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#metRpM4Zrcb-details-360" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="metRpM4Zrcb-details-360"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, we perform continual learning with filter atom swapping. In other words, we learn for each task a new filter subspace for each convolutional layer, i.e., hundreds of parameters as filter atoms, but keep subspace coefficients shared across tasks. By maintaining a small footprint memory of filter atoms, we can easily archive models for past tasks to avoid forgetting. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically. The proposed atom swapping framework further enables flexible and efficient model ensemble with members selected within a task or across tasks to improve the performance in different continual learning settings. Being validated on multiple benchmark datasets with different convolutional network structures, the proposed method outperforms the state-of-the-art methods in both accuracy and scalability.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7YDLgf9_zgm" data-number="3027">
        <h4>
          <a href="https://openreview.net/forum?id=7YDLgf9_zgm">
              Continual Learning with Recursive Gradient Optimization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7YDLgf9_zgm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hao_Liu18" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Liu18">Hao Liu</a>, <a href="https://openreview.net/profile?id=~Huaping_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huaping_Liu1">Huaping Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7YDLgf9_zgm-details-761" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7YDLgf9_zgm-details-761"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continual learning, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning(CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer(FEL) that represents different long-term structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance on 20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher average accuracy than Single-Task Learning(STL), this method is flexible and reliable to provide continual learning capabilities for learning models that rely on gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper proposes a novel method for continual learning in a fixed capacity network in the non-replay regime, which minimizes the loss on the current task while also minimizing an upper bound of loss increment on previous tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=7YDLgf9_zgm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade   active in" id="poster-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="HndgQudNb91" data-number="4722">
        <h4>
          <a href="https://openreview.net/forum?id=HndgQudNb91">
              Learning to Downsample for Segmentation of Ultra-High Resolution Images
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HndgQudNb91" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chen_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen_Jin3">Chen Jin</a>, <a href="https://openreview.net/profile?id=~Ryutaro_Tanno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryutaro_Tanno1">Ryutaro Tanno</a>, <a href="https://openreview.net/profile?id=~Thomy_Mertzanidou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomy_Mertzanidou1">Thomy Mertzanidou</a>, <a href="https://openreview.net/profile?id=~Eleftheria_Panagiotaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleftheria_Panagiotaki1">Eleftheria Panagiotaki</a>, <a href="https://openreview.net/profile?id=~Daniel_C._Alexander1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_C._Alexander1">Daniel C. Alexander</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HndgQudNb91-details-623" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HndgQudNb91-details-623"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ultra-high resolution image segmentation, non-uniform dowmsampling, efficient segmentation, large volume image segmentation, medical image segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance
        because the segmentation difficulty varies spatially (see Figure 1 â€œUniformâ€). We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling
        module learns to sample more densely at difficult locations, thereby improving the segmentation performance (see Figure 1 "Ours"). Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method for learning to downsample ultra high-resolution images that reflects the importance of each location.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7fFO4cMBx_9" data-number="4721">
        <h4>
          <a href="https://openreview.net/forum?id=7fFO4cMBx_9">
              Variational Neural Cellular Automata
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7fFO4cMBx_9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rasmus_Berg_Palm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rasmus_Berg_Palm1">Rasmus Berg Palm</a>, <a href="https://openreview.net/profile?id=~Miguel_Gonz%C3%A1lez_Duque1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miguel_GonzÃ¡lez_Duque1">Miguel GonzÃ¡lez Duque</a>, <a href="https://openreview.net/profile?id=~Shyam_Sudhakaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shyam_Sudhakaran1">Shyam Sudhakaran</a>, <a href="https://openreview.net/profile?id=~Sebastian_Risi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sebastian_Risi1">Sebastian Risi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7fFO4cMBx_9-details-481" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7fFO4cMBx_9-details-481"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Cellular Automata, Cellular Automata, Self-Organization, Generative Models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process.
        Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, the self-organizing nature bestows the VNCA with some inherent robustness against perturbations in the early stages of growth.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose and evaluate the Variational Neural Cellular Automata, a self-organising generative model based on neural cellular automata</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FKp8-pIRo3y" data-number="4719">
        <h4>
          <a href="https://openreview.net/forum?id=FKp8-pIRo3y">
              Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FKp8-pIRo3y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Todor_Davchev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Todor_Davchev1">Todor Davchev</a>, <a href="https://openreview.net/profile?id=~Oleg_Olegovich_Sushkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oleg_Olegovich_Sushkov1">Oleg Olegovich Sushkov</a>, <a href="https://openreview.net/profile?id=~Jean-Baptiste_Regli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jean-Baptiste_Regli1">Jean-Baptiste Regli</a>, <a href="https://openreview.net/profile?id=~Stefan_Schaal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefan_Schaal1">Stefan Schaal</a>, <a href="https://openreview.net/profile?id=~Yusuf_Aytar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yusuf_Aytar1">Yusuf Aytar</a>, <a href="https://openreview.net/profile?id=~Markus_Wulfmeier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Markus_Wulfmeier1">Markus Wulfmeier</a>, <a href="https://openreview.net/profile?id=~Jon_Scholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jon_Scholz1">Jon Scholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 08 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FKp8-pIRo3y-details-595" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FKp8-pIRo3y-details-595"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">goal-conditioned reinforcement learning, learning from demonstrations, long-horizon dexterous manipulation, bi-manual manipulation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. 
        Various tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KntaNRo6R48" data-number="4717">
        <h4>
          <a href="https://openreview.net/forum?id=KntaNRo6R48">
              L0-Sparse Canonical Correlation Analysis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KntaNRo6R48" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ofir_Lindenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Lindenbaum1">Ofir Lindenbaum</a>, <a href="https://openreview.net/profile?id=~Moshe_Salhov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Moshe_Salhov1">Moshe Salhov</a>, <a href="https://openreview.net/profile?id=~Amir_Averbuch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Averbuch1">Amir Averbuch</a>, <a href="https://openreview.net/profile?id=~Yuval_Kluger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuval_Kluger1">Yuval Kluger</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 07 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KntaNRo6R48-details-372" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KntaNRo6R48-details-372"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \textit{canonically correlated variates}. Here, we propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="48" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities.
        Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="49" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-regularized correlation loss. 
        We further propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="50" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="51" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-CCA method for learning correlated representations based on sparse subsets of variables from two observed modalities.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=KntaNRo6R48&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="B7ZbqNLDn-_" data-number="4715">
        <h4>
          <a href="https://openreview.net/forum?id=B7ZbqNLDn-_">
              Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=B7ZbqNLDn-_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheikh_Shams_Azam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheikh_Shams_Azam1">Sheikh Shams Azam</a>, <a href="https://openreview.net/profile?id=~Seyyedali_Hosseinalipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyyedali_Hosseinalipour1">Seyyedali Hosseinalipour</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Christopher_Brinton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Brinton1">Christopher Brinton</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#B7ZbqNLDn-_-details-224" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B7ZbqNLDn-_-details-224"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Distributed Machine Learning, Federated Learning, Gradient Subspace, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance. Motivated by this, we propose the "Look-back Gradient Multiplier" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning. Operationally, LBGM recycles the gradients between model update rounds to significantly reduce the number of parameters to be propagated through the system. We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We observe that "gradient-space is low rank" and propose the LBGM algorithm that utilitizes this low-rank property to recycle gradients between model update rounds in federated learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ucASPPD9GKN" data-number="4711">
        <h4>
          <a href="https://openreview.net/forum?id=ucASPPD9GKN">
              Is Homophily a Necessity for Graph Neural Networks?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ucASPPD9GKN" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yao_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yao_Ma3">Yao Ma</a>, <a href="https://openreview.net/profile?id=~Xiaorui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaorui_Liu1">Xiaorui Liu</a>, <a href="https://openreview.net/profile?id=~Neil_Shah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Neil_Shah2">Neil Shah</a>, <a href="https://openreview.net/profile?id=~Jiliang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiliang_Tang1">Jiliang Tang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ucASPPD9GKN-details-93" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ucASPPD9GKN-details-93"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification,  GNNs are widely believed to work well due to the homophily assumption (``like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance.  We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations.  Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ucASPPD9GKN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ve0Wth3ptT_" data-number="4703">
        <h4>
          <a href="https://openreview.net/forum?id=Ve0Wth3ptT_">
              DEGREE: Decomposition Based Explanation for Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ve0Wth3ptT_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qizhang_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qizhang_Feng1">Qizhang Feng</a>, <a href="https://openreview.net/profile?id=~Ninghao_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ninghao_Liu2">Ninghao Liu</a>, <a href="https://openreview.net/profile?id=~Fan_Yang27" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fan_Yang27">Fan Yang</a>, <a href="https://openreview.net/profile?id=~Ruixiang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruixiang_Tang1">Ruixiang Tang</a>, <a href="https://openreview.net/profile?id=~Mengnan_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengnan_Du1">Mengnan Du</a>, <a href="https://openreview.net/profile?id=~Xia_Hu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xia_Hu4">Xia Hu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ve0Wth3ptT_-details-621" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ve0Wth3ptT_-details-621"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">XAI, GNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new decomposition based explanation for Graph Neural Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="T0B9AoM_bFg" data-number="4668">
        <h4>
          <a href="https://openreview.net/forum?id=T0B9AoM_bFg">
              Improving Mutual Information Estimation with Annealed and Energy-Based Bounds
          </a>
        
          
            <a href="https://openreview.net/pdf?id=T0B9AoM_bFg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rob_Brekelmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rob_Brekelmans1">Rob Brekelmans</a>, <a href="https://openreview.net/profile?id=~Sicong_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sicong_Huang1">Sicong Huang</a>, <a href="https://openreview.net/profile?id=~Marzyeh_Ghassemi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marzyeh_Ghassemi2">Marzyeh Ghassemi</a>, <a href="https://openreview.net/profile?id=~Greg_Ver_Steeg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Greg_Ver_Steeg1">Greg Ver Steeg</a>, <a href="https://openreview.net/profile?id=~Roger_Baker_Grosse1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roger_Baker_Grosse1">Roger Baker Grosse</a>, <a href="https://openreview.net/profile?id=~Alireza_Makhzani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Makhzani1">Alireza Makhzani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#T0B9AoM_bFg-details-82" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T0B9AoM_bFg-details-82"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">mutual information estimation, annealed importance sampling, energy-based models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We derive new annealed importance sampling and energy-based bounds, resulting in vastly more accurate estimates of mutual information.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bp-LJ4y_XC" data-number="4662">
        <h4>
          <a href="https://openreview.net/forum?id=bp-LJ4y_XC">
              Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bp-LJ4y_XC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xueyuan_She1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xueyuan_She1">Xueyuan She</a>, <a href="https://openreview.net/profile?id=~Saurabh_Dash1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Dash1">Saurabh Dash</a>, <a href="https://openreview.net/profile?id=~Saibal_Mukhopadhyay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saibal_Mukhopadhyay1">Saibal Mukhopadhyay</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bp-LJ4y_XC-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bp-LJ4y_XC-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spiking neural network, spatiotemporal processing, feedforward network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A theoretical approache to study the approximation capability of feedforward spiking neural network and optimization methods for such network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=bp-LJ4y_XC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="nwKXyFvaUm" data-number="4660">
        <h4>
          <a href="https://openreview.net/forum?id=nwKXyFvaUm">
              Diverse Client Selection for Federated Learning via Submodular Maximization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=nwKXyFvaUm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ravikumar_Balakrishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ravikumar_Balakrishnan1">Ravikumar Balakrishnan</a>, <a href="https://openreview.net/profile?id=~Tian_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tian_Li1">Tian Li</a>, <a href="https://openreview.net/profile?id=~Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyi_Zhou1">Tianyi Zhou</a>, <a href="https://openreview.net/profile?email=nageen.himayat%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="nageen.himayat@intel.com">Nageen Himayat</a>, <a href="https://openreview.net/profile?id=~Virginia_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Virginia_Smith1">Virginia Smith</a>, <a href="https://openreview.net/profile?id=~Jeff_Bilmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeff_Bilmes1">Jeff Bilmes</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#nwKXyFvaUm-details-873" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nwKXyFvaUm-details-873"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, submodularity, diversity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In every communication round of federated learning, a random subset of clients communicate  their  model  updates  back  to  the  server  which  then  aggregates them all.  The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server.  Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce â€œfederated averaging with diverse client selection (DivFL)â€. We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper addresses a key challenge of selecting the most representative clients iteratively for federated learning through formulating it as a submodular optimization problem and developing efficient algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="jT1EwXu-4hj" data-number="4651">
        <h4>
          <a href="https://openreview.net/forum?id=jT1EwXu-4hj">
              From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=jT1EwXu-4hj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Da_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Da_Xu2">Da Xu</a>, <a href="https://openreview.net/profile?id=~Yuting_Ye3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuting_Ye3">Yuting Ye</a>, <a href="https://openreview.net/profile?id=~Chuanwei_Ruan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chuanwei_Ruan1">Chuanwei Ruan</a>, <a href="https://openreview.net/profile?id=~Evren_Korpeoglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Evren_Korpeoglu1">Evren Korpeoglu</a>, <a href="https://openreview.net/profile?id=~Sushant_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sushant_Kumar1">Sushant Kumar</a>, <a href="https://openreview.net/profile?id=~Kannan_Achan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kannan_Achan1">Kannan Achan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#jT1EwXu-4hj-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jT1EwXu-4hj-details-997"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Information retrieval, Learning theory, Causal inference, Missing data, Overlapping, Reweighting, Optimal transport</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game.
        We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose and study a novel domain-transportation view for optimizing recommendation for information retrieval systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=jT1EwXu-4hj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JxFgJbZ-wft" data-number="4647">
        <h4>
          <a href="https://openreview.net/forum?id=JxFgJbZ-wft">
              Variational Predictive Routing with Nested Subjective Timescales
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JxFgJbZ-wft" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexey_Zakharov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_Zakharov1">Alexey Zakharov</a>, <a href="https://openreview.net/profile?id=~Qinghai_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghai_Guo1">Qinghai Guo</a>, <a href="https://openreview.net/profile?id=~Zafeirios_Fountas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zafeirios_Fountas1">Zafeirios Fountas</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JxFgJbZ-wft-details-666" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JxFgJbZ-wft-details-666"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hierarchical temporal abstraction, event discovery, hierarchical generative models, variational inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) â€“ a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the systemâ€™s latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the modelâ€™s latent hierarchy.  Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Variational inference hierarchical model that relies on a change detection mechanism to impose a nested temporal hierarchy on its latent structure.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RhB1AdoFfGE" data-number="4630">
        <h4>
          <a href="https://openreview.net/forum?id=RhB1AdoFfGE">
              Sample and Computation Redistribution for Efficient Face Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RhB1AdoFfGE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jia_Guo1">Jia Guo</a>, <a href="https://openreview.net/profile?id=~Jiankang_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiankang_Deng1">Jiankang Deng</a>, <a href="https://openreview.net/profile?id=~Alexandros_Lattas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexandros_Lattas1">Alexandros Lattas</a>, <a href="https://openreview.net/profile?id=~Stefanos_Zafeiriou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanos_Zafeiriou1">Stefanos Zafeiriou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RhB1AdoFfGE-details-180" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RhB1AdoFfGE-details-180"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">efficient face detection, computation redistribution, sample redistribution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages. The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="52" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4.78</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> (AP at hard set) while being more than 3<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="53" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> faster on GPUs with VGA-resolution images. Code is available at: https://github.com/deepinsight/insightface/tree/master/detection/scrfd.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We search for optimised computation distribution and training sample distribution for the task of face detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=RhB1AdoFfGE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NkZq4OEYN-" data-number="4629">
        <h4>
          <a href="https://openreview.net/forum?id=NkZq4OEYN-">
              Sound Adversarial Audio-Visual Navigation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NkZq4OEYN-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yinfeng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yinfeng_Yu1">Yinfeng Yu</a>, <a href="https://openreview.net/profile?id=~Wenbing_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wenbing_Huang1">Wenbing Huang</a>, <a href="https://openreview.net/profile?id=~Fuchun_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuchun_Sun2">Fuchun Sun</a>, <a href="https://openreview.net/profile?id=~Changan_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Changan_Chen2">Changan Chen</a>, <a href="https://openreview.net/profile?id=~Yikai_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yikai_Wang2">Yikai Wang</a>, <a href="https://openreview.net/profile?id=~Xiaohong_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaohong_Liu3">Xiaohong Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NkZq4OEYN--details-929" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NkZq4OEYN--details-929"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN .</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work aims to do an adversarial sound intervention for robust audio-visual navigation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NkZq4OEYN-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="12RoR2o32T" data-number="4618">
        <h4>
          <a href="https://openreview.net/forum?id=12RoR2o32T">
              Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=12RoR2o32T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aahlad_Manas_Puli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aahlad_Manas_Puli1">Aahlad Manas Puli</a>, <a href="https://openreview.net/profile?id=~Lily_H_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lily_H_Zhang1">Lily H Zhang</a>, <a href="https://openreview.net/profile?id=~Eric_Karl_Oermann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eric_Karl_Oermann1">Eric Karl Oermann</a>, <a href="https://openreview.net/profile?id=~Rajesh_Ranganath2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rajesh_Ranganath2">Rajesh Ranganath</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#12RoR2o32T-details-484" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="12RoR2o32T-details-484"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spurious correlations, out of distribution generalization, ml for health, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper build models robust to nuisance-induced spurious correlations by constructing a representation that distills out the influence of the nuisance variables, while also maximizing its information with the label.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="OM_lYiHXiCL" data-number="4615">
        <h4>
          <a href="https://openreview.net/forum?id=OM_lYiHXiCL">
              AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=OM_lYiHXiCL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Junfeng_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junfeng_Guo2">Junfeng Guo</a>, <a href="https://openreview.net/profile?id=~Ang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ang_Li1">Ang Li</a>, <a href="https://openreview.net/profile?id=~Cong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cong_Liu2">Cong Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 25 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#OM_lYiHXiCL-details-712" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OM_lYiHXiCL-details-712"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples,  which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution;  a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ECQL05ub0J" data-number="4609">
        <h4>
          <a href="https://openreview.net/forum?id=5ECQL05ub0J">
              Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ECQL05ub0J" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kirby_Banman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kirby_Banman1">Kirby Banman</a>, <a href="https://openreview.net/profile?id=~Garnet_Liam_Peet-Pare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Garnet_Liam_Peet-Pare1">Garnet Liam Peet-Pare</a>, <a href="https://openreview.net/profile?id=~Nidhi_Hegde1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nidhi_Hegde1">Nidhi Hegde</a>, <a href="https://openreview.net/profile?id=~Alona_Fyshe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alona_Fyshe1">Alona Fyshe</a>, <a href="https://openreview.net/profile?id=~Martha_White1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martha_White1">Martha White</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ECQL05ub0J-details-327" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ECQL05ub0J-details-327"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">optimization, momentum, stochastic gradient descent, non-iid sampling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid  sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that SGDm under covariate shift with fixed step-size can be unstable and diverge due to a phenomenon known as parametric resonance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ECQL05ub0J&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WqoBaaPHS-" data-number="4592">
        <h4>
          <a href="https://openreview.net/forum?id=WqoBaaPHS-">
              Top-label calibration and multiclass-to-binary reductions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WqoBaaPHS-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chirag_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chirag_Gupta1">Chirag Gupta</a>, <a href="https://openreview.net/profile?id=~Aaditya_Ramdas2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aaditya_Ramdas2">Aaditya Ramdas</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WqoBaaPHS--details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WqoBaaPHS--details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">calibration, multiclass, uncertainty quantification, distribution-free, histogram binning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported and we must condition on what is reported. However, the popular notion of confidence calibration erroneously skips this conditioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction framework that unifies confidence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to different binary calibration problems; various types of multiclass calibration can then be achieved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose top-label calibration, a new and arguably natural notion for multiclass calibration, along with 'wrapper' calibration algorithms that reduce multiclass calibration to binary calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WqoBaaPHS-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JfaWawZ8BmX" data-number="4589">
        <h4>
          <a href="https://openreview.net/forum?id=JfaWawZ8BmX">
              Anisotropic Random Feature Regression in High Dimensions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JfaWawZ8BmX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gabriel_Mel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gabriel_Mel1">Gabriel Mel</a>, <a href="https://openreview.net/profile?id=~Jeffrey_Pennington1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeffrey_Pennington1">Jeffrey Pennington</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JfaWawZ8BmX-details-842" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JfaWawZ8BmX-details-842"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random feature models, high dimensional asymptotics, generalization, learning curves, double descent, multiple descent, alignment</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but typically these analyses have utilized isotropic distributional assumptions on the underlying data generation process, thereby failing to provide a realistic characterization of real-world models that are designed to identify and harness the structure in natural data. In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function. We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment. We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs. Finally, these signatures are related to phase transitions in the spectrum of the feature kernel matrix, and unlike the double descent peak, persist even under optimal regularization.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We derive exact asymptotic formulas for the total error, bias, and variance of random feature regression with anisotropic inputs and target weights, and identify a new type of singularity in sample-wise learning curves. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JfaWawZ8BmX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="L01Nn_VJ9i" data-number="4586">
        <h4>
          <a href="https://openreview.net/forum?id=L01Nn_VJ9i">
              Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future
          </a>
        
          
            <a href="https://openreview.net/pdf?id=L01Nn_VJ9i" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Harshavardhan_Kamarthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Harshavardhan_Kamarthi1">Harshavardhan Kamarthi</a>, <a href="https://openreview.net/profile?id=~Alexander_Rodr%C3%ADguez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_RodrÃ­guez1">Alexander RodrÃ­guez</a>, <a href="https://openreview.net/profile?id=~B._Aditya_Prakash2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~B._Aditya_Prakash2">B. Aditya Prakash</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#L01Nn_VJ9i-details-582" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="L01Nn_VJ9i-details-582"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Epidemic Forecasting, Data revisions, Graph Representation learning, Time Series Forecasting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called â€˜backfillâ€™ phenomenon and its effect on model performance have been barely addressed in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. 
        We construct a detailed dataset composed of relevant signals over the past year of the pandemic. 
        We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting. Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines. In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the problem of multi-variate backfill for both features and targets and show how to leverage our insights for more general neural framework to improve both model predictions and evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=L01Nn_VJ9i&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lrocYB-0ST2" data-number="4570">
        <h4>
          <a href="https://openreview.net/forum?id=lrocYB-0ST2">
              Approximation and Learning with Deep Convolutional Models: a Kernel Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lrocYB-0ST2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alberto_Bietti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alberto_Bietti1">Alberto Bietti</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lrocYB-0ST2-details-901" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lrocYB-0ST2-details-901"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">kernel methods, deep learning theory, convolution, approximation, generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the inductive bias of multi-layer convolutional models through a kernel lens, showing generalization benefits of various architectural choices such as locality, depth, and pooling layers.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lrocYB-0ST2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vgqS1vkkCbE" data-number="4569">
        <h4>
          <a href="https://openreview.net/forum?id=vgqS1vkkCbE">
              Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vgqS1vkkCbE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dhruv_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dhruv_Shah1">Dhruv Shah</a>, <a href="https://openreview.net/profile?id=~Peng_Xu9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Xu9">Peng Xu</a>, <a href="https://openreview.net/profile?id=~Yao_Lu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yao_Lu13">Yao Lu</a>, <a href="https://openreview.net/profile?id=~Ted_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ted_Xiao1">Ted Xiao</a>, <a href="https://openreview.net/profile?id=~Alexander_T_Toshev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_T_Toshev1">Alexander T Toshev</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~brian_ichter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~brian_ichter1">brian ichter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vgqS1vkkCbE-details-455" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vgqS1vkkCbE-details-455"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">hierarchical reinforcement learning, planning, representation learning, robotics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a  representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce value function spaces, a learned representation of state through the values of low-level skills, which capture affordances and ignores distractors to enable long-horizon reasoning and zero-shot generalization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gNp54NxHUPJ" data-number="4544">
        <h4>
          <a href="https://openreview.net/forum?id=gNp54NxHUPJ">
              Fast Regression for Structured Inputs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gNp54NxHUPJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raphael_A_Meyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raphael_A_Meyer1">Raphael A Meyer</a>, <a href="https://openreview.net/profile?id=~Cameron_N_Musco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cameron_N_Musco1">Cameron N Musco</a>, <a href="https://openreview.net/profile?id=~Christopher_P_Musco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_P_Musco1">Christopher P Musco</a>, <a href="https://openreview.net/profile?id=~David_Woodruff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Woodruff1">David Woodruff</a>, <a href="https://openreview.net/profile?id=~Samson_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samson_Zhou1">Samson Zhou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gNp54NxHUPJ-details-253" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gNp54NxHUPJ-details-253"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">regression, sublinear time algorithm, structured input</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="54" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression problem, which requires finding <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="55" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> that minimizes <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="56" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="ORD">âˆ¥</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">A</mi></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>âˆ’</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><msub><mo data-mjx-texclass="ORD">âˆ¥</mo><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> for a matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="57" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">A</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>Ã—</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> and response vector <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="58" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="59" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is very large. However, all known subsampling approaches have run time that depends exponentially on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="60" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>, typically, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="61" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, which can be prohibitively expensive. 
        
        We improve on this work by showing that for a large class of common \emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="62" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression that depend polynomially on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="63" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>. For example, we give an algorithm for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="64" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression on Vandermonde matrices that runs in time <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="65" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-msup space="2"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.421em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D714 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mtext><mjx-mstyle><mjx-mspace style="width: 0.167em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><msup><mi>log</mi><mn>3</mn></msup><mo data-mjx-texclass="NONE">â¡</mo><mi>n</mi><mo>+</mo><mo stretchy="false">(</mo><mi>d</mi><msup><mi>p</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mn>0.5</mn><mo>+</mo><mi>Ï‰</mi></mrow></msup><mo>â‹…</mo><mtext>polylog</mtext><mstyle scriptlevel="0"><mspace width="0.167em"></mspace></mstyle><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="66" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D714 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï‰</mi></math></mjx-assistive-mml></mjx-container> is the exponent of matrix multiplication. The polynomial dependence on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="67" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> crucially allows our algorithms to extend naturally to efficient algorithms for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="68" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi mathvariant="normal">âˆž</mi></msub></math></mjx-assistive-mml></mjx-container> regression, via approximation of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="69" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi mathvariant="normal">âˆž</mi></msub></math></mjx-assistive-mml></mjx-container> by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="70" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub></math></mjx-assistive-mml></mjx-container>. Of practical interest, we also develop a new subsampling algorithm for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="71" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression for arbitrary matrices, which is simpler than previous approaches for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="72" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>â‰¥</mo><mn>4</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=gNp54NxHUPJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qhC8mr2LEKq" data-number="4543">
        <h4>
          <a href="https://openreview.net/forum?id=qhC8mr2LEKq">
              CrossBeam: Learning to Search in Bottom-Up Program Synthesis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qhC8mr2LEKq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kensen_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kensen_Shi1">Kensen Shi</a>, <a href="https://openreview.net/profile?id=~Hanjun_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanjun_Dai1">Hanjun Dai</a>, <a href="https://openreview.net/profile?id=~Kevin_Ellis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kevin_Ellis1">Kevin Ellis</a>, <a href="https://openreview.net/profile?id=~Charles_Sutton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Charles_Sutton1">Charles Sutton</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qhC8mr2LEKq-details-954" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qhC8mr2LEKq-details-954"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Program Synthesis, Bottom-Up Search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose training a neural model to learn a hands-on search policy for bottom-up program synthesis, in an effort to tame the search space blowup.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="M6M8BEmd6dq" data-number="4542">
        <h4>
          <a href="https://openreview.net/forum?id=M6M8BEmd6dq">
              PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=M6M8BEmd6dq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seng_Pei_Liew1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seng_Pei_Liew1">Seng Pei Liew</a>, <a href="https://openreview.net/profile?id=~Tsubasa_Takahashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tsubasa_Takahashi1">Tsubasa Takahashi</a>, <a href="https://openreview.net/profile?id=~Michihiko_Ueno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michihiko_Ueno1">Michihiko Ueno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#M6M8BEmd6dq-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="M6M8BEmd6dq-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Differential Privacy, Generative Model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new framework of synthesizing data using deep generative models in a differentially private manner.
        Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data.
        Hence, no extra privacy costs or model constraints are incurred, in contrast to popular gradient sanitization approaches, which, among other issues, cause degradation in privacy guarantees as the training iteration increases.
        We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well.
        Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aOX3a9q3RVV" data-number="4523">
        <h4>
          <a href="https://openreview.net/forum?id=aOX3a9q3RVV">
              Divisive Feature Normalization Improves Image Recognition Performance in AlexNet
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aOX3a9q3RVV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michelle_Miller3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michelle_Miller3">Michelle Miller</a>, <a href="https://openreview.net/profile?id=~SueYeon_Chung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SueYeon_Chung1">SueYeon Chung</a>, <a href="https://openreview.net/profile?id=~Kenneth_D._Miller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kenneth_D._Miller2">Kenneth D. Miller</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aOX3a9q3RVV-details-776" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aOX3a9q3RVV-details-776"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">divisive normalization, AlexNet, ImageNet, CIFAR-100, manifold capacity, sparsity, receptive fields, Batch Normalization, Group Normalization, Layer Normalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">DIVISIVE FEATURE NORMALIZATION IMPROVES IMAGE RECOGNITION PERFORMANCE AND IN- CREASES MANIFOLD CAPACITY, SPARSITY, AND LOW-FREQUENCY REPRESENTATION IN DEEP NETS</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bTteFbU99ye" data-number="4509">
        <h4>
          <a href="https://openreview.net/forum?id=bTteFbU99ye">
              Evaluating Distributional Distortion in Neural Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bTteFbU99ye" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Benjamin_LeBrun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Benjamin_LeBrun1">Benjamin LeBrun</a>, <a href="https://openreview.net/profile?id=~Alessandro_Sordoni2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alessandro_Sordoni2">Alessandro Sordoni</a>, <a href="https://openreview.net/profile?id=~Timothy_J._O%26%23x27%3BDonnell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Timothy_J._O&#39;Donnell1">Timothy J. O'Donnell</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bTteFbU99ye-details-386" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bTteFbU99ye-details-386"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate.  As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="r5qumLiYwf9" data-number="4501">
        <h4>
          <a href="https://openreview.net/forum?id=r5qumLiYwf9">
              MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining
          </a>
        
          
            <a href="https://openreview.net/pdf?id=r5qumLiYwf9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ahmed_Imtiaz_Humayun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ahmed_Imtiaz_Humayun1">Ahmed Imtiaz Humayun</a>, <a href="https://openreview.net/profile?id=~Randall_Balestriero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Randall_Balestriero1">Randall Balestriero</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#r5qumLiYwf9-details-869" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r5qumLiYwf9-details-869"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Generative Networks, Uniform Sampling, Fairness, Data Augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \&amp; recall by 4.12\% \&amp; 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a differential-geometry-based technique to provably sample uniformly from the data manifold of a trained Deep Generative Network without the need for retraining.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xnYACQquaGV" data-number="4495">
        <h4>
          <a href="https://openreview.net/forum?id=xnYACQquaGV">
              Neural Contextual Bandits with Deep Representation and Shallow Exploration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xnYACQquaGV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pan_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pan_Xu1">Pan Xu</a>, <a href="https://openreview.net/profile?id=~Zheng_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zheng_Wen1">Zheng Wen</a>, <a href="https://openreview.net/profile?id=~Handong_Zhao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Handong_Zhao3">Handong Zhao</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanquan_Gu1">Quanquan Gu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xnYACQquaGV-details-331" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xnYACQquaGV-details-331"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural network, deep representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="73" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.465em; margin-bottom: -0.215em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c7E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.169em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> finite-time regret, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="74" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new neural network based algorithm for contextual bandit problems with theoretical guarantees and empirical advantages.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xnYACQquaGV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NoB8YgRuoFU" data-number="4494">
        <h4>
          <a href="https://openreview.net/forum?id=NoB8YgRuoFU">
              PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NoB8YgRuoFU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyan_Liu1">Siyan Liu</a>, <a href="https://openreview.net/profile?id=~Pei_Zhang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pei_Zhang6">Pei Zhang</a>, <a href="https://openreview.net/profile?id=~Dan_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Lu1">Dan Lu</a>, <a href="https://openreview.net/profile?id=~Guannan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guannan_Zhang1">Guannan Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NoB8YgRuoFU-details-418" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NoB8YgRuoFU-details-418"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NoB8YgRuoFU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kj0_45Y4r9i" data-number="4489">
        <h4>
          <a href="https://openreview.net/forum?id=kj0_45Y4r9i">
              Discriminative Similarity for Data Clustering
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kj0_45Y4r9i" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yingzhen_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yingzhen_Yang1">Yingzhen Yang</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kj0_45Y4r9i-details-324" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kj0_45Y4r9i-details-324"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Discriminative Similarity, Rademacher Complexity, Generalization Bound, Data Clustering</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\em Clustering by  Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel discriminative similarity for data clustering, and the discriminative similarity is induced by generalization error bound for unsupervised classifier </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="q4tZR1Y-UIs" data-number="4485">
        <h4>
          <a href="https://openreview.net/forum?id=q4tZR1Y-UIs">
              It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=q4tZR1Y-UIs" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuqing_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuqing_Du1">Yuqing Du</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Aditya_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Grover1">Aditya Grover</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#q4tZR1Y-UIs-details-931" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q4tZR1Y-UIs-details-931"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">curriculum generation, unsupervised reinforcement learning, goal conditioned reinforcement learning, multi agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=q4tZR1Y-UIs&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HOjLHrlZhmx" data-number="4469">
        <h4>
          <a href="https://openreview.net/forum?id=HOjLHrlZhmx">
              CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HOjLHrlZhmx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fan_Wu6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fan_Wu6">Fan Wu</a>, <a href="https://openreview.net/profile?id=~Linyi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Linyi_Li1">Linyi Li</a>, <a href="https://openreview.net/profile?id=~Zijian_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zijian_Huang2">Zijian Huang</a>, <a href="https://openreview.net/profile?id=~Yevgeniy_Vorobeychik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yevgeniy_Vorobeychik1">Yevgeniy Vorobeychik</a>, <a href="https://openreview.net/profile?id=~Ding_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ding_Zhao1">Ding Zhao</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HOjLHrlZhmx-details-419" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HOjLHrlZhmx-details-419"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the ï¬rst uniï¬ed framework CROP (Certifying Robust Policies for RL) to provide robustness certiï¬cation on both action and reward levels. In particular, we propose two robustness certiï¬cation criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a ï¬nite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certiï¬cation. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certiï¬cations are often tight. All experiment results are available at website https://crop-leaderboard.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=HOjLHrlZhmx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CCu6RcUMwK0" data-number="4448">
        <h4>
          <a href="https://openreview.net/forum?id=CCu6RcUMwK0">
              Neural Link Prediction with Walk Pooling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CCu6RcUMwK0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liming_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liming_Pan1">Liming Pan</a>, <a href="https://openreview.net/profile?id=~Cheng_Shi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng_Shi2">Cheng Shi</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_DokmaniÄ‡1">Ivan DokmaniÄ‡</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CCu6RcUMwK0-details-297" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CCu6RcUMwK0-details-297"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph neural network, Link prediction, Random walk, Graph topology.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a ``predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme.   </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=CCu6RcUMwK0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YeShU5mLfLt" data-number="4436">
        <h4>
          <a href="https://openreview.net/forum?id=YeShU5mLfLt">
              On the Convergence of Certified Robust Training with Interval Bound Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YeShU5mLfLt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yihan_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yihan_Wang2">Yihan Wang</a>, <a href="https://openreview.net/profile?id=~Zhouxing_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhouxing_Shi1">Zhouxing Shi</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanquan_Gu1">Quanquan Gu</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YeShU5mLfLt-details-800" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YeShU5mLfLt-details-800"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Certified robustness, Adversarial robustness, Convergence</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using  IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if  we have sufficiently small perturbation radius and large network width.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present the first theoretical analysis on the convergence of certified robust training with interval bound propagation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sX3XaHwotOg" data-number="4429">
        <h4>
          <a href="https://openreview.net/forum?id=sX3XaHwotOg">
              Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sX3XaHwotOg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yu_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Meng1">Yu Meng</a>, <a href="https://openreview.net/profile?id=~Chenyan_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenyan_Xiong1">Chenyan Xiong</a>, <a href="https://openreview.net/profile?id=~Payal_Bajaj2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payal_Bajaj2">Payal Bajaj</a>, <a href="https://openreview.net/profile?id=~saurabh_tiwary1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~saurabh_tiwary1">saurabh tiwary</a>, <a href="https://openreview.net/profile?id=~Paul_N._Bennett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paul_N._Bennett1">Paul N. Bennett</a>, <a href="https://openreview.net/profile?id=~Jiawei_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Han1">Jiawei Han</a>, <a href="https://openreview.net/profile?id=~Xia_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xia_Song1">Xia Song</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sX3XaHwotOg-details-7" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sX3XaHwotOg-details-7"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Language Model Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present AMOS, a new method that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0jP2n0YFmKG" data-number="4408">
        <h4>
          <a href="https://openreview.net/forum?id=0jP2n0YFmKG">
              Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0jP2n0YFmKG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anuroop_Sriram1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anuroop_Sriram1">Anuroop Sriram</a>, <a href="https://openreview.net/profile?id=~Abhishek_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Abhishek_Das1">Abhishek Das</a>, <a href="https://openreview.net/profile?id=~Brandon_M_Wood1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Brandon_M_Wood1">Brandon M Wood</a>, <a href="https://openreview.net/profile?id=~Siddharth_Goyal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siddharth_Goyal2">Siddharth Goyal</a>, <a href="https://openreview.net/profile?id=~C._Lawrence_Zitnick2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~C._Lawrence_Zitnick2">C. Lawrence Zitnick</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0jP2n0YFmKG-details-161" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0jP2n0YFmKG-details-161"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Atomic Simulations, Computational Chemistry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the recently proposed DimeNet++ and GemNet models by over an order of magnitude in the number of parameters. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric on the S2EF task and 2) 21% on the AFbT metric on the IS2RS task, establishing new state-of-the-art results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We scale GNNs used for modeling atomic simulations by an order of magnitude and obtain large performance improvements on the Open Catalyst 2020 dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="shbAgEsk3qM" data-number="4376">
        <h4>
          <a href="https://openreview.net/forum?id=shbAgEsk3qM">
              Understanding and Leveraging Overparameterization in Recursive Value Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=shbAgEsk3qM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chenjun_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenjun_Xiao1">Chenjun Xiao</a>, <a href="https://openreview.net/profile?id=~Bo_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Dai1">Bo Dai</a>, <a href="https://openreview.net/profile?id=~Jincheng_Mei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jincheng_Mei1">Jincheng Mei</a>, <a href="https://openreview.net/profile?id=~Oscar_A_Ramirez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oscar_A_Ramirez1">Oscar A Ramirez</a>, <a href="https://openreview.net/profile?id=~Ramki_Gummadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ramki_Gummadi1">Ramki Gummadi</a>, <a href="https://openreview.net/profile?id=~Chris_Harris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chris_Harris1">Chris Harris</a>, <a href="https://openreview.net/profile?id=~Dale_Schuurmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dale_Schuurmans1">Dale Schuurmans</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">30 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#shbAgEsk3qM-details-528" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="shbAgEsk3qM-details-528"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Temporal Difference Learning, Residual Minimization, Value Estimation, Overparameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization.  Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck.  To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \emph{overparameterized} linear representations that provides useful, transferable findings.  First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \emph{different} fixed points than residual minimization (RM) in the overparameterized linear case.  We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints.  A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability.  Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM.  
        Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. 
        In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively.  Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present an analysis of value estimation under overparameterized linear representations, and develop new algorithmic tools for improving recursive value estimation with deep models based on the new findings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dPyRNUlttBv" data-number="4368">
        <h4>
          <a href="https://openreview.net/forum?id=dPyRNUlttBv">
              Optimization and Adaptive Generalization of Three layer Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dPyRNUlttBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Khashayar_Gatmiry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Khashayar_Gatmiry1">Khashayar Gatmiry</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>, <a href="https://openreview.net/profile?id=~Jonathan_Kelner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Kelner1">Jonathan Kelner</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dPyRNUlttBv-details-269" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dPyRNUlttBv-details-269"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning theory, adaptive kernel, robust deep learning, neural tangent kernel, adaptive generalization, non-convex optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While there has been substantial recent work studying  generalization of neural networks, 
        the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding.  
        As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel. We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes. In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound.  </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Algorithmically obtaining noise-robust and adaptive generalization bounds for a three layer network model by going beyond the linear approximation of the network</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dPyRNUlttBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-TSe5o7STVR" data-number="4365">
        <h4>
          <a href="https://openreview.net/forum?id=-TSe5o7STVR">
              Non-Parallel Text Style Transfer with Self-Parallel Supervision
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-TSe5o7STVR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ruibo_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruibo_Liu1">Ruibo Liu</a>, <a href="https://openreview.net/profile?id=~Chongyang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chongyang_Gao1">Chongyang Gao</a>, <a href="https://openreview.net/profile?id=~Chenyan_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenyan_Jia1">Chenyan Jia</a>, <a href="https://openreview.net/profile?id=~Guangxuan_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangxuan_Xu1">Guangxuan Xu</a>, <a href="https://openreview.net/profile?id=~Soroush_Vosoughi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Soroush_Vosoughi1">Soroush Vosoughi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-TSe5o7STVR-details-929" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-TSe5o7STVR-details-929"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">style transfer, non-parallel corpus, imitation learning, language models, political stance transfer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style.
        
        In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment &amp; formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new text style transfer model for non-parallel corpus with supervision from intrinsic parallelism.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=-TSe5o7STVR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qhkFX-HLuHV" data-number="4364">
        <h4>
          <a href="https://openreview.net/forum?id=qhkFX-HLuHV">
              Can an Image Classifier Suffice For Action Recognition?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qhkFX-HLuHV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Quanfu_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanfu_Fan1">Quanfu Fan</a>, <a href="https://openreview.net/profile?id=~Chun-Fu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Fu_Chen1">Chun-Fu Chen</a>, <a href="https://openreview.net/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rameswar_Panda1">Rameswar Panda</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qhkFX-HLuHV-details-194" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qhkFX-HLuHV-details-194"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">action recognition, image classifier, super image, vision transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that transformer-based image classifiers alone can suffice for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at \url{https://github.com/IBM/sifar-pytorch}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the idea of super images to re-purpose an image classifer for action recognition.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IK9ap6nxXr2" data-number="4326">
        <h4>
          <a href="https://openreview.net/forum?id=IK9ap6nxXr2">
              Interacting Contour Stochastic Gradient Langevin Dynamics
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IK9ap6nxXr2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Deng1">Wei Deng</a>, <a href="https://openreview.net/profile?id=~Siqi_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siqi_Liang1">Siqi Liang</a>, <a href="https://openreview.net/profile?id=~Botao_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Botao_Hao1">Botao Hao</a>, <a href="https://openreview.net/profile?id=~Guang_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guang_Lin1">Guang Lin</a>, <a href="https://openreview.net/profile?id=~Faming_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Faming_Liang1">Faming Liang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IK9ap6nxXr2-details-218" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IK9ap6nxXr2-details-218"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stochastic gradient Langevin dynamics, MCMC, importance sampling, Wang-Landau algorithm, Parallel MCMC Methods, stochastic approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an interacting contour stochastic gradient Langevin dynamics sampler and prove it can be theoretically more efficient than a single-chain process with an equivalent computational budget.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=IK9ap6nxXr2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MIX3fJkl_1" data-number="4325">
        <h4>
          <a href="https://openreview.net/forum?id=MIX3fJkl_1">
              NeuPL: Neural Population Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MIX3fJkl_1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siqi_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siqi_Liu1">Siqi Liu</a>, <a href="https://openreview.net/profile?id=~Luke_Marris2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luke_Marris2">Luke Marris</a>, <a href="https://openreview.net/profile?id=~Daniel_Hennes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Hennes1">Daniel Hennes</a>, <a href="https://openreview.net/profile?id=~Josh_Merel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Josh_Merel1">Josh Merel</a>, <a href="https://openreview.net/profile?id=~Nicolas_Heess1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicolas_Heess1">Nicolas Heess</a>, <a href="https://openreview.net/profile?id=~Thore_Graepel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thore_Graepel1">Thore Graepel</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MIX3fJkl_1-details-642" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MIX3fJkl_1-details-642"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-Agent Learning, Game Theory, Population Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose NeuPL, a general and efficient population learning framework that learns and represents diverse policies in symmetric zero-sum games within a single conditional network via self-play.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hniLRD_XCA" data-number="4319">
        <h4>
          <a href="https://openreview.net/forum?id=hniLRD_XCA">
              DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hniLRD_XCA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minghao_Han2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghao_Han2">Minghao Han</a>, <a href="https://openreview.net/profile?id=~Jacob_Euler-Rolle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_Euler-Rolle1">Jacob Euler-Rolle</a>, <a href="https://openreview.net/profile?id=~Robert_K._Katzschmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_K._Katzschmann1">Robert K. Katzschmann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hniLRD_XCA-details-423" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hniLRD_XCA-details-423"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Koopman Operator, Robust Control, Robotics, Model Predictive Control, Soft Robotics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Koopman operator theory linearly describes nonlinear dynamical systems in a high-dimensional functional space and it allows to apply linear control methods to highly nonlinear systems. However, the Koopman operator does not account for any uncertainty in dynamical systems, causing it to perform poorly in real-world applications.
        Therefore, we propose a deep stochastic Koopman operator (DeSKO) model in a robust learning control framework to guarantee stability of nonlinear stochastic systems. The DeSKO model captures a dynamical system's uncertainty by inferring a distribution of observables. We use the inferred distribution to design a robust, stabilizing closed-loop controller for a dynamical system. Modeling and control experiments on several advanced control benchmarks show that our framework is more robust and scalable than state-of-the-art deep Koopman operators and reinforcement learning methods. Tested control benchmarks include a soft robotic arm, a legged robot, and a biological gene regulatory network. We also demonstrate that this robust control method resists previously unseen uncertainties, such as external disturbances, with a magnitude of up to five times the maximum control input. Our approach opens up new possibilities in learning control for high-dimensional nonlinear systems while robustly managing internal or external uncertainty.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust learning control framework with guarantee stability based on deep stochastic Koopman operator models</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=hniLRD_XCA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oiZJwC_fyS" data-number="4300">
        <h4>
          <a href="https://openreview.net/forum?id=oiZJwC_fyS">
              Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oiZJwC_fyS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Panagiotis_Misiakos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Panagiotis_Misiakos1">Panagiotis Misiakos</a>, <a href="https://openreview.net/profile?id=~Georgios_Smyrnis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Smyrnis1">Georgios Smyrnis</a>, <a href="https://openreview.net/profile?id=~George_Retsinas2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Retsinas2">George Retsinas</a>, <a href="https://openreview.net/profile?id=~Petros_Maragos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Petros_Maragos1">Petros Maragos</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oiZJwC_fyS-details-2" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oiZJwC_fyS-details-2"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Tropical Geometry, Zonotopes, Hausdorff Approximation, Neural Network Compression</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work we theoretically contribute to neural network approximation by providing a novel tropical geometrical viewpoint to structured neural network compression. In particular, we show that the approximation error between two neural networks with ReLU activations and one hidden layer depends on the Hausdorff distance of the tropical zonotopes of the networks. This theorem comes as a first step towards a purely geometrical interpretation of neural network approximation. Based on this theoretical contribution, we propose geometrical methods that employ the K-means algorithm to compress the fully connected parts of ReLU activated deep neural networks. We analyze the error bounds of our algorithms theoretically based on our approximation theorem and evaluate them empirically on neural network compression. Our experiments follow a proof-of-concept strategy and indicate that our geometrical tools achieve improved performance over relevant tropical geometry techniques and can be competitive against non-tropical methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oiZJwC_fyS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hqkhcFHOeKD" data-number="4295">
        <h4>
          <a href="https://openreview.net/forum?id=hqkhcFHOeKD">
              Learning Towards The Largest Margins
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hqkhcFHOeKD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiong_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiong_Zhou3">Xiong Zhou</a>, <a href="https://openreview.net/profile?id=~Xianming_Liu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xianming_Liu5">Xianming Liu</a>, <a href="https://openreview.net/profile?id=~Deming_Zhai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deming_Zhai2">Deming Zhai</a>, <a href="https://openreview.net/profile?id=~Junjun_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junjun_Jiang2">Junjun Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Gao1">Xin Gao</a>, <a href="https://openreview.net/profile?id=~Xiangyang_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiangyang_Ji1">Xiangyang Ji</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hqkhcFHOeKD-details-929" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hqkhcFHOeKD-details-929"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">loss function design, margin-based loss, classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One of the main challenges for feature representation in deep learning-based classification is the design of appropriate loss functions that exhibit strong discriminative power. The classical softmax loss does not explicitly encourage discriminative learning of features. A popular direction of research is to incorporate margins in well-established losses in order to enforce extra intra-class compactness and inter-class separability, which, however, were developed through heuristic means, as opposed to rigorous mathematical principles. In this work, we attempt to address this limitation by formulating the principled optimization objective as learning towards the largest margins. Specifically, we firstly propose to employ the class margin as the measure of inter-class separability, and the sample margin as the measure of intra-class compactness. Accordingly, to encourage discriminative representation of features, the loss function should promote the largest possible margins for both classes and samples. Furthermore, we derive a generalized margin softmax loss to draw general conclusions for the existing margin-based losses. Not only does this principled framework offer new perspectives to understand and interpret existing margin-based losses, but it also provides new insights that can guide the design of new tools, including \textit{sample margin regularization} and \textit{largest margin softmax loss} for class balanced cases, and \textit{zero centroid regularization} for class imbalanced cases. Experimental results demonstrate the effectiveness of our strategy for multiple tasks including visual classification, imbalanced classification, person re-identification, and face verification.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=hqkhcFHOeKD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="28ib9tf6zhr" data-number="4289">
        <h4>
          <a href="https://openreview.net/forum?id=28ib9tf6zhr">
              Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=28ib9tf6zhr" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yonggan_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonggan_Fu1">Yonggan Fu</a>, <a href="https://openreview.net/profile?email=sz74%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="sz74@rice.edu">Shunyao Zhang</a>, <a href="https://openreview.net/profile?email=sw99%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="sw99@rice.edu">Shang Wu</a>, <a href="https://openreview.net/profile?id=~Cheng_Wan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng_Wan2">Cheng Wan</a>, <a href="https://openreview.net/profile?id=~Yingyan_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yingyan_Lin1">Yingyan Lin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#28ib9tf6zhr-details-204" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="28ib9tf6zhr-details-204"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision transformer, adversarial examples, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: "Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?" Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs. It can be expected that our Patch-Fool framework will shed light on both future architecture designs and training schemes for robustifying ViTs towards their real-world deployment. Our codes are available at https://github.com/RICE-EIC/Patch-Fool.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the Patch-Fool attack to unveil a vulnerability perspective of ViTs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Q5uh1Nvv5dm" data-number="4287">
        <h4>
          <a href="https://openreview.net/forum?id=Q5uh1Nvv5dm">
              AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Q5uh1Nvv5dm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~David_Berthelot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Berthelot1">David Berthelot</a>, <a href="https://openreview.net/profile?id=~Rebecca_Roelofs1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rebecca_Roelofs1">Rebecca Roelofs</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Nicholas_Carlini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Carlini1">Nicholas Carlini</a>, <a href="https://openreview.net/profile?id=~Alexey_Kurakin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_Kurakin1">Alexey Kurakin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Q5uh1Nvv5dm-details-467" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q5uh1Nvv5dm-details-467"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised domain adaptation, semi-supervised learning, semi-supervised domain adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a unified solution for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA and find that AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce AdaMatch, a unified solution that achieves state-of-the-art results for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="l_amHf1oaK" data-number="4286">
        <h4>
          <a href="https://openreview.net/forum?id=l_amHf1oaK">
              Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=l_amHf1oaK" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Claudio_Ferrari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Claudio_Ferrari2">Claudio Ferrari</a>, <a href="https://openreview.net/profile?id=~Mark_Niklas_Mueller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mark_Niklas_Mueller2">Mark Niklas Mueller</a>, <a href="https://openreview.net/profile?id=~Nikola_Jovanovi%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikola_JovanoviÄ‡1">Nikola JovanoviÄ‡</a>, <a href="https://openreview.net/profile?id=~Martin_Vechev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martin_Vechev1">Martin Vechev</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#l_amHf1oaK-details-284" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l_amHf1oaK-details-284"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Certified Robustness, Branch-and-Bound, Convex Relaxation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new state-of-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We obtain a state-of-the-art GPU-based neural network verifier by leveraging tight multi-neuron constraints in a Branch-and-Bound setting.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="VFBjuF8HEp" data-number="4276">
        <h4>
          <a href="https://openreview.net/forum?id=VFBjuF8HEp">
              Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality
          </a>
        
          
            <a href="https://openreview.net/pdf?id=VFBjuF8HEp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daniel_Watson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Watson1">Daniel Watson</a>, <a href="https://openreview.net/profile?id=~William_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~William_Chan1">William Chan</a>, <a href="https://openreview.net/profile?id=~Jonathan_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Ho1">Jonathan Ho</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Norouzi1">Mohammad Norouzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 26 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#VFBjuF8HEp-details-149" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VFBjuF8HEp-details-149"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method to discover fast, high-fidelity samplers for diffusion probabilistic models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="18">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="submitted-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="youe3QQepVB" data-number="4714">
        <h4>
          <a href="https://openreview.net/forum?id=youe3QQepVB">
              Generative Modeling for Multitask Visual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=youe3QQepVB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhipeng_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhipeng_Bao1">Zhipeng Bao</a>, <a href="https://openreview.net/profile?id=~Yu-Xiong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Xiong_Wang1">Yu-Xiong Wang</a>, <a href="https://openreview.net/profile?id=~Martial_Hebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martial_Hebert1">Martial Hebert</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#youe3QQepVB-details-120" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="youe3QQepVB-details-120"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, especially in the low-data regimes, and our model consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a general multi-task oriented generative modeling (MGM) framework that introduces generative models to facilitate multi-task learning and it consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2aC0_RxkBL_" data-number="4707">
        <h4>
          <a href="https://openreview.net/forum?id=2aC0_RxkBL_">
              Where is the bottleneck in long-tailed classification?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2aC0_RxkBL_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaid_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zaid_Khan1">Zaid Khan</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yun_Fu1">Yun Fu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2aC0_RxkBL_-details-644" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2aC0_RxkBL_-details-644"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, bias, long tailed learning, imbalanced learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A commonly held belief in deep-learning based long-tailed classiï¬cation is that the representations learned from long-tailed data are â€good enoughâ€ and the performance bottleneck is the classiï¬cation head atop the representation learner. We design experiments to investigate this folk wisdom, and ï¬nd that representations learned from long-tailed data distributions substantially differ from the representations learned from â€normalâ€ data distributions. We show that the long-tailed representations are volatile and brittle with respect to the true data distribution. Compared to the representations learned from the true, balanced distributions, long-tailed representations fail to localize tail classes and display vastly worse inter-class separation and intra-class compactness when unseen samples from the true data distribution are embedded into the feature space. We provide an explanation for why data augmentation helps long-tailed classiï¬cation despite leaving the dataset imbalance unchanged â€” it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate how learning from long-tailed distributions harms representations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="voEpzgY8gsT" data-number="4704">
        <h4>
          <a href="https://openreview.net/forum?id=voEpzgY8gsT">
              Additive Poisson Process: Learning Intensity of Higher-Order Interaction in Poisson Processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=voEpzgY8gsT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Simon_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Luo1">Simon Luo</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Zhou9">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~lamiae_azizi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~lamiae_azizi1">lamiae azizi</a>, <a href="https://openreview.net/profile?id=~Mahito_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mahito_Sugiyama1">Mahito Sugiyama</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#voEpzgY8gsT-details-633" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="voEpzgY8gsT-details-633"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Poisson Process, Log-Linear Model, Energy-Based Model, Generalized Additive Models, Information Geometry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Additive Poisson Process (APP), a novel framework that can model the higher-order interaction effects of the intensity functions in Poisson processes using projections into lower-dimensional space. Our model combines the techniques in information geometry to model higher-order interactions on a statistical manifold and in generalized additive models to use lower-dimensional projections to overcome the effects from the curse of dimensionality. Our approach solves a convex optimization problem by minimizing the KL divergence from a sample distribution in lower-dimensional projections to the distribution modeled by an intensity function in the Poisson process. Our empirical results show that our model is able to use samples observed in the lower dimensional space to estimate the higher-order intensity function with extremely sparse observations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An efficient technique that uses a log-linear model on a partial order structure to approximate a high-dimensional intensity functions in a Poisson Process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=voEpzgY8gsT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qfLJBJf_DnH" data-number="4701">
        <h4>
          <a href="https://openreview.net/forum?id=qfLJBJf_DnH">
              Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qfLJBJf_DnH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Laureline_Logiaco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Laureline_Logiaco1">Laureline Logiaco</a>, <a href="https://openreview.net/profile?id=~G_Sean_Escola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~G_Sean_Escola1">G Sean Escola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qfLJBJf_DnH-details-473" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qfLJBJf_DnH-details-473"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuroscience, dynamical systems, thalamocortical architecture, motor preparation, continual learning, hierarchical continuous motor control, out-of-distribution generalization, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of learning dynamics that can produce hierarchically organized continuous outputs consisting of the flexible chaining of re-usable motor â€˜motifsâ€™ from which complex behavior is generated. Can a motif library be efficiently and extendably learned without interference between motifs, and can these motifs be chained in arbitrary orders without first learning the corresponding motif transitions during training? This requires (i) parameter updates while learning a new motif that do not interfere with the parameters used for the previously acquired ones; and (ii) successful motif generation when starting from the network states reached at the end of any of the other motifs, even if these states were not present during training (a case of out-of-distribution generalization). We meet the first requirement by designing recurrent neural networks (RNNs) with specific architectures that segregate motif-dependent parameters (as customary in continual learning works), and try a standard method to address the second by training with random initial states. We find that these standard RNNs are very unreliable during zero-shot transfer to motif chaining. We then use insights from the motor thalamocortical circuit, featuring a specific module that shapes motif transitions. We develop a method to constrain the RNNs to function similarly to the thalamocortical circuit during motif transitions, while preserving the large expressivity afforded by gradient-based training of non-analytically tractable RNNs. We then show that this thalamocortical inductive bias not only acts in synergy with gradient-descent RNN training to improve accuracy during in-training-distribution motif production, but also leads to zero-shot transfer to new motif chains with no performance cost. Besides proposing an efficient, robust and flexible RNN architecture, our results shed new light on the function of motor preparation in the brain.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Motor preparation in nonlinear RNNs supports robust chaining of accurate continuous motor motifs in never-experienced orders.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rF5UoZFrsF4" data-number="4697">
        <h4>
          <a href="https://openreview.net/forum?id=rF5UoZFrsF4">
              VUT: Versatile UI Transformer for Multimodal Multi-Task User Interface Modeling 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rF5UoZFrsF4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Li2">Yang Li</a>, <a href="https://openreview.net/profile?id=~Gang_Li13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Li13">Gang Li</a>, <a href="https://openreview.net/profile?id=~Xin_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Zhou3">Xin Zhou</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rF5UoZFrsF4-details-499" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rF5UoZFrsF4-details-499"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">User Interface Modeling, Multimodal input, Multi-task learning, Transformer, Layout Detection, Language Grounding, Image Captioning, Screen Summarization, Tappability Prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT has achieved accuracy either on par with or exceeding the accuracy when the model is trained for individual tasks separately.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The work addresses unique challenges of multimodal multi-task learning of distinct tasks for user interface modeling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3M3t3tUbA2Y" data-number="4686">
        <h4>
          <a href="https://openreview.net/forum?id=3M3t3tUbA2Y">
              DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3M3t3tUbA2Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fei_Deng1">Fei Deng</a>, <a href="https://openreview.net/profile?id=~Ingook_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingook_Jang1">Ingook Jang</a>, <a href="https://openreview.net/profile?id=~Sungjin_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungjin_Ahn1">Sungjin Ahn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3M3t3tUbA2Y-details-535" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3M3t3tUbA2Y-details-535"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction
        often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions. To address this issue, previous work has proposed to contrastively learn the latent representations and its temporal dynamics, but showed inconsistent performance, often worse than Dreamer. Although, in computer vision, an alternative prototypical approach has often shown to be more accurate and robust, it is elusive how this approach can be combined best with the temporal dynamics learning in MBRL. In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal. Similar to SwAV, by encouraging uniform cluster assignment across the batch, we implicitly push apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details. We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model is consistently better than the previous models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dy8gq-LuckD" data-number="4680">
        <h4>
          <a href="https://openreview.net/forum?id=Dy8gq-LuckD">
              Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dy8gq-LuckD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Wu1">Nan Wu</a>, <a href="https://openreview.net/profile?id=~Stanislaw_Kamil_Jastrzebski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stanislaw_Kamil_Jastrzebski1">Stanislaw Kamil Jastrzebski</a>, <a href="https://openreview.net/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyunghyun_Cho1">Kyunghyun Cho</a>, <a href="https://openreview.net/profile?id=~Krzysztof_J._Geras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krzysztof_J._Geras1">Krzysztof J. Geras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dy8gq-LuckD-details-452" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dy8gq-LuckD-details-452"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-modal learning, deep neural networks, multi-view learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities. We observe empirically that such behavior hurts its overall generalization. We validate our hypothesis by estimating the gain on the accuracy when the model has access to an additional modality. We refer to this gain as the conditional utilization rate of the modality. In the experiments, we consistently observe an imbalance in conditional utilization rate between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. We thus propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the modelâ€™s generalization on three datasets: Colored MNIST (Kim et al., 2019), Princeton ModelNet40 (Wu et al., 2015), and NVIDIA Dynamic Hand Gesture Dataset (Molchanov et al., 2016).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UeE41VsK1KJ" data-number="4677">
        <h4>
          <a href="https://openreview.net/forum?id=UeE41VsK1KJ">
              Subjective Learning for Open-Ended Data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UeE41VsK1KJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tianren_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianren_Zhang1">Tianren Zhang</a>, <a href="https://openreview.net/profile?id=~Yizhou_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yizhou_Jiang1">Yizhou Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Su1">Xin Su</a>, <a href="https://openreview.net/profile?id=~Shangqi_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shangqi_Guo2">Shangqi Guo</a>, <a href="https://openreview.net/profile?id=~Feng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Chen1">Feng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UeE41VsK1KJ-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UeE41VsK1KJ-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Open-ended data, machine learning, supervised learning, data conflict</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional supervised learning typically assumes that the learning task can be solved by learning a single function since the data is sampled from a fixed distribution. However, this assumption is invalid in open-ended environments where no task-level data partitioning is available. In this paper, we present a novel supervised learning framework of learning from open-ended data, which is modeled as data implicitly sampled from multiple domains with the data in each domain obeying a domain-specific target function. Since different domains may possess distinct target functions, open-ended data inherently requires multiple functions to capture all its input-output relations, rendering training a single global model problematic. To address this issue, we devise an Open-ended Supervised Learning (OSL) framework, of which the key component is a subjective function that allocates the data among multiple candidate models to resolve the "conflict'' between the data from different domains, exhibiting a natural hierarchy. We theoretically analyze the learnability and the generalization error of OSL, and empirically validate its efficacy in both open-ended regression and classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We formalize the problem of learning from open-ended data that implicitly comes from multiple domains and inherently requires multiple functions to fully capture its input-output relations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UeE41VsK1KJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2RNpZ8S4alJ" data-number="4675">
        <h4>
          <a href="https://openreview.net/forum?id=2RNpZ8S4alJ">
              KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2RNpZ8S4alJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alireza_Rezazadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Rezazadeh1">Alireza Rezazadeh</a>, <a href="https://openreview.net/profile?email=cchoi%40umn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="cchoi@umn.edu">Changhyun Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2RNpZ8S4alJ-details-128" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2RNpZ8S4alJ-details-128"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object-centric representation is an essential abstraction for physical reasoning and forward prediction. Most existing approaches learn this representation through extensive supervision (e.g, object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network)---an end-to-end unsupervised framework to reason about object interactions in complex systems based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, and novel object geometries. Experiments demonstrate the effectiveness of our model to accurately perform forward prediction and learn plannable object-centric representations which can also be used in downstream model-based control tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="an_ndI09oVZ" data-number="4667">
        <h4>
          <a href="https://openreview.net/forum?id=an_ndI09oVZ">
              Deep banach space kernels
          </a>
        
          
            <a href="https://openreview.net/pdf?id=an_ndI09oVZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mrityunjay_Bhardwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mrityunjay_Bhardwaj1">Mrityunjay Bhardwaj</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#an_ndI09oVZ-details-786" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="an_ndI09oVZ-details-786"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RKBS, RKHS, concatenated kernel learning, representation learning, deep learning, MLMKL, Deep Gaussian Processes, gaussian processes, kernel machines</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recent success of deep learning has encouraged many researchers to explore the deep/concatenated variants of classical kernel methods. Some of which includes MLMKL, DGP and DKL. Although, These methods have proven to be quite useful in various real-world settings. They still suffer from the limitations of only utilizing kernels from Hilbert spaces. In this paper, we address these shortcomings by introducing a new class of concatenated kernel learning methods that use the kernels from the reproducing kernel Banach spaces(RKBSs). These spaces turned out to be one of the most general spaces where a reproducing Kernel exists. We propose a framework of construction for these Deep RKBS models and then provide a representer theorem for regularized learning problems. We also describe the relationship with its deep RKHS variant as well as standard Deep Gaussian Processes. In the end, we construct and implement a two-layer deep RKBS model and demonstrate it on a range of machine learning tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">a new class of deep kernel methods which uses kernels from reproducing kernel banach spaces (RKBS).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0Kj5mhn6sw" data-number="4650">
        <h4>
          <a href="https://openreview.net/forum?id=0Kj5mhn6sw">
              Gesture2Vec: Clustering Gestures using  Representation Learning Methods for Co-speech Gesture Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0Kj5mhn6sw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Payam_Jome_Yazdian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_Jome_Yazdian1">Payam Jome Yazdian</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?email=angelica%40sfu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="angelica@sfu.ca">Angelica Lim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0Kj5mhn6sw-details-996" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0Kj5mhn6sw-details-996"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation learning, gesture generation, vector quantization, machine translation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans. Similarly, the co-speech gesture is a key ingredient in human-agent interaction including both virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. Furthermore, we use a machine translation model that takes input text and translates it into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated quantized gestures from the input text as an input to the autoencoderâ€™s decoder to produce gesture sequences. The resulting gestures can be applied to both virtual agents and humanoid robots. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0Kj5mhn6sw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WXy4C-RjET" data-number="4644">
        <h4>
          <a href="https://openreview.net/forum?id=WXy4C-RjET">
              Logit Attenuating Weight Normalization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WXy4C-RjET" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aman_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aman_Gupta1">Aman Gupta</a>, <a href="https://openreview.net/profile?id=~Rohan_Ramanath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Ramanath1">Rohan Ramanath</a>, <a href="https://openreview.net/profile?id=~Jun_Shi4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jun_Shi4">Jun Shi</a>, <a href="https://openreview.net/profile?id=~Anika_Ramachandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anika_Ramachandran1">Anika Ramachandran</a>, <a href="https://openreview.net/profile?id=~SIROU_ZHU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SIROU_ZHU1">SIROU ZHU</a>, <a href="https://openreview.net/profile?id=~Mingzhou_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mingzhou_Zhou1">Mingzhou Zhou</a>, <a href="https://openreview.net/profile?id=~Sathiya_Keerthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sathiya_Keerthi1">Sathiya Keerthi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WXy4C-RjET-details-877" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WXy4C-RjET-details-877"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning, gradient methods, stochastic optimization, generalization gap, imagenet, adam, large batch training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over-parameterized deep networks trained using gradient-based optimizers is a popular way of solving classification and ranking problems. Without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that, even with weight decay (WD) and normal hyper-parameter tuning, adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain.
        
        An alternative to WD for improving a network's adaptivity is to directly control the magnitude of the weights and hence the logits. We propose a method called Logit Attenuating Weight Normalization (LAWN), that can be stacked onto any gradient-based optimizer.  LAWN initially starts off training in a free (unregularized) mode and, after some initial epochs, it constrains the weight norms of layers, thereby controlling the logits and improving adaptivity. This is a new regularization approach that does not use WD anywhere; instead, the number of initial free epochs becomes the new hyper-parameter. The resulting LAWN variant of adaptive optimizers gives a solid lift to generalization performance, making their performance equal or even exceed SGD's performance on benchmark image classification and recommender datasets. Another important feature is that LAWN also greatly improves the adaptive optimizers when used with large batch sizes.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An optimizer for deep learning called Logit Attenuating Weight Normalization (LAWN) for superior generalization performance and scaling to large batches</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WXy4C-RjET&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxBH9j8IE_H" data-number="4643">
        <h4>
          <a href="https://openreview.net/forum?id=UxBH9j8IE_H">
              Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxBH9j8IE_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~BITHIKA_PAL1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BITHIKA_PAL1">BITHIKA PAL</a>, <a href="https://openreview.net/profile?id=~Arindam_Biswas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arindam_Biswas1">Arindam Biswas</a>, <a href="https://openreview.net/profile?id=~Pabitra_Mitra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pabitra_Mitra1">Pabitra Mitra</a>, <a href="https://openreview.net/profile?id=~BISWAJIT_BASU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BISWAJIT_BASU1">BISWAJIT BASU</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxBH9j8IE_H-details-987" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxBH9j8IE_H-details-987"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Neural Networks, Network Pruning, Ramanujan Graphs, Eigenvalue bounds, Spectral Gap</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A Ramanujan graph perspective to explain the lottery ticket hypothesis</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a0SRWViFYW" data-number="4642">
        <h4>
          <a href="https://openreview.net/forum?id=a0SRWViFYW">
              Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a0SRWViFYW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Patrick_R._Johnstone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_R._Johnstone1">Patrick R. Johnstone</a>, <a href="https://openreview.net/profile?id=~Jonathan_Eckstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Eckstein1">Jonathan Eckstein</a>, <a href="https://openreview.net/profile?id=~Thomas_Flynn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Flynn1">Thomas Flynn</a>, <a href="https://openreview.net/profile?id=~Shinjae_Yoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinjae_Yoo1">Shinjae Yoo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a0SRWViFYW-details-629" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a0SRWViFYW-details-629"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">convex optimization, min-max games, saddle-point problems, first-order stochastic methods, proximal methods, operator splitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new, stochastic variant of the projective splitting (PS) family of algorithms for monotone inclusion problems.  It can solve min-max and noncooperative game formulations arising in applications such as robust ML without the convergence issues associated with gradient descent-ascent, the current de facto standard approach in ML applications.  Our proposal is the first version of PS able to use stochastic gradient oracles. It can solve min-max games while handling multiple constraints and nonsmooth regularizers via projection and proximal operators. Unlike other stochastic splitting methods that can solve such problems, our method does not rely on a product-space reformulation of the original problem. We prove almost-sure convergence of the iterates to the solution and a convergence rate for the expected residual.  By working with monotone inclusions rather than variational inequalities, our analysis avoids the drawbacks of measuring convergence through the restricted gap function. We close with numerical experiments on a distributionally robust sparse logistic regression problem.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a stochastic splitting method that can easily handle min-max problems with multiple regularizers and constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=a0SRWViFYW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_K6rwRjW9WO" data-number="4637">
        <h4>
          <a href="https://openreview.net/forum?id=_K6rwRjW9WO">
              RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_K6rwRjW9WO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Victor_Quintas-Martinez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Quintas-Martinez1">Victor Quintas-Martinez</a>, <a href="https://openreview.net/profile?id=~Victor_Chernozhukov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Chernozhukov1">Victor Chernozhukov</a>, <a href="https://openreview.net/profile?id=~Vasilis_Syrgkanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vasilis_Syrgkanis1">Vasilis Syrgkanis</a>, <a href="https://openreview.net/profile?id=~Whitney_Newey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Whitney_Newey1">Whitney Newey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_K6rwRjW9WO-details-111" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_K6rwRjW9WO-details-111"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="75" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.281em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></mjx-assistive-mml></mjx-container>-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, that is derived based on a functional-specific theoretical derivation of what is known as the influence function and which leads to properties such as double robustness and Neyman orthogonality. We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method solely requires value query oracle access to the linear functional. We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Reisz representer and regression loss, while sharing representation layers for the two functions. We also propose a random forest method which learns a locally linear representation of the Reisz function. Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We implement an automatic debiasing procedure for causal and policy effects based on automatically learning their corresponding Riesz representation, using Neural Nets and Random Forests.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_K6rwRjW9WO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Mo9R9oqzPo" data-number="4636">
        <h4>
          <a href="https://openreview.net/forum?id=Mo9R9oqzPo">
              New Definitions and Evaluations for Saliency Methods: Staying Intrinsic and Sound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Mo9R9oqzPo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arushi_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arushi_Gupta1">Arushi Gupta</a>, <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Dingli_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dingli_Yu1">Dingli Yu</a>, <a href="https://openreview.net/profile?id=~Kaifeng_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaifeng_Lyu2">Kaifeng Lyu</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sanjeev_Arora1">Sanjeev Arora</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Mo9R9oqzPo-details-432" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mo9R9oqzPo-details-432"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">saliency, masking based methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Saliency methods seek to provide human-interpretable explanations for the output of machine learning model on a given input. A plethora of saliency methods exist, as well as an extensive literature on their justifications/criticisms/evaluations. This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans. It tries to introduce methods and evaluations for masked-based saliency methods that are {\em intrinsic} --- use just the training dataset and the trained net, and do not use separately trained nets, distractor distributions, human evaluations or annotations. Since a mask can be seen as a "certificate" justifying the net's answer, we introduce notions of {\em completeness} and {\em soundness} (the latter being the new contribution) motivated by logical proof systems. These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.V. regularization, upscaling). </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mo9R9oqzPo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GthNKCqdDg" data-number="4623">
        <h4>
          <a href="https://openreview.net/forum?id=GthNKCqdDg">
              Selective Token Generation for Few-shot Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GthNKCqdDg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daejin_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daejin_Jo1">Daejin Jo</a>, <a href="https://openreview.net/profile?id=~Taehwan_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Taehwan_Kwon1">Taehwan Kwon</a>, <a href="https://openreview.net/profile?id=~Sungwoong_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungwoong_Kim2">Sungwoong Kim</a>, <a href="https://openreview.net/profile?id=~Eun-Sol_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eun-Sol_Kim1">Eun-Sol Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GthNKCqdDg-details-251" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GthNKCqdDg-details-251"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Generation, Reinforcement Learning, Few-shot Learning, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among these transfer learning algorithms from PLMs, additive learning that incorporates a task-specific adapter on top of the fixed PLM has been popularly used to alleviate the severe overfitting problem in the few-shot setting. However, this added task-specific adapter is generally trained by maximum likelihood estimation that can easily suffer from the so-called exposure bias problem, especially in sequential text generation. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) for few-shot natural language generation (NLG) tasks. In particular, we propose to use a selective token generation between the transformer-based PLM and the task-specific adapter during both training and inference. This output token selection between the two generators allows the adapter to take into account only on the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, in order to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="INO8hGXD2M" data-number="4614">
        <h4>
          <a href="https://openreview.net/forum?id=INO8hGXD2M">
              Adversarial Distributions Against Out-of-Distribution Detectors
          </a>
        
          
            <a href="https://openreview.net/pdf?id=INO8hGXD2M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sangwoong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sangwoong_Yoon1">Sangwoong Yoon</a>, <a href="https://openreview.net/profile?id=~Jinwon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinwon_Choi1">Jinwon Choi</a>, <a href="https://openreview.net/profile?id=~Yonghyeon_LEE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonghyeon_LEE1">Yonghyeon LEE</a>, <a href="https://openreview.net/profile?id=~Yung-Kyun_Noh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yung-Kyun_Noh1">Yung-Kyun Noh</a>, <a href="https://openreview.net/profile?id=~Frank_C._Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_C._Park1">Frank C. Park</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#INO8hGXD2M-details-332" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="INO8hGXD2M-details-332"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">out-of-distribution detection, outlier detection, adversarial attack, model evaluation, markov chain monte carlo</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution. As an outlier may deviate from the training distribution in unexpected ways, an ideal OOD detector should be able to detect all types of outliers. However, current evaluation protocols test a detector over OOD datasets that cover only a small fraction of all possible outliers, leading to overly optimistic views of OOD detector performance.  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers.  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector. Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel evaluation method for out-of-distribution detectors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eOdSD0B5TE" data-number="4606">
        <h4>
          <a href="https://openreview.net/forum?id=eOdSD0B5TE">
              On the Implicit Biases of Architecture &amp; Gradient Descent
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eOdSD0B5TE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jeremy_Bernstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeremy_Bernstein1">Jeremy Bernstein</a>, <a href="https://openreview.net/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisong_Yue1">Yisong Yue</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eOdSD0B5TE-details-964" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eOdSD0B5TE-details-964"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalisation, function space, PAC-Bayes, NNGP, orthants, margin</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Do neural networks generalise because of bias in the functions returned by gradient descent, or bias already present in the network architecture? <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="76" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">Â¿</mjx-utext><mjx-c class="mjx-c1D443 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45E TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">Ã©</mjx-utext><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c3F TEX-MI"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">Â¿Por quÃ© no los dos?</mtext></math></mjx-assistive-mml></mjx-container> This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent. To measure the implicit bias of architecture, new technical tools are developed to both <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="77" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">analytically bound</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="78" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">consistently estimate</mtext></math></mjx-assistive-mml></mjx-container> the average test error of the neural network--Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-PÃ©rez et al. (2019) and underscoring the importance of architecture. Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior. This highlights a curious fact: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="79" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">minimum a posteriori</mtext></math></mjx-assistive-mml></mjx-container> functions can generalise best, and gradient descent can select for those functions. In summary, new technical tools suggest a nuanced portrait of generalisation involving both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">New technical tools suggest a nuanced portrait of generalisation that involves both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=eOdSD0B5TE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vPK-G5HbnWg" data-number="4603">
        <h4>
          <a href="https://openreview.net/forum?id=vPK-G5HbnWg">
              PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vPK-G5HbnWg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zehao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zehao_Dong1">Zehao Dong</a>, <a href="https://openreview.net/profile?id=~Muhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhan_Zhang1">Muhan Zhang</a>, <a href="https://openreview.net/profile?id=~Fuhai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuhai_Li1">Fuhai Li</a>, <a href="https://openreview.net/profile?id=~Yixin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yixin_Chen1">Yixin Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vPK-G5HbnWg-details-199" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vPK-G5HbnWg-details-199"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DAG encoder, graph neural network, Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optimization of directed acyclic graph (DAG) structures has many applications, such as neural architecture search (NAS) and probabilistic graphical model learning. Encoding DAGs into real vectors is a dominant component in most neural-network-based DAG optimization frameworks. Currently, most popular DAG encoders use an asynchronous message passing scheme which sequentially processes nodes according to the dependency between nodes in a DAG. That is, a node must not be processed until all its predecessors are processed. As a result, they are inherently not parallelizable. In this work, we propose a Parallelizable Attention-based Computation structure Encoder (PACE) that processes nodes simultaneously and encodes DAGs in parallel. We demonstrate the superiority of PACE through  encoder-dependent optimization subroutines that search the optimal DAG structure based on the learned DAG embeddings. Experiments show that PACE not only improves the effectiveness over previous sequential DAG encoders with a significantly boosted training and inference speed, but also generates smooth latent (DAG encoding) spaces that are beneficial to downstream optimization subroutines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper introduces a novel DAG encoder based on Transformer to encode the computation structure defined by DAGs in a fully parallelizable manner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vPK-G5HbnWg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hEiwVblq4P" data-number="4602">
        <h4>
          <a href="https://openreview.net/forum?id=hEiwVblq4P">
              Proper Straight-Through Estimator: Breaking symmetry promotes convergence to true minimum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hEiwVblq4P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shinya_Gongyo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinya_Gongyo1">Shinya Gongyo</a>, <a href="https://openreview.net/profile?id=~Kohta_Ishikawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kohta_Ishikawa1">Kohta Ishikawa</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hEiwVblq4P-details-443" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hEiwVblq4P-details-443"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">quantization, binary network, low bit network, Straight through estimator, STE</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the quantized network, its gradient shows either vanishing or diverging. The network thus cannot be learned by the standard back-propagation, so that an alternative approach called Straight Through Estimator (STE), which replaces the part of the gradient with a simple differentiable function, is used. While STE is known to work well for learning the quantized network empirically, it has not been established theoretically. A recent study by Yin et. al. (2019) has provided theoretical support for STE. However, its justification is still limited to the model in the one-hidden layer network with the binary activation where  Gaussian generates the input data, and the true labels are output from the teacher network with the same binary network architecture. In this paper, we discuss the effectiveness of STEs in more general situations without assuming the shape of the input distribution and the labels. By considering the scale symmetry of the network and specific properties of the STEs, we find that STE with clipped Relu is superior to STEs with identity function and vanilla Relu. The clipped Relu STE, which breaks the scale symmetry, may pick up one of the local minima degenerated in scales, while the identity STE and vanilla Relu STE, which keep the scale symmetry, may not pick it up. To confirm this observation, we further present an analysis of a simple misspecified model as an example. We find that all the stationary points are identical with the vanishing points of the cRelu STE gradient, while some of them are not identical with the vanishing points of the identity and Relu STE.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We discuss breaking symmetry embedded in the network by Straight through estimators enhances the possibility of convergence to the true minimum.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="W6BpshgRi0q" data-number="4599">
        <h4>
          <a href="https://openreview.net/forum?id=W6BpshgRi0q">
              Ask2Mask: Guided Data Selection for Masked Speech Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=W6BpshgRi0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Murali_Karthick_Baskar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Murali_Karthick_Baskar1">Murali Karthick Baskar</a>, <a href="https://openreview.net/profile?id=~Andrew_Rosenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Rosenberg1">Andrew Rosenberg</a>, <a href="https://openreview.net/profile?id=~Bhuvana_Ramabhadran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhuvana_Ramabhadran2">Bhuvana Ramabhadran</a>, <a href="https://openreview.net/profile?id=~Yu_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zhang2">Yu Zhang</a>, <a href="https://openreview.net/profile?email=pedro%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="pedro@google.com">Pedro Moreno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#W6BpshgRi0q-details-945" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W6BpshgRi0q-details-945"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM), Data selection, Self-supervision, ASR, Speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work,  we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training.  ATM employs an external ASR model or \textit{scorer} to weight unsupervised input samples in two different ways: 1) A fine-grained data selection is performed by masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. 2) ATM is further extended to focus at utterance-level by weighting the final MSM loss with the utterance-level confidence score.  We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and AMI (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6\% relative) while still yielding modest improvements under matched conditions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Data selection Approach for masked speech model to focus on relevant samples to learn meaningful speech representations</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZWykq5n4zx" data-number="4598">
        <h4>
          <a href="https://openreview.net/forum?id=ZWykq5n4zx">
              Boosting the Confidence of Near-Tight Generalization Bounds for Uniformly Stable Randomized Algorithms
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZWykq5n4zx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaotong_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaotong_Yuan1">Xiaotong Yuan</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZWykq5n4zx-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZWykq5n4zx-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uniform stability, Randomized learning algorithms, Bagging, Generalization bounds, Stochastic gradient methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">High probability generalization bounds of uniformly stable learning algorithms have recently been actively studied with a series of near-tight results established by~\citet{feldman2019high,bousquet2020sharper}. However, for randomized algorithms with on-average uniform stability, such as stochastic gradient descent (SGD) with time decaying learning rates, it still remains less well understood if these deviation bounds still hold with high confidence over the internal randomness of algorithm. This paper addresses this open question and makes progress towards answering it inside a classic framework of confidence-boosting. To this end, we first establish an in-expectation first moment generalization error bound for randomized learning algorithm with on-average uniform stability, based on which we then show that a properly designed subbagging process leads to near-tight high probability generalization bounds over the randomness of data and algorithm. We further substantialize these generic results to SGD to derive improved high probability generalization bounds for convex or non-convex optimization with natural time decaying learning rates, which have not been possible to prove with the existing uniform stability results. Specially for deterministic uniformly stable algorithms, our confidence-boosting results improve upon the best known generalization bounds in terms of a logarithmic factor on sample size, which moves a step forward towards resolving an open question raised by~\citet{bousquet2020sharper}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A confidence-boosting method for deriving near-tight generalization bounds with high probability for uniformly stable randomized learning algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EhwEUb2ynIa" data-number="4591">
        <h4>
          <a href="https://openreview.net/forum?id=EhwEUb2ynIa">
              How to Adapt Your Large-Scale Vision-and-Language Model
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EhwEUb2ynIa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konwoo_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Konwoo_Kim1">Konwoo Kim</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepak_Pathak1">Deepak Pathak</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EhwEUb2ynIa-details-285" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EhwEUb2ynIa-details-285"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transfer learning, fine-tuning, layernorm, CLIP, prompt-tuning, adaptation, zero-shot, pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training large-scale vision and language models (e.g. CLIP) has shown promising results in representation and transfer learning. We investigate the question of how to efficiently adapt these models to downstream tasks. For image classification, linear probes have been the standard for ease of use and efficiency, while for language, other approaches like prompt tuning have emerged. We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one. We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board. We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks. Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models. Website at https://sites.google.com/view/adapt-large-scale-models</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a thorough analysis of different methods on how to adapt large-scale pretrained vision-and-language models to several downstream classification tasks, and find that just tuning LayerNorm is an effective fine-tuning baseline.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zLb9oSWy933" data-number="4583">
        <h4>
          <a href="https://openreview.net/forum?id=zLb9oSWy933">
              Fast Finite Width Neural Tangent Kernel
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zLb9oSWy933" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Roman_Novak2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roman_Novak2">Roman Novak</a>, <a href="https://openreview.net/profile?id=~Jascha_Sohl-Dickstein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jascha_Sohl-Dickstein2">Jascha Sohl-Dickstein</a>, <a href="https://openreview.net/profile?id=~Samuel_Stern_Schoenholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samuel_Stern_Schoenholz1">Samuel Stern Schoenholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">29 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zLb9oSWy933-details-296" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zLb9oSWy933-details-296"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Tangent Kernel, NTK, Finite Width, Fast, Algorithm, JAX, Jacobian, Software</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="80" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-msup><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.577em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="normal">Î˜</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>âˆ‚</mi><mi>f</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>âˆ‚</mi><mi>Î¸</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>âˆ‚</mi><mi>f</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>âˆ‚</mi><mi>Î¸</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><mi>T</mi></msup></math></mjx-assistive-mml></mjx-container>, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite-width NTK is notoriously expensive to compute, which severely limits its practical utility. 
        
        We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. 
        Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency.
        
        We open-source (https://github.com/iclr2022anon/fast_finite_width_ntk) our two algorithms as general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc.) and introduce no new hyper-parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop and open-source a new algorithm for fast computation of the finite width Neural Tangent Kernel, the outer product of Jacobians of a neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zLb9oSWy933&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zaALYtvbRlH" data-number="4574">
        <h4>
          <a href="https://openreview.net/forum?id=zaALYtvbRlH">
              SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zaALYtvbRlH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Peng_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Qi1">Peng Qi</a>, <a href="https://openreview.net/profile?id=~Guangtao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangtao_Wang1">Guangtao Wang</a>, <a href="https://openreview.net/profile?id=~Jing_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jing_Huang3">Jing Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zaALYtvbRlH-details-987" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zaALYtvbRlH-details-987"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequential data, sample efficiency, data augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zaALYtvbRlH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PC8u74o7xc2" data-number="4571">
        <h4>
          <a href="https://openreview.net/forum?id=PC8u74o7xc2">
              Embedding models through the lens of Stable Coloring
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PC8u74o7xc2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aditya_Desai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Desai1">Aditya Desai</a>, <a href="https://openreview.net/profile?id=~Shashank_Sonkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shashank_Sonkar1">Shashank Sonkar</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PC8u74o7xc2-details-38" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PC8u74o7xc2-details-38"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Embedding-based approaches find the semantic meaning of tokens in structured data such as natural language, graphs, and even images. To a great degree, these approaches have developed independently in different domains. However, we find a common principle underlying these formulations, and it is rooted in solutions to the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For instance, we find links between stable coloring, distribution hypothesis in natural language processing, and non-local-means denoising algorithm in image signal processing. We even find that stable coloring has strong connections to a broad class of unsupervised embedding models which is surprising at first since stable coloring is generally applied for combinatorial problems. To establish this connection concretely we define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them. Grounded on this framework, we show that many algorithms ranging across different domains are, in fact, searching for continuous stable coloring solutions of an underlying graph corresponding to the domain.  We show that popular and widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and Vis-Transformer can be understood  as instantiations of our general algorithm that solves the problem of continuous stable coloring. These instantiations offer useful insights into the workings of state-of-the-art models like BERT stimulating new research directions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose unified theoretical framework underlying the state-of-the art embedding models</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XIZaWGCPl0b" data-number="4564">
        <h4>
          <a href="https://openreview.net/forum?id=XIZaWGCPl0b">
              Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XIZaWGCPl0b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Atul_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Atul_Sharma1">Atul Sharma</a>, <a href="https://openreview.net/profile?id=~Wei_Chen26" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Chen26">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Joshua_Christian_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_Christian_Zhao1">Joshua Christian Zhao</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Somali_Chaterji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Somali_Chaterji1">Somali Chaterji</a>, <a href="https://openreview.net/profile?id=~Saurabh_Bagchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Bagchi1">Saurabh Bagchi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">33 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XIZaWGCPl0b-details-970" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XIZaWGCPl0b-details-970"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, aggregation, security, untargeted model poisoning attack</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learningâ€”multi-party, distributed learning in a decentralized environmentâ€”is vulnerable to model poisoning attacks, even more so than centralized learning approaches.  This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Trimmed mean, and FoolsGold.  However, a recently developed targeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be pushed away from the optima to increase the test error rate. In this work, we develop tesseractâ€”a defense against this directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT is based on a simple intuition that in a federated learning setting, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that TESSERACT provides robustness against even an adaptive white-box version of the attack.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">How to defend federated learning against local model poisoning attack, the most effective attack known to date, using the pattern of progression of gradients as each client learns.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XIZaWGCPl0b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rX3rZYP8zZF" data-number="4561">
        <h4>
          <a href="https://openreview.net/forum?id=rX3rZYP8zZF">
              CareGraph: A Graph-based Recommender System for Diabetes Self-Care
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rX3rZYP8zZF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sirinart_Tangruamsub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sirinart_Tangruamsub1">Sirinart Tangruamsub</a>, <a href="https://openreview.net/profile?id=~Karthik_Kappaganthu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Kappaganthu1">Karthik Kappaganthu</a>, <a href="https://openreview.net/profile?email=jodonovan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="jodonovan@teladochealth.com">John O'Donovan</a>, <a href="https://openreview.net/profile?email=anmol.madan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="anmol.madan@teladochealth.com">Anmol Madan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rX3rZYP8zZF-details-366" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rX3rZYP8zZF-details-366"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graph, knowledge graph embedding, recommendation system</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management.  We propose a Deep Neural Network-based recommender that uses the knowledge graph embeddings to recommend health nudges for maximizing engagement by combating the cold-start and sparsity problems. We use a leave-one-out approach to evaluate the model. We compare the proposed model performance with a text similarity and Deep-and-Cross Network-based approach as the baseline. The overall improvement in Click-Through-Rate prediction AUC for the Knowledge-Graph-based model was 11%. We also observe that our model improved the average AUC by 5% in cold-start situations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nct9j3BVswZ" data-number="4558">
        <h4>
          <a href="https://openreview.net/forum?id=Nct9j3BVswZ">
              Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nct9j3BVswZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Sercan_O_Arik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sercan_O_Arik1">Sercan O Arik</a>, <a href="https://openreview.net/profile?id=~Chen-Yu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen-Yu_Lee2">Chen-Yu Lee</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tomas_Pfister1">Tomas Pfister</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nct9j3BVswZ-details-58" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nct9j3BVswZ-details-58"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Anomaly detection, Data refinement, Iterative training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classification method by 6.3 AUC and 12.5 average precision / 22.9 F1-score.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QKEkEFpKBBv" data-number="4557">
        <h4>
          <a href="https://openreview.net/forum?id=QKEkEFpKBBv">
              DNBP: Differentiable Nonparametric Belief Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QKEkEFpKBBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anthony_Opipari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anthony_Opipari1">Anthony Opipari</a>, <a href="https://openreview.net/profile?id=~Jana_Pavlasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jana_Pavlasek1">Jana Pavlasek</a>, <a href="https://openreview.net/profile?email=joecc%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="joecc@umich.edu">Chao Chen</a>, <a href="https://openreview.net/profile?email=shoutian%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shoutian@umich.edu">Shoutian Wang</a>, <a href="https://openreview.net/profile?id=~Karthik_Desingh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Desingh1">Karthik Desingh</a>, <a href="https://openreview.net/profile?id=~Odest_Jenkins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Odest_Jenkins1">Odest Jenkins</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QKEkEFpKBBv-details-615" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QKEkEFpKBBv-details-615"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Belief Propagation, Bayesian Inference, Nonparametric Inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. Existing nonparametric belief propagation methods rely on domain-specific features encoded in the probabilistic factors of a graphical model. In this work, we replace each crafted factor with a differentiable neural network enabling the factors to be learned using an efficient optimization routine from labeled data. By combining differentiable neural networks with an efficient belief propagation algorithm, our method learns to maintain a set of marginal posterior samples using end-to-end training. We evaluate our differentiable nonparametric belief propagation (DNBP) method on a set of articulated pose tracking tasks and compare performance with learned baselines. Results from these experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches. The project webpage is available at: https://sites.google.com/view/diff-nbp</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=QKEkEFpKBBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TEt7PsVZux6" data-number="4540">
        <h4>
          <a href="https://openreview.net/forum?id=TEt7PsVZux6">
              I-PGD-AT: Efficient Adversarial Training via Imitating Iterative PGD Attack 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TEt7PsVZux6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaosen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaosen_Wang1">Xiaosen Wang</a>, <a href="https://openreview.net/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>, <a href="https://openreview.net/profile?id=~Krishnaram_Kenthapadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krishnaram_Kenthapadi1">Krishnaram Kenthapadi</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TEt7PsVZux6-details-245" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TEt7PsVZux6-details-245"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Single-step Adversarial Training, Catastrophic Overfitting, Adversarial Robustness, Adversarial Example</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial training has been widely used in various machine learning paradigms to improve the robustness; while it would increase the training cost due to the perturbation optimization process. To improve the efficiency, recent studies leverage Fast Gradient Sign Method with Random Start (FGSM-RS) for adversarial training. However, such methods would lead to relatively low robustness and catastrophic overfitting, which means the robustness against iterative attacks (e.g. Projected Gradient Descent (PGD)) would suddenly drop to 0%. Different approaches have been proposed to address this problem, while later studies show that catastrophic overfitting still remains. In this paper, motivated by the fact that expensive iterative adversarial training methods achieve high robustness without catastrophic overfitting, we aim to ask: Can we perform iterative adversarial training in an efficient way? To this end, we first analyze the difference of perturbation generated by FGSM-RS and PGD and find that PGD tends to craft diverse discrete values instead of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="81" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Â±</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> in FGSM-RS. Based on this observation, we propose an efficient single-step adversarial training method I-PGD-AT by adopting I-PGD attack for training, in which I-PGD imitates PGD virtually. Unlike FGSM that crafts the perturbation directly using the sign of gradient, I-PGD imitates the perturbation of PGD based on the magnitude of gradient. Extensive empirical evaluations on CIFAR-10 and Tiny ImageNet demonstrate that our I-PGD-AT can improve the robustness compared with the baselines and significantly delay catastrophic overfitting. Moreover, we explore and discuss the factors that affect catastrophic overfitting. Finally, to demonstrate the generality of I-PGD-AT, we integrate it into PGD adversarial training and show that it can even further improve the robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an efficient adversarial training approach I-PGD-AT by imitating PGD virtually to improve single-step adversarial training effectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TEt7PsVZux6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wQ7RCayXUSl" data-number="4539">
        <h4>
          <a href="https://openreview.net/forum?id=wQ7RCayXUSl">
              Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters.
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wQ7RCayXUSl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seyed_Kamyar_Seyed_Ghasemipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyed_Kamyar_Seyed_Ghasemipour1">Seyed Kamyar Seyed Ghasemipour</a>, <a href="https://openreview.net/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Nachum1">Ofir Nachum</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wQ7RCayXUSl-details-5" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQ7RCayXUSl-details-5"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline reinforcement learning, batch reinforcement learning, ensembles, uncertainty estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order to achieve strong performance in offline reinforcement learning (RL),  it is necessary to act conservatively with respect to confident lower-bounds on anticipated values of actions. Thus, a valuable approach would be to obtain high quality uncertainty estimates on action values. In current supervised learning literature, state-of-the-art approaches to uncertainty estimation and calibration rely on ensembling methods. In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL. We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble. Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods. Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains. In the more challenging domains of the D4RL offline RL benchmark, MSG significantly surpasses highly well-tuned state-of-the-art methods in batch RL. Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective. We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles. We hope our work engenders increased focus into deep network uncertainty estimation techniques directed for reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We demonstrate how significantly beneficial uncertainty estimation through ensembles can be for offline RL and demonstrate much work is still needed for efficient ensembles to be as effective as deep ensembles.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQ7RCayXUSl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="E9e18Ms5TeV" data-number="4537">
        <h4>
          <a href="https://openreview.net/forum?id=E9e18Ms5TeV">
              A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=E9e18Ms5TeV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zachary_Nado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zachary_Nado1">Zachary Nado</a>, <a href="https://openreview.net/profile?id=~Justin_Gilmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Justin_Gilmer1">Justin Gilmer</a>, <a href="https://openreview.net/profile?id=~Christopher_J_Shallue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_J_Shallue1">Christopher J Shallue</a>, <a href="https://openreview.net/profile?id=~Rohan_Anil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Anil1">Rohan Anil</a>, <a href="https://openreview.net/profile?id=~George_Edward_Dahl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Edward_Dahl1">George Edward Dahl</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#E9e18Ms5TeV-details-896" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E9e18Ms5TeV-details-896"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural networks, deep learning, neural network optimization, hyperparameter tuning, optimizer comparison</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We retune the Nesterov/Adam optimizers on pipelines where LARS/LAMB are commonly used and achieve similar or better performance, providing competitive baselines for the large batch training setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=E9e18Ms5TeV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ueTHF0yAlZ" data-number="4533">
        <h4>
          <a href="https://openreview.net/forum?id=5ueTHF0yAlZ">
              Improving greedy core-set configurations for active learning with uncertainty-scaled distances
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ueTHF0yAlZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuchen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuchen_Li1">Yuchen Li</a>, <a href="https://openreview.net/profile?id=~Frank_Rudzicz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_Rudzicz2">Frank Rudzicz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ueTHF0yAlZ-details-544" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ueTHF0yAlZ-details-544"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Active learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We scale perceived distances of the core-set algorithm by a factor of uncertainty and search for low-confidence configurations, finding significant improvements in sample efficiency across CIFAR10/100 and SVHN image classification, especially in larger acquisition sizes. We show the necessity of our modifications and explain how the improvement is due to a probabilistic quadratic speed-up in the convergence of core-set loss, under assumptions about the relationship of model uncertainty and misclassification.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Improved core-set for active learning using confidence-scaled distances.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ueTHF0yAlZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nus6fOfh1HW" data-number="4531">
        <h4>
          <a href="https://openreview.net/forum?id=Nus6fOfh1HW">
              On the Relationship between Heterophily and Robustness of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nus6fOfh1HW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiong_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiong_Zhu1">Jiong Zhu</a>, <a href="https://openreview.net/profile?id=~Junchen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junchen_Jin1">Junchen Jin</a>, <a href="https://openreview.net/profile?id=~Donald_Loveland2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Donald_Loveland2">Donald Loveland</a>, <a href="https://openreview.net/profile?id=~Michael_T_Schaub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_T_Schaub1">Michael T Schaub</a>, <a href="https://openreview.net/profile?id=~Danai_Koutra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danai_Koutra1">Danai Koutra</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nus6fOfh1HW-details-142" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nus6fOfh1HW-details-142"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph neural networks, adversarial attacks, heterophily, structural perturbation, robustness, relation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels). In this work, we formalize the relation between heterophily and robustness, bridging two topics previously investigated by separate lines of research. We theoretically and empirically show that for graphs exhibiting homophily (low heterophily), impactful structural attacks always lead to increased levels of heterophily, while for graph with heterophily the change in the homophily level depends on the node degrees. By leveraging these insights, we deduce that a design principle identified to significantly improve predictive performance under heterophilyâ€”separate aggregators for ego- and neighbor-embeddingsâ€”can also inherently offer increased robustness to GNNs. Our extensive empirical analysis shows that GNNs adopting this design alone can achieve significantly improved empirical and certifiable robustness compared to the best-performing unvaccinated model. Furthermore, models with this design can be readily combined with explicit defense mechanisms to yield improved robustness with up to 18.33% increase in performance under attacks compared to the best-performing vaccinated model.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We explore the interplay between heterophily &amp; robustness in GNNs, and show that 1) effective structural attacks on homophilous graphs increase heterophily, 2) heterophilous GNN designs can be combined with defense mechanisms for improved robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Nus6fOfh1HW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-29uFS4FiDZ" data-number="4524">
        <h4>
          <a href="https://openreview.net/forum?id=-29uFS4FiDZ">
              Word Sense Induction with Knowledge Distillation from BERT
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-29uFS4FiDZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anik_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anik_Saha1">Anik Saha</a>, <a href="https://openreview.net/profile?id=~Alex_Gittens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Gittens1">Alex Gittens</a>, <a href="https://openreview.net/profile?id=~Bulent_Yener2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bulent_Yener2">Bulent Yener</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-29uFS4FiDZ-details-453" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-29uFS4FiDZ-details-453"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embeddings, sense embeddings, word sense induction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems.  Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Effective approach to distil word meaning from contextual embeddings to word sense embeddings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PGGjnBiQ84G" data-number="4520">
        <h4>
          <a href="https://openreview.net/forum?id=PGGjnBiQ84G">
              Learning Surface Parameterization for Document Image Unwarping
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PGGjnBiQ84G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sagnik_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sagnik_Das1">Sagnik Das</a>, <a href="https://openreview.net/profile?id=~Ke_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ke_Ma3">Ke Ma</a>, <a href="https://openreview.net/profile?id=~Zhixin_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhixin_Shu1">Zhixin Shu</a>, <a href="https://openreview.net/profile?id=~Dimitris_Samaras3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dimitris_Samaras3">Dimitris Samaras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PGGjnBiQ84G-details-547" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PGGjnBiQ84G-details-547"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit functions, texture mapping, surface parameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping. We propose an efficient method to learn surface parameterization by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates. Our surface parameterization network can be conveniently plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Recent work on differentiable rendering techniques for implicit surfaces has shown high-quality 3D scene reconstruction and view synthesis results. However, these methods typically learn the appearance color as a function of the surface points and lack explicit surface parameterization. Thus they do not allow texture map extraction or texture editing. By introducing explicit surface parameterization and learning with a recent differentiable renderer for implicit surfaces, we demonstrate state-of-the-art document-unwarping via texture extraction. We show that our approach can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. We also demonstrate the usefulness of our system by applying it to document texture editing.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning surface parameterization using rendering loss and multiview images</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PGGjnBiQ84G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4JlwgTbmzXQ" data-number="4519">
        <h4>
          <a href="https://openreview.net/forum?id=4JlwgTbmzXQ">
              EqR: Equivariant Representations for Data-Efficient Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4JlwgTbmzXQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnab_Kumar_Mondal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnab_Kumar_Mondal1">Arnab Kumar Mondal</a>, <a href="https://openreview.net/profile?id=~Vineet_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vineet_Jain1">Vineet Jain</a>, <a href="https://openreview.net/profile?id=~Kaleem_Siddiqi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaleem_Siddiqi1">Kaleem Siddiqi</a>, <a href="https://openreview.net/profile?id=~Siamak_Ravanbakhsh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siamak_Ravanbakhsh1">Siamak Ravanbakhsh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4JlwgTbmzXQ-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4JlwgTbmzXQ-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Equivariance, Invariance, Representation learning, Reinforcement learning, Symmetric MDPs, MDP homomorphism, Lie parameterization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agentâ€™s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Equivariant representation learning for data-efficient reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=4JlwgTbmzXQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JHXjK94yH-y" data-number="4518">
        <h4>
          <a href="https://openreview.net/forum?id=JHXjK94yH-y">
              Explore and Control with Adversarial Surprise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JHXjK94yH-y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnaud_Fickinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnaud_Fickinger1">Arnaud Fickinger</a>, <a href="https://openreview.net/profile?id=~Natasha_Jaques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Natasha_Jaques1">Natasha Jaques</a>, <a href="https://openreview.net/profile?id=~Samyak_Parajuli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samyak_Parajuli1">Samyak Parajuli</a>, <a href="https://openreview.net/profile?id=~Michael_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Chang1">Michael Chang</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stuart_Russell1">Stuart Russell</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JHXjK94yH-y-details-764" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JHXjK94yH-y-details-764"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, intrinsic motivation, exploration, multi-agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering. However, a central challenge in unsupervised RL is to extract behaviors that meaningfully affect the world and cover the range of possible outcomes, without getting distracted by inherently unpredictable, uncontrollable, and stochastic elements in the environment. To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences. The Explore agent seeks out states that maximally surprise the Control agent, which in turn aims to minimize surprise, and thereby manipulate the environment to return to familiar and predictable states. The competition between these two policies drives them to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions. Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Two policies play a multi-agent adversarial game over the amount of surprise or observation entropy an agent experiences, leading the agent to fully explore the underlying state space and learn meaningful behaviors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ck_iw4jMC4l" data-number="4508">
        <h4>
          <a href="https://openreview.net/forum?id=Ck_iw4jMC4l">
              Logical Activation Functions: Logit-space equivalents of Boolean Operators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ck_iw4jMC4l" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Scott_C_Lowe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Scott_C_Lowe1">Scott C Lowe</a>, <a href="https://openreview.net/profile?email=robearle11%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="robearle11@gmail.com">Robert Earle</a>, <a href="https://openreview.net/profile?id=~Jason_d%26%23x27%3BEon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_d&#39;Eon1">Jason d'Eon</a>, <a href="https://openreview.net/profile?id=~Thomas_Trappenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Trappenberg1">Thomas Trappenberg</a>, <a href="https://openreview.net/profile?id=~Sageev_Oore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sageev_Oore1">Sageev Oore</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ck_iw4jMC4l-details-220" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ck_iw4jMC4l-details-220"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">activation functions, logits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence (versus absence) of features within the stimulus. Under this interpretation, we can derive the probability <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="82" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>âˆ©</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> that a pair of independent features are both present in the stimulus from their logits. By converting the resulting probability back into a logit, we obtain a logit-space equivalent of the AND operation. However, since this function involves taking multiple exponents and logarithms, it is not well suited to be directly used within neural networks. We thus constructed an efficient approximation named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="83" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> (the AND operator Approximate for Independent Logits) utilizing only comparison and addition operations, which can be deployed as an activation function in neural networks. Like MaxOut, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="84" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> is a generalization of ReLU to two-dimensions. Additionally, we constructed efficient approximations of the logit-space equivalents to the OR and XNOR operators. We deployed these new activation functions, both in isolation and in conjunction, and demonstrated their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We motivate novel activation functions which are logit-space equivalents to boolean operations, and find they work well on a wide variety of tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ofLwshMBL_H" data-number="4507">
        <h4>
          <a href="https://openreview.net/forum?id=ofLwshMBL_H">
              Continual Learning Using Task Conditional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ofLwshMBL_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Honglin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Honglin_Li1">Honglin Li</a>, <a href="https://openreview.net/profile?id=~Frieder_Ganz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frieder_Ganz1">Frieder Ganz</a>, <a href="https://openreview.net/profile?id=~David_J._Sharp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_J._Sharp1">David J. Sharp</a>, <a href="https://openreview.net/profile?id=~Payam_M._Barnaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_M._Barnaghi1">Payam M. Barnaghi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ofLwshMBL_H-details-816" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ofLwshMBL_H-details-816"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">catastrophic forgetting, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning changes, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. Dynamic approaches, which assign new neuron resources to the upcoming tasks, are introduced to address this issue. However, most of the dynamic methods need task information about the upcoming tasks during the inference phase to activate the corresponding neurons. To address this issue, we introduce Task Conditional Neural Network which allows the model to identify the task information automatically. The proposed model can continually learn and embed new tasks into the model without losing the information about previously learned tasks. We evaluate the proposed model combined with the mixture of experts approach on the MNIST and CIFAR100 datasets and show how it significantly improves the continual learning process without requiring task information in advance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A dynamic approach for continual learning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xspalMXAB0M" data-number="4483">
        <h4>
          <a href="https://openreview.net/forum?id=xspalMXAB0M">
              A Boosting Approach to Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xspalMXAB0M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nataly_Brukhim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nataly_Brukhim1">Nataly Brukhim</a>, <a href="https://openreview.net/profile?id=~Elad_Hazan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elad_Hazan1">Elad Hazan</a>, <a href="https://openreview.net/profile?id=~Karan_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karan_Singh1">Karan Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xspalMXAB0M-details-260" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xspalMXAB0M-details-260"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form.
            Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function, or by relaxing the solution guarantee to a local optimality condition.
        
            We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an effective policy. The notion of weak learning we study is that of sampled-based approximate optimization of linear functions over policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods iteratively. We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular, our bound does not explicitly depend on the number of states.
        
            A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global optimality guarantees.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xspalMXAB0M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HUeyM2qVey2" data-number="4482">
        <h4>
          <a href="https://openreview.net/forum?id=HUeyM2qVey2">
              Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HUeyM2qVey2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michael_Anthony_Puthawala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Anthony_Puthawala1">Michael Anthony Puthawala</a>, <a href="https://openreview.net/profile?id=~Matti_Lassas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matti_Lassas1">Matti Lassas</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_DokmaniÄ‡1">Ivan DokmaniÄ‡</a>, <a href="https://openreview.net/profile?id=~Maarten_V._de_Hoop2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maarten_V._de_Hoop2">Maarten V. de Hoop</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HUeyM2qVey2-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HUeyM2qVey2-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Universality, Flow Networks, Manifold Learning, Density Estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements. We find that such networks universally approximate a large class of manifolds simultaneously with densities supported on them. Among others, our results apply to the well-known coupling and autoregressive flows. We build on the work of Teshima et al. 2020 on bijective flows and study injective architectures proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a new theoretical device called the \emph{embedding gap}, which measures how far one continuous manifold is from embedding another. We relate the embedding gap to a relaxation of universally we call the \emph{manifold embedding property}, capturing the geometric part of universality. Our proof also establishes that optimality of a network can be established ``in reverse,''  resolving a conjecture made in Brehmer et al. 2020 and opening the door for simple layer-wise training schemes. Finally, we show that the studied networks admit an exact layer-wise projection result, Bayesian uncertainty quantification, and black-box recovery of network weights.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements and find that such networks universally approximate a large class of manifolds and densities there on.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zrdUVVAvcP2" data-number="4477">
        <h4>
          <a href="https://openreview.net/forum?id=zrdUVVAvcP2">
              GrASP: Gradient-Based Affordance Selection for Planning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zrdUVVAvcP2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vivek_Veeriah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vivek_Veeriah2">Vivek Veeriah</a>, <a href="https://openreview.net/profile?id=~Zeyu_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zeyu_Zheng1">Zeyu Zheng</a>, <a href="https://openreview.net/profile?id=~Richard_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Lewis1">Richard Lewis</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satinder_Singh2">Satinder Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zrdUVVAvcP2-details-99" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zrdUVVAvcP2-details-99"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, affordances</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for \emph{selecting} affordances useful for planning---for learning which small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option  affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning to select affordances in the form of options and primitive actions for lookahead planning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="14kbUbOaZUc" data-number="4476">
        <h4>
          <a href="https://openreview.net/forum?id=14kbUbOaZUc">
              Metric Learning on Temporal Graphs via Few-Shot Examples
          </a>
        
          
            <a href="https://openreview.net/pdf?id=14kbUbOaZUc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongqi_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongqi_Fu1">Dongqi Fu</a>, <a href="https://openreview.net/profile?id=~Liri_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liri_Fang1">Liri Fang</a>, <a href="https://openreview.net/profile?id=~Ross_Maciejewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ross_Maciejewski1">Ross Maciejewski</a>, <a href="https://openreview.net/profile?id=~Vetle_I_Torvik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vetle_I_Torvik1">Vetle I Torvik</a>, <a href="https://openreview.net/profile?id=~Jingrui_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingrui_He1">Jingrui He</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#14kbUbOaZUc-details-668" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="14kbUbOaZUc-details-668"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Metric Learning, Few-Shot Learning, Temporal Graph</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs; Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales. Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The first attempt to learn temporal graph representations, on the graph-level, covering the whole lifetime, and only consuming a few labeled samples. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=14kbUbOaZUc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a61qArWbjw_" data-number="4475">
        <h4>
          <a href="https://openreview.net/forum?id=a61qArWbjw_">
              Scalable multimodal variational autoencoders with surrogate joint posterior
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a61qArWbjw_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Masahiro_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Masahiro_Suzuki1">Masahiro Suzuki</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yutaka_Matsuo1">Yutaka Matsuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a61qArWbjw_-details-133" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a61qArWbjw_-details-133"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep generative models, multimodal learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To obtain a joint representation from multimodal data in variational autoencoders (VAEs), it is important to infer the representation from arbitrary subsets of modalities after learning.  A scalable way to achieve this is to aggregate the inferences of each modality as experts. A state-of-the-art approach to learning this aggregation of experts is to encourage all modalities to be reconstructed and cross-generated from arbitrary subsets. However, this learning may be insufficient if cross-generation is difficult. Furthermore, to evaluate its objective function, exponential generation paths concerning the number of modalities are required. To alleviate these problems, we propose to explicitly minimize the divergence between inferences from arbitrary subsets and the surrogate joint posterior that approximates the true joint posterior. We also proposed using a gradient origin network, a deep generative model that learns inferences without using an inference network, thereby reducing the need for additional parameters by introducing the surrogate posterior. We demonstrate that our method performs better than existing scalable multimodal VAEs in inference and generation.    
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We proposed a scalable and high performance multimodal VAE in the framework of approximating inferences from arbitrary subsets of modalities to a surrogate joint posterior.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fwJWhOxuzV9" data-number="4471">
        <h4>
          <a href="https://openreview.net/forum?id=fwJWhOxuzV9">
              Semi-supervised Offline Reinforcement Learning with Pre-trained Decision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fwJWhOxuzV9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Catherine_Cang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Catherine_Cang1">Catherine Cang</a>, <a href="https://openreview.net/profile?id=~Kourosh_Hakhamaneshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Hakhamaneshi1">Kourosh Hakhamaneshi</a>, <a href="https://openreview.net/profile?id=~Ryan_Rudes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryan_Rudes1">Ryan Rudes</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Aravind_Rajeswaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aravind_Rajeswaran1">Aravind Rajeswaran</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fwJWhOxuzV9-details-563" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fwJWhOxuzV9-details-563"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-task RL, Decision Transformer, self-supervised RL, Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training deep neural network models using large unlabelled datasets followed by fine-tuning them on small task-specific datasets has emerged as a dominant paradigm in natural language processing (NLP) and computer vision (CV). Despite the widespread success, such a paradigm has remained atypical in reinforcement learning (RL).
        In this paper, we investigate how we can leverage large reward-free (i.e. task-agnostic) offline datasets of prior interactions to pre-train agents that can then be fine-tuned using a small reward-annotated dataset. To this end, we present Pre-trained Decision Transformer (PDT), a simple yet powerful algorithm for semi-supervised Offline RL. By masking reward tokens during pre-training, the transformer learns to autoregressivley predict actions based on previous state and action context and effectively extracts behaviors present in the dataset. During fine-tuning, rewards are un-masked and the agent learns the set of skills that should be invoked for the desired behavior as per the reward function. We demonstrate the efficacy of this simple and flexible approach on tasks from the D4RL benchmark with limited reward annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce Pre-trained Decision Transformers, a simple and flexible architecture that can be pre-trained on unlabeled environment interactions and can quickly adapt to several downstream tasks with just a small reward-annotated fine-tuning dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_Ko4kT3ckWy" data-number="4470">
        <h4>
          <a href="https://openreview.net/forum?id=_Ko4kT3ckWy">
              Increase and Conquer: Training Graph Neural Networks on Growing Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_Ko4kT3ckWy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Juan_Cervino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_Cervino1">Juan Cervino</a>, <a href="https://openreview.net/profile?id=~Luana_Ruiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luana_Ruiz1">Luana Ruiz</a>, <a href="https://openreview.net/profile?id=~Alejandro_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alejandro_Ribeiro1">Alejandro Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_Ko4kT3ckWy-details-22" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_Ko4kT3ckWy-details-22"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Graph Neural Networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) use graph convolutions to exploit network invariances and learn meaningful features from network data. However, on large-scale graphs convolutions incur in high computational cost, leading to scalability limitations. Leveraging the graphon --- the limit object of a graph --- in this paper we consider the problem of learning a graphon neural network (WNN) --- the limit object of a GNN --- by training GNNs on graphs sampled Bernoulli from the graphon. Under smoothness conditions, we show that: (i) the expected distance between the learning steps on the GNN and on the WNN decreases asymptotically with the size of the graph, and (ii) when training on a sequence of growing graphs, gradient descent follows the learning direction of the WNN. Inspired by these results, we propose a novel algorithm to learn GNNs on large-scale graphs that, starting from a moderate number of nodes, successively increases the size of the graph during training. This algorithm is benchmarked on both a recommendation system and a decentralized control problem where it is shown to retain comparable performance, to its large-scale counterpart, at a reduced computational cost.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper describes a way to train GNNs on a sequence of growing graphs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_Ko4kT3ckWy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MQuxKr2F1Xw" data-number="4468">
        <h4>
          <a href="https://openreview.net/forum?id=MQuxKr2F1Xw">
              Multi-Trigger-Key: Towards Multi-Task Privacy-Preserving In Deep Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MQuxKr2F1Xw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ren_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ren_Wang1">Ren Wang</a>, <a href="https://openreview.net/profile?id=~Zhe_Xu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhe_Xu7">Zhe Xu</a>, <a href="https://openreview.net/profile?id=~Alfred_Hero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alfred_Hero1">Alfred Hero</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MQuxKr2F1Xw-details-540" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MQuxKr2F1Xw-details-540"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning-based Multi-Task Classification (MTC) is widely used in applications like facial attribute and healthcare that warrant strong privacy guarantees. In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective. MTK associates each secured task in the multi-task dataset with a specifically designed trigger-key. The true information can be revealed by adding the trigger-key if the user is authorized. We obtain such an MTK model by training it with a newly generated training set. To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance. Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=MQuxKr2F1Xw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="31">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="desk-rejected-withdrawn-submissions">

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="XNYOJD0QdBD" data-number="4713">
        <h4>
          <a href="https://openreview.net/forum?id=XNYOJD0QdBD">
              Personalized PageRank meets Graph Attention Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XNYOJD0QdBD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Julie_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julie_Choi1">Julie Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XNYOJD0QdBD-details-956" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XNYOJD0QdBD-details-956"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GNN, Personalized PageRank, Graph Attention Network, Graph Neural Network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rising interest in graph neural networks (GNNs) for representation learning over the past few years. GNNs provide a general and efficient framework to learn from graph-structured data. However, GNNs typically only use the information of a very limited neighborhood for each node. A larger neighborhood would be desirable to provide the model with more information. However, increasing the size of the neighborhood is not trivial since neighborhood aggregation over many layers leads to over-smoothing. In this work, we incorporate the limit distribution of Personalized PageRank (PPR) into graph attention networks (GATs) to address this issue. Intuitively, message aggregation based on Personalized PageRank corresponds to infinitely many neighborhood aggregation layers. We show that our models outperform a variety ofbaseline models across all datasets used for our experiments. Our implementation is publicly available online.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Personalized PageRank meets Graph Attention Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HLTLhiBtUcW" data-number="4712">
        <h4>
          <a href="https://openreview.net/forum?id=HLTLhiBtUcW">
              Enhanced neural network regularization with macro-block dropout
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HLTLhiBtUcW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HLTLhiBtUcW-details-214" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HLTLhiBtUcW-details-214"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">macro block dropout, regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> This paper proposes a new regularization algorithm referred to as macro-block dropout. The overfitting issue has been a difficult problem in training large network models. The dropout technique has proven to be simple yet very effective for regularization by preventing complex co-adaptations on training data.  In this work, we observe that in the hidden outputs, the correlations between geometrically close elements are usually stronger than those between distant elements. Motivated by this observation, we define a macro-block that contains multiple elements of the hidden output layer in order to reduce co-adaptations more effectively. Rather than applying dropout to each element, we apply random dropout to each macro-block. In our experiments with  image classification tasks on the MNIST and the ImageNet datasets as well as a speech recognition task on the LibriSpeech set, this simple algorithm has shown a quite significant improvement over the conventional dropout approach</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a macro-block dropout for better regularization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XY1DWeh58WR" data-number="4709">
        <h4>
          <a href="https://openreview.net/forum?id=XY1DWeh58WR">
              Deep Recurrent Neural Network Layers with Layerwise Loss
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XY1DWeh58WR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XY1DWeh58WR-details-203" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XY1DWeh58WR-details-203"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning techniques have brought significant performance improvement to various areas of machine learning. Especially in the computer vision area, very deep networks such as ResNet have shown notable performance improvement. However, in speech recognition or language processing, such kinds of a very deep network have not been extensively employed. In this paper, we propose a very deep LSTM structure and their training strategy. In our training strategy, we first start training a conventional model with several LSTM layers. One notable difference is that for the top LSTM layer of the initial model, the Connectionist Temporal Classification (CTC) loss is applied both to the input and output of this top LSTM layer. Once this initial model is sufficiently layered, this top layer is copied to construct a very deep LSTM stack. For this newly constructed stack, the CTC loss is applied to every output of the LSTM layer as well as the top of the stack. Experimental results show that this deep LSTM structure shows significantly better results than the conventional model with 5 ~ 6 layers with a comparable number of parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a very deep neural network with layerwise loss</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="np5BgCFSsbm" data-number="4433">
        <h4>
          <a href="https://openreview.net/forum?id=np5BgCFSsbm">
              Neocortical cell type classification from electrophysiology recordings using deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=np5BgCFSsbm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raymond_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raymond_Wang1">Raymond Wang</a>, <a href="https://openreview.net/profile?id=~Sang_Min_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sang_Min_Han1">Sang Min Han</a>, <a href="https://openreview.net/profile?id=~Marta_Agnieszka_Gajowa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marta_Agnieszka_Gajowa1">Marta Agnieszka Gajowa</a>, <a href="https://openreview.net/profile?id=~Chunlei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chunlei_Liu1">Chunlei Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#np5BgCFSsbm-details-640" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="np5BgCFSsbm-details-640"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuron type classification, convolutional neural network, electrophysiology</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the neural code requires identifying different functional units involved in the neural circuits. One way to identify these functional units is to solve a neuron type classification problem. For decades, current clamp electrophysiology recordings have provided the means to classify the neurons based on subtle differences in action potential shapes and spiking patterns. However, significant variations in neuronal type definitions, classification pipelines, and variability in the neuronal activities make unambiguous determination of neuron type challenging. Previous solutions to this electrophysiology-based cell type classification problem consisted of dimensionality reduction juxtaposed with clustering using hand-crafted action potential features. Recent discoveries have allowed genetic-based cell-type classifications, which have fewer ambiguities, but they are less practical in vivo and have even lower throughput. Leveraging the unprecedented ground truth data published in the Allen Institute Cell Types Database, which contains anatomical, genetic, and electrophysiology characterizations of neurons in the mouse neocortex, we construct a robust and efficient convolutional neural network (CNN) that successfully classifies neurons according to their genetic label or broad type (excitatory or inhibitory) solely using current-clamp electrophysiology recordings. The CNN is configured as a multiple-input single-output network consisting of three subnetworks that take in the raw time series electrophysiology recording as well as the real and imaginary components of its Fourier coefficients. Our single pipeline method is fast and streamlined while simultaneously outperforming previous methods and achieving more classification classes using only single current-clamp trace as the input. This end-to-end convolutional neural network-based classification method removes the need for hand-crafted features, specific knowledge, or human intervention for quick identification of the cell type with high accuracy, enabling interpretation of the experimental data in a bias-free manner and a much broader scientific context.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust and efficient convolutional neural network successfully classifies neurons according to their genetic label and broad type using only current-clamp electrophysiology recordings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NMSugaVzIT" data-number="4294">
        <h4>
          <a href="https://openreview.net/forum?id=NMSugaVzIT">
              Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMSugaVzIT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Meena_Jagadeesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Meena_Jagadeesan1">Meena Jagadeesan</a>, <a href="https://openreview.net/profile?id=~Ilya_Razenshteyn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ilya_Razenshteyn1">Ilya Razenshteyn</a>, <a href="https://openreview.net/profile?id=~Suriya_Gunasekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Suriya_Gunasekar1">Suriya Gunasekar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMSugaVzIT-details-919" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMSugaVzIT-details-919"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">minimizing parameter l2 norm, representation cost, implicit bias</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We provide a function space characterization of the inductive bias resulting from minimizing the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="85" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of the weights in multi-channel linear convolutional networks. We define an \textit{induced regularizer} in the function space as the minimum <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="86" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of weights of a network required to realize a function.  For two layer linear convolutional networks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="87" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> output channels and kernel size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="88" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, we show the following: (a) If the inputs to the network have a single channel, the induced regularizer for any <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="89" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is \textit{independent} of the number of output channels <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="90" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for networks with multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias \emph{does} depend on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="91" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. However, for sufficiently large <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="92" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>, the induced regularizer is again given by an SDP that is independent of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="93" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. In particular, the induced regularizer for  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="94" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="95" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mi>D</mi></math></mjx-assistive-mml></mjx-container> are given in closed form as the nuclear norm and the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="96" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> group-sparse norm, respectively, of the Fourier coefficients.
        We investigate the applicability of our theoretical results to a broader scope of ReLU convolutional networks through experiments on MNIST and CIFAR-10 datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the function space view of minimizing l2 norm of weights in multi-channel linear convolutional networks, uncovering an invariance to the number of output channels. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMSugaVzIT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DVSN9nJB1_" data-number="3792">
        <h4>
          <a href="https://openreview.net/forum?id=DVSN9nJB1_">
              E-LANG: Energy-based Joint Inferencing of Super and Swift Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DVSN9nJB1_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mohammad_Akbari3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Akbari3">Mohammad Akbari</a>, <a href="https://openreview.net/profile?id=~Amin_Banitalebi-Dehkordi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amin_Banitalebi-Dehkordi1">Amin Banitalebi-Dehkordi</a>, <a href="https://openreview.net/profile?id=~Yong_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yong_Zhang2">Yong Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DVSN9nJB1_-details-500" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DVSN9nJB1_-details-500"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based models, dynamic inference, joint language models, super model optimization, NLP, BERT, T5</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building very large and highly capable language models has been a trend in the past several years. Despite their great performance, they incur a high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the incoming samples to one of the two models based on the energy characteristics of the representations in the latent space. The proposed approach is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. Unlike existing methods that are for the most part only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The performance of the proposed Energy-based joint inferencing of LANGuage models, E-LANG, is verified through an extensive set of experiments with T5 and BERT architectures on GLUE, SuperGLUE, and WMT benchmarks. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA (state-of-the-art) on GLUE with 3.2X less computations. Code is available in the supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we present E-LANG, an energy-based joint inference approach, which combines Super and Swift language models for achieving efficient inference without sacrificing the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=DVSN9nJB1_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="k_Zy6glYaqc" data-number="3557">
        <h4>
          <a href="https://openreview.net/forum?id=k_Zy6glYaqc">
              Quantum Alphatron
          </a>
        
          
            <a href="https://openreview.net/pdf?id=k_Zy6glYaqc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyi_Yang1">Siyi Yang</a>, <a href="https://openreview.net/profile?id=~Patrick_Rebentrost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_Rebentrost1">Patrick Rebentrost</a>, <a href="https://openreview.net/profile?id=~Miklos_Santha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miklos_Santha1">Miklos Santha</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#k_Zy6glYaqc-details-782" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="k_Zy6glYaqc-details-782"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finding provably efficient algorithms for learning neural networks is a fundamental challenge in the theory of machine learning. The Alphatron of Goel and Klivans is the first provably efficient algorithm for learning neural networks with more than one nonlinear layer. The algorithm succeeds with any distribution on the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="97" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-dimensional unit ball and without any assumption on the structure of the network. In this work, we refine the original Alphatron by a pre-computing phase for its most time-consuming part, the evaluation of the kernel function. This refined algorithm improves the run time of the original Alphatron, while retaining the same learning guarantee. Based on the refined algorithm, we quantize the pre-computing phase with provable learning guarantee in the fault-tolerant quantum computing model. In a well-defined learning model, this quantum algorithm is able to provide a quadratic speedup in the data dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="98" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>. In addition, we discuss the second type of speedup, quantizing the evaluation of the gradient in the stochastic gradient descent procedure. Our work contributes to the study of quantum learning with kernels and from samples.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QWc35QxXPzZ" data-number="3480">
        <h4>
          <a href="https://openreview.net/forum?id=QWc35QxXPzZ">
              The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QWc35QxXPzZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chi_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chi_Jin1">Chi Jin</a>, <a href="https://openreview.net/profile?id=~Qinghua_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghua_Liu1">Qinghua Liu</a>, <a href="https://openreview.net/profile?id=~Tiancheng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tiancheng_Yu1">Tiancheng Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QWc35QxXPzZ-details-22" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QWc35QxXPzZ-details-22"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">theoretical reinforcement learning, Markov games with general function approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern reinforcement learning (RL) commonly engages practical problems with large state spaces, where function approximation must be deployed to approximate either the value function or the policy. While recent progresses in RL theory address a rich set of RL problems with general function approximation, such successes are mostly restricted to the single-agent setting. It remains elusive how to extend these results to multi-agent RL, especially due to the new challenges arising from its game-theoretical nature. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algorithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension -- a new complexity measure adapted from its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter, which facilitates the learning of the main player by deliberately exploiting her weakness. Our theoretical framework is generic, which applies to a wide range of models including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper studies sample-efficient learning of Markov Games with general function approximation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6Q5RdltG3L" data-number="3243">
        <h4>
          <a href="https://openreview.net/forum?id=6Q5RdltG3L">
              Human imperceptible attacks and applications to improve fairness
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6Q5RdltG3L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xinru_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xinru_Hua1">Xinru Hua</a>, <a href="https://openreview.net/profile?id=~Huanzhong_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huanzhong_Xu1">Huanzhong Xu</a>, <a href="https://openreview.net/profile?id=~Jose_Blanchet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jose_Blanchet1">Jose Blanchet</a>, <a href="https://openreview.net/profile?id=~Viet_Anh_Nguyen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Viet_Anh_Nguyen2">Viet Anh Nguyen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6Q5RdltG3L-details-179" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Q5RdltG3L-details-179"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural networks are able to perform at least as well as humans in numerous tasks involving object classification and image generation. However, there is also evidence that perturbations which are imperceptible to humans may significantly degrade the performance of well-trained deep neural networks. We provide a Distributionally Robust Optimization (DRO) framework which integrates human-based image quality assessment methods to design optimal attacks that are imperceptible to humans but significantly damaging to deep neural networks. Our attack algorithm can generate better-quality (less perceptible to humans) attacks than other state-of-the-art human imperceptible attack methods. We provide an algorithmic implementation of independent interest which can speed up DRO training significantly. Finally, we demonstrate how the use of optimally designed human imperceptible attacks can improve group fairness in image classification while maintaining a similar accuracy.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=6Q5RdltG3L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vNDHZZa-Q92" data-number="3220">
        <h4>
          <a href="https://openreview.net/forum?id=vNDHZZa-Q92">
              Neural Extensions: Training Neural Networks with Set Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vNDHZZa-Q92" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nikolaos_Karalias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikolaos_Karalias1">Nikolaos Karalias</a>, <a href="https://openreview.net/profile?id=~Joshua_David_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_David_Robinson1">Joshua David Robinson</a>, <a href="https://openreview.net/profile?id=~Andreas_Loukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Loukas1">Andreas Loukas</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vNDHZZa-Q92-details-615" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vNDHZZa-Q92-details-615"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continuous extensions, algorithmic reasoning, set functions, machine learning, combinatorial optimization, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integrating discrete computational steps into deep learning architectures is an important consideration when learning to reason over discrete items. However, many tasks that involve discrete choices are defined via (combinatorial) set functions, and thereby pose challenges for end-to-end training. In this work, we explore a general framework to construct continuous extensions of such discrete functions that enables training via gradient methods. Our framework includes well-known extensions such as the Lovasz extension of submodular set functions and facilitates the design of novel continuous extensions based on problem-specific considerations, including constraints. We demonstrate the versatility of our framework on tasks ranging from combinatorial optimization to image classification. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A principled framework for continuous extensions of set functions in machine learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oykI6Kmq3Xi" data-number="3194">
        <h4>
          <a href="https://openreview.net/forum?id=oykI6Kmq3Xi">
              Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oykI6Kmq3Xi" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ryann_Sim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryann_Sim1">Ryann Sim</a>, <a href="https://openreview.net/profile?id=~EFSTRATIOS_PANTELEIMON_SKOULAKIS2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~EFSTRATIOS_PANTELEIMON_SKOULAKIS2">EFSTRATIOS PANTELEIMON SKOULAKIS</a>, <a href="https://openreview.net/profile?id=~Lillian_J_Ratliff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lillian_J_Ratliff1">Lillian J Ratliff</a>, <a href="https://openreview.net/profile?id=~Georgios_Piliouras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Piliouras1">Georgios Piliouras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oykI6Kmq3Xi-details-729" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oykI6Kmq3Xi-details-729"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">extensive form games, network extensive form games, online learning, optimistic gradient descent ascent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFG) and particularly in EFGs with many agents lags far behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="99" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> rate of convergence to the set of Nash equilibria. Moreover, we show that the day-to-day behavior also converges to Nash with rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="100" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mrow data-mjx-texclass="ORD"><mo>âˆ’</mo><mi>t</mi></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for some game-dependent constant <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="101" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mo>&gt;</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide a formulation of network zero-sum extensive form games and show that optimistic gradient ascent admits fast convergence to Nash, both in time average and in the day-to-day sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oykI6Kmq3Xi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JvVFSmFV8G" data-number="3163">
        <h4>
          <a href="https://openreview.net/forum?id=JvVFSmFV8G">
              Which model to trust: assessing the influence of models on the performance of reinforcement learning algorithms for continuous control tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JvVFSmFV8G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Giacomo_Arcieri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Giacomo_Arcieri1">Giacomo Arcieri</a>, <a href="https://openreview.net/profile?email=woelfle%40fzi.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="woelfle@fzi.de">David WÃ¶lfle</a>, <a href="https://openreview.net/profile?id=~Eleni_Chatzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleni_Chatzi1">Eleni Chatzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JvVFSmFV8G-details-149" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JvVFSmFV8G-details-149"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, model-based reinforcement learning, deep learning, bayesian deep learning, gaussian processes, continuous control, model uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JvVFSmFV8G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ulj0tR-k7q" data-number="3070">
        <h4>
          <a href="https://openreview.net/forum?id=Ulj0tR-k7q">
              On strong convergence of the two-tower model for recommender system
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ulj0tR-k7q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~SHIRONG_XU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SHIRONG_XU1">SHIRONG XU</a>, <a href="https://openreview.net/profile?id=~Junhui_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junhui_Wang3">Junhui Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ulj0tR-k7q-details-12" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ulj0tR-k7q-details-12"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Artificial neural networks, Collaborative filtering, Empirical process, Recommender system, Two-tower model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recommender system is capable of predicting preferred items for a user by integrating information from similar users or items. A popular model in recommender system is the so-called two-tower model, which employs two deep neural networks to embed users and items into a low-dimensional space, and predicts ratings via the geometrical relationship of the embeddings of user and item in the embedded space. Even though it is popularly used for recommendations, its theoretical properties remain largely unknown. In this paper, we establish some asymptotic results of the two-tower model in terms of its strong convergence to the optimal recommender system, showing that it achieves a fast convergence rate depending on the intrinsic dimensions of inputs features. To the best of our knowledge, this is among the first attempts to establish the statistical guarantee of the two-tower model. Through numerical experiments, we also demonstrate that the two-tower model is capable of capturing the effects of users' and items' features on ratings, leading to higher prediction accuracy over its competitors in both simulated examples and a real application data set. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Establishing theoretical guarantee for the two-tower model in recommender system</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ulj0tR-k7q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UquMPXFTpgp" data-number="2976">
        <h4>
          <a href="https://openreview.net/forum?id=UquMPXFTpgp">
              Cluster Tree for Nearest Neighbor Search
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UquMPXFTpgp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dan_Kushnir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Kushnir1">Dan Kushnir</a>, <a href="https://openreview.net/profile?id=~Sandeep_Silwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandeep_Silwal1">Sandeep Silwal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UquMPXFTpgp-details-857" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UquMPXFTpgp-details-857"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nearest neighbor search, tree algorithms, graph cuts, random projections</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Tree-based algorithms are an important and widely used class of algorithms for  Nearest Neighbor Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in spite of possessing theoretical guarantees and strong practical performance, a major drawback of the RP tree is its lack of adaptability to the input dataset.
        
        Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing ClusterTree, a new tree based algorithm. Our approach utilizes randomness as in RP trees while adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees and other tree based methods for NNS while maintaining efficient construction time. In addition, we show theoretically and empirically that ClusterTree finds partitions which are superior to those found by RP trees in preserving the cluster structure of the input dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel tree-based algorithm for nearest neighbor search which adapts to the cluster structure of the input dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xGZcxaYbJBF" data-number="2193">
        <h4>
          <a href="https://openreview.net/forum?id=xGZcxaYbJBF">
              A Multi-Task Learning Algorithm for Non-personalized Recommendations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xGZcxaYbJBF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Zhang8">Jiawei Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xGZcxaYbJBF-details-854" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xGZcxaYbJBF-details-854"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Recommendation and Ranking, Non-personalized Recommendations, Multitask Learning, collaborative filtering, Two-tower DNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce a multi-task learning (MTL) algorithm for recommending non-personalized videos to watch next on industrial video sharing platforms. Personalized recommendations have been studied for decades, while researches on non-personalized solutions are very rare to be seen, which still remain a huge portion in industry. As an indispensable part in recommender system, non-personalized video recommender system also faces several real-world challenges, including maintaining high relevance between source item and target items, as well as achieving multiple competing ranking objectives. To solve these, we largely extended model-based collaborative filtering algorithm by adding related candidate generation stage, Two-tower DNN structure and a multi-task learning mechanism. Compared with typical baseline solutions, our proposed algorithm can capture both linear and non-linear relationships from user-item interactions, and live experiments demonstrate that it can significantly advance the state of the art on recommendation quality.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A multi-task learning (MTL) algorithm is introduced for recommending non-personalized videos to watch next on industrial video sharing platforms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XTzAhbVbKgq" data-number="2123">
        <h4>
          <a href="https://openreview.net/forum?id=XTzAhbVbKgq">
              Batched Lipschitz Bandits
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XTzAhbVbKgq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yasong_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yasong_Feng1">Yasong Feng</a>, <a href="https://openreview.net/profile?id=~Zengfeng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zengfeng_Huang1">Zengfeng Huang</a>, <a href="https://openreview.net/profile?id=~Tianyu_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyu_Wang4">Tianyu Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XTzAhbVbKgq-details-932" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XTzAhbVbKgq-details-932"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-armed bandits, online learning, batched bandits, Lipschitz bandits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we study the batched Lipschitz bandit problem, where the expected reward is Lipschitz and the reward observations are collected in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback setting. In particular, we show that for a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="102" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>-step problem with Lipschitz reward of zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="103" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, our algorithm achieves theoretically optimal regret rate of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="104" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.509em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.501em; margin-left: 0.056em;"><mjx-texatom size="s" texclass="ORD"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mi>T</mi><mrow data-mjx-texclass="ORD"><mfrac><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>2</mn></mrow></mfrac></mrow></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> using only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="105" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" style="font-size: 83.3%;"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mrow><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>T</mi></mrow><msub><mi>d</mi><mi>z</mi></msub></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> batches. For the lower bound, we show that in an environment with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="106" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>-batches, for any policy <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="107" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï€</mi></math></mjx-assistive-mml></mjx-container>, there exists a problem instance such that the expected regret is lower bounded by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="108" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.361em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c28" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 1.233em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-msup><mjx-mrow><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.763em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c29" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi mathvariant="normal">Î©</mi><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>âˆ’</mo><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow><mi>B</mi></msup></mrow></mfrac></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="109" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="110" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="111" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the dimension of the arm space. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel landscape-aware algorithm to solve the batched Lipschitz bandit problem, and show that our algorithm matches the optimal regret upper bound using less than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="112" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> batches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XTzAhbVbKgq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CdqsSPLNx-" data-number="2094">
        <h4>
          <a href="https://openreview.net/forum?id=CdqsSPLNx-">
              Deep Dynamic Attention Model with Gate Mechanism for Solving Time-dependent Vehicle Routing Problems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CdqsSPLNx-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Feng_Guo7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Guo7">Feng Guo</a>, <a href="https://openreview.net/profile?id=~Qu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qu_Wei1">Qu Wei</a>, <a href="https://openreview.net/profile?id=~Miao_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miao_Wang2">Miao Wang</a>, <a href="https://openreview.net/profile?id=~Zhaoxia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhaoxia_Guo1">Zhaoxia Guo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CdqsSPLNx--details-487" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CdqsSPLNx--details-487"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vehicle routing problems (VRPs) are a type of classical combinatorial optimization problems widely existing in logistics and transportation operations. There has been an increasing interest to use deep reinforcement learning (DRL) techniques to tackle VRPs, and previous DRL-based studies assumed time-independent travel times between customers. However, travel times in real-world road networks are time-varying, which need to be considered in practical VRPs. We thus propose a Deep Dynamic Attention Models with Gate Mechanisms (DDAM-GM) to learn heuristics for time-dependent VRPs (TDVRPs) in real-world road networks. It extracts the information of node location, node demand, and time-varying travel times between nodes to obtain enhanced node embeddings through a dimension-reducing MHA layer and a synchronous encoder. In addition, we use a gate mechanism to obtain better context embedding. On the basis of a 110-day travel time dataset with 240 time periods per day from an urban road network with 408 nodes and 1250 directed links, we conduct a series of experiments to validate the effectiveness of the proposed model on TDVRPs without and with consideration of time windows, respectively. Experimental results show that our model outperforms significantly two state-of-the-art DRL-based models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lDvJM5XUyrx" data-number="1436">
        <h4>
          <a href="https://openreview.net/forum?id=lDvJM5XUyrx">
              Towards Understanding Catastrophic Overfitting in Fast Adversarial Training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lDvJM5XUyrx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Renjie_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Renjie_Chen2">Renjie Chen</a>, <a href="https://openreview.net/profile?id=~Yuan_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan_Luo1">Yuan Luo</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisen_Wang1">Yisen Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lDvJM5XUyrx-details-664" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lDvJM5XUyrx-details-664"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robustness, Fast Adversarial Training, Catastrophic Overfitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">After adversarial training was proposed, a series of works focus on improving the compunational efficiency of adversarial training for deep neural networks (DNNs). Recently, FGSM based single-step adversarial training has been found to be able to train a robust model with the robustness comparable to the one trained by multi-step PGD, but it is an order of magnitude faster. However, there exists a failure mode called Catastrophic Overfitting (CO) where the network loses its robustness against PGD attack suddenly and can be hardly recovered by itself during the training process. In this paper, we identify that CO is closely related to the high-order terms in Taylor expansion after rethinking and decomposing the min-max problem in adversarial training. The negative high-order terms lead to a phenomenon called Perturbation Loss Distortion, which is the underlying cause of CO. Based on the observations, we propose a simple but effective regularization method named Fast Linear Adversarial Training (FLAT) to avoid CO in the single-step adversarial training by making the loss surface flat.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Fast Linear Adversarial Training (FLAT) can help prevent the Catastrophic Overfitting in single-step adversarial training.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lDvJM5XUyrx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZVqsBl2HapR" data-number="1434">
        <h4>
          <a href="https://openreview.net/forum?id=ZVqsBl2HapR">
              Error-based or target-based? A unifying framework for learning in recurrent spiking networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZVqsBl2HapR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cristiano_Capone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cristiano_Capone1">Cristiano Capone</a>, <a href="https://openreview.net/profile?id=~Paolo_Muratore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paolo_Muratore1">Paolo Muratore</a>, <a href="https://openreview.net/profile?id=~Pier_Stanislao_Paolucci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pier_Stanislao_Paolucci1">Pier Stanislao Paolucci</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZVqsBl2HapR-details-483" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZVqsBl2HapR-details-483"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">target-based, error-based, recurrent neural network, spiking neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in biological or artificial networks means changing the laws governing the network dynamics in order to better behave in a specific situation. In the field of supervised learning, two complementary approaches stand out: error-based and target-based learning. However, there exists no consensus on which is better suited for which task, and what is the most biologically plausible. Here we propose a comprehensive theoretical framework that includes these two frameworks as special cases. This novel theoretical formulation offers major insights into the differences between the two approaches. In particular, we show how target-based naturally emerges from error-based when the number of constraints on the target dynamics, and as a consequence on the internal network dynamics, is comparable to the degrees of freedom of the network. Moreover, given the experimental evidences on the relevance that spikes have in biological networks, we investigate the role of coding with specific patterns of spikes by introducing a parameter that defines the tolerance to precise spike timing during learning. Our approach naturally lends itself to Imitation Learning (and Behavioral Cloning in particular) and we apply it to solve relevant closed-loop tasks such as the button-and-food task, and the 2D Bipedal Walker. We show that a high dimensionality feedback structure is extremely important when it is necessary to solve a task that requires retaining memory for a long time (button-and-food). On the other hand, we find that coding with specific patterns of spikes enables optimal performances in a motor task (the 2D Bipedal Walker). Finally, we show that our theoretical formulation suggests protocols to deduce the structure of learning feedback in biological networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new learning framework that includes target- and error-based approches as special cases. It allows to understand their relationship and to explore what learning rule is optimal in the different tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZVqsBl2HapR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="x8l2miKNqPb" data-number="1409">
        <h4>
          <a href="https://openreview.net/forum?id=x8l2miKNqPb">
              Generate Triggers  in Neural Relation Extraction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=x8l2miKNqPb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Yujiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Yujiang1">Liu Yujiang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#x8l2miKNqPb-details-180" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x8l2miKNqPb-details-180"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">relation triggersï¼Œevolutive maskï¼Œ pointer network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the relation extraction task, the relationship between two entities is determined by some specific words in their source text. These words are called relation triggers, which are the evidence to explain the relationship; other words are called ir-relevant words. The current relationship extraction neural network model aims at identifying the relation type between two entities mentioned in source text by encoding the text and entities. However, these models cannot output the relation triggers, but only gives the result of relation classification. Although models can generate weights for every single word through the improvement of attention mechanism, the weights will be affected by irrelevant words essentially, which are not required by the relation extraction task. In order to output re-lation triggers accurately, we propose a novel training frame-work for Relation Extraction (RE) that reduces the negative effect of irrelevant words on them in the encoding stage. In specific, we leverage Evolutive Mask based Point Network (EMPN) as a decoder to generate relation triggers and encode these words again. For an ordered output in relation triggers, we utilize order loss to constrain the output order in them. Ex-tensive experiment results demonstrate that the effectiveness of our proposed model achieves state-of-the-art performance on three RE benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Predict relation type and generate trigger words to make the results reasonable with Evolutive Mask based Point Network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=x8l2miKNqPb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IPwwNwMvHFW" data-number="1384">
        <h4>
          <a href="https://openreview.net/forum?id=IPwwNwMvHFW">
              Multi-Agent Decentralized Belief Propagation on Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IPwwNwMvHFW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yitao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yitao_Chen1">Yitao Chen</a>, <a href="https://openreview.net/profile?id=~Deepanshu_Vasal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepanshu_Vasal1">Deepanshu Vasal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IPwwNwMvHFW-details-727" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IPwwNwMvHFW-details-727"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">I-pomdps, Belief propagation, Multi-agent control</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the problem of interactive partially observable Markov decision processes (I-POMDPs),where the agents are located at the nodes of a communication network.  Specifically, we assume a certain message type for all messages.  Moreover, each agent makes individual decisions based on the interactive belief states, the information observed locally and the messages received from its neighbors over the network.Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors.  We propose a decentralized belief propagation algorithm for the problem,  and prove the convergence of our algorithm.Finally we show multiple applications of our framework. Our work appears to be the first study of decentralized belief propagation algorithm for networked multi-agent I-POMDPs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a methodology to do multi agent belief propagation on grahps</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PU3VGS93gxD" data-number="1202">
        <h4>
          <a href="https://openreview.net/forum?id=PU3VGS93gxD">
              Sample Complexity of Deep Active Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PU3VGS93gxD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhao_Song6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Song6">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Baocheng_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Baocheng_Sun1">Baocheng Sun</a>, <a href="https://openreview.net/profile?id=~Danyang_Zhuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danyang_Zhuo1">Danyang Zhuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PU3VGS93gxD-details-296" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PU3VGS93gxD-details-296"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning algorithms require large numbers of labeled training data to deliver state-of-the-art results. However, in many domains of AI, there are abundant unlabeled data but it is costly to get data labeled by experts, such as medical diagnosis and fraud detection. In these domains, active learning, where an algorithm maximizes model accuracy while requiring the least number of labeled data, is appealing.
        Active learning uses both labeled and unlabeled data to train models, and the learning algorithm decides which subset of data should acquire labels.
        Due to the costly label acquisition, it is interesting to know whether it is possible from a theoretical perspective to understand how many labeled data are actually needed to train a machine learning model. This question is known as the sample complexity problem, and it has been extensively explored for training linear machine learning models (e.g., linear regression). Today, deep learning has become the de facto method for machine learning, but the sample complexity problem for deep active learning remains unsolved. This problem is challenging due to the non-linear nature of neural networks.
        In this paper, we present the first deep active learning algorithm which has a provable sample complexity. Using this algorithm, we have derived the first upper bound on the number of required labeled data for training neural networks. 
        Our upper bound shows that the minimum number of labeled data a neural net needs does not depend on the data distribution or the width of the neural network but is determined by the smoothness of non-linear activation and the dimension of the input data.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove the first upper bound on the sample complexity of active learning for training one-hidden layer neural networks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PU3VGS93gxD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xO4xryFQltO" data-number="943">
        <h4>
          <a href="https://openreview.net/forum?id=xO4xryFQltO">
              A new perspective on probabilistic image modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xO4xryFQltO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Gepperth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Gepperth1">Alexander Gepperth</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xO4xryFQltO-details-612" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xO4xryFQltO-details-612"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep mixture models, sum-product networks, probabilistic circuits, image modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new probabilistic approach for image modeling capable of density estimation, sampling and tractable inference. DCGMM instances exhibit a CNN-like layered structure, in which the principal building  blocks are convolutional Gaussian Mixture (cGMM) layers. A key innovation w.r.t. related models lile sum-produdct networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer optimizes an independent loss function and therefore has an independent probabilistic interpretation. This modular approach permits intervening transformation layers to harness the full spectrum of 
        (potentially non-invertible) mappings available to CNNs, e.g., max-pooling or (half-)convolutions. DCGMM sampling and inference are realized by a deep chain of hierarchical priors, where samples generated by each cGMM layer parameterize sampling in the next-lower cGMM layer. For sampling through non-invertible transformation layers, we introduce a new gradient-based sharpening technique that exploits redundancy (overlap) in, e.g., half-convolutions. The basic quantities forward-transported through a DCGMM instance are the posterior probabilities of cGMM layers, which ensures numerical stability and facilitates the selection of learning rates.
        DCGMMs can be trained end-to-end by SGD from random initial conditions, much like CNNs. We experimentally show that DCGMMs compare favorably to several recent PC and SPN models in terms of inference, classification and sampling, the latter particularly for challenging datasets such as SVHN. A public TF2 implementation is provided as well.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A conceptually new approach for probabilistic image modeling based on multiple linked GMMs, which can generate samples of excellent quality w.r.t. related approaches, particularly for SVHN.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UajXTGRjuKB" data-number="840">
        <h4>
          <a href="https://openreview.net/forum?id=UajXTGRjuKB">
              Sampling Before Training: Rethinking the Effect of Edges in the Process of Training Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UajXTGRjuKB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hengyuan_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hengyuan_Ma1">Hengyuan Ma</a>, <a href="https://openreview.net/profile?email=xianmu.yq%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="xianmu.yq@antgroup.com">Qi Yang</a>, <a href="https://openreview.net/profile?email=wenxi.sbw%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="wenxi.sbw@antgroup.com">Bowen Sun</a>, <a href="https://openreview.net/profile?email=shunlong.wxd%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shunlong.wxd@antgroup.com">Long Shun</a>, <a href="https://openreview.net/profile?email=kui.lijk%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="kui.lijk@antgroup.com">Junkui Li</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Feng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianfeng_Feng2">Jianfeng Feng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UajXTGRjuKB-details-968" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UajXTGRjuKB-details-968"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNN) demonstrate excellent performance on many graph-based tasks; however, they also impose a heavy computational burden when trained on a large-scale graph. Although various sampling methods have been proposed to speed up training GNN by shrinking the scale of the graph during training, they become unavailable if we need to perform sampling before training. In this paper, we quantify the importance of every edge for training in the graph with the extra information they convey in addition to the node features, as inspired by a manifold learning algorithm called diffusion map. Based on this calculation, we propose Graph Diffusion Sampling (GDS), a simple but effective sampling method for shrinking the size of the edge set before training. GDS prefers to sample edges with high importance, and edges dropped by GDS will never be used in the training procedure. We empirically show that GDS preserves the edges crucial for training in a variety of models (GCN, GraphSAGE, GAT, and JKNet). Compared to training on the full graph, GDS can guarantee the performance of the model while only samples a small fraction of the edges.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UajXTGRjuKB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xREjEGUoY4c" data-number="761">
        <h4>
          <a href="https://openreview.net/forum?id=xREjEGUoY4c">
              Robot Intent Recognition Method Based on State Grid Business Office
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xREjEGUoY4c" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lanfang_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lanfang_Dong1">Lanfang Dong</a>, <a href="https://openreview.net/profile?id=~Zhao_Pu_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Pu_Hu1">Zhao Pu Hu</a>, <a href="https://openreview.net/profile?id=~Hanchao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanchao_Liu1">Hanchao Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xREjEGUoY4c-details-390" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xREjEGUoY4c-details-390"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial intelligence is currently in an era of change, not only changing the artificial intelligence technology itself, but also changing human society. It has become more and more common to use artificial intelligence as the core human-computer interaction technology to replace manpower. Intention recognition is an important part of the human-machine dialogue system, and deep learning technology is gradually being applied to the task of intent recognition. However, intent recognition based on deep learning often has problems such as low recognition accuracy and slow recognition speed. In response to these problems, this paper designs a BERT fine-tuning to improve the network structure based on the pre-training model and proposes new continuous pre-training goals. To improve the accuracy of intent recognition, a method based on multi-teacher model compression is proposed to compress the pre-training model, which reduces the time consumption of model inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gTdmGt48ht1" data-number="691">
        <h4>
          <a href="https://openreview.net/forum?id=gTdmGt48ht1">
              On the Double Descent of Random Features Models Trained with SGD
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gTdmGt48ht1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fanghui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fanghui_Liu1">Fanghui Liu</a>, <a href="https://openreview.net/profile?id=~Johan_Suykens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johan_Suykens1">Johan Suykens</a>, <a href="https://openreview.net/profile?id=~Volkan_Cevher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Volkan_Cevher1">Volkan Cevher</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gTdmGt48ht1-details-814" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gTdmGt48ht1-details-814"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random features, over-parameterized model, double descent, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study generalization properties of random features (RF) regression in high dimensions  optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds of RF regression under both constant and adaptive step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well in the interpolation setting, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm interpolator, as a theoretical justification of using SGD in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that, random features models trained with SGD in high dimensions still generalizes well for interpolation learning, recovers double descent, and incurs no loss in the excess risk when compared to the exact closed-form solution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fG9WttDhAaa" data-number="4723">
        <h4>
          <a href="https://openreview.net/forum?id=fG9WttDhAaa">
              Rethinking Positional Encoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fG9WttDhAaa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jianqiao_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianqiao_Zheng1">Jianqiao Zheng</a>, <a href="https://openreview.net/profile?id=~Sameera_Ramasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sameera_Ramasinghe1">Sameera Ramasinghe</a>, <a href="https://openreview.net/profile?email=simon.lucey%40adelaide.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="simon.lucey@adelaide.edu.au">Simon Lucey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fG9WttDhAaa-details-998" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fG9WttDhAaa-details-998"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions.  Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=fG9WttDhAaa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LZVXOnSrD0Y" data-number="4720">
        <h4>
          <a href="https://openreview.net/forum?id=LZVXOnSrD0Y">
              Pareto Frontier Approximation Network (PA-Net) Applied to Multi-objective TSP
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LZVXOnSrD0Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ishaan_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ishaan_Mehta1">Ishaan Mehta</a>, <a href="https://openreview.net/profile?email=s.saeedi%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="s.saeedi@ryerson.ca">Sajad Saeedi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LZVXOnSrD0Y-details-801" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LZVXOnSrD0Y-details-801"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotics, planning, TSP, RL, Multi Objective Optimization, Pareto Optimality</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-objective optimization is used in various areas of robotics like control, planning etc. Their solutions are dependent on multiple objective functions, which can be conflicting in nature. In such cases, the optimality is defined in terms of Pareto optimality. A set of these Pareto Optimal solutions in the objective space form a Pareto front (or frontier). Each solution has its own trade off. For instance, the travelling salesman problem (TSP) is used in robotics for task/resource allocation. Often this allocation is influenced by multiple objective functions and is solved using Multi-objective travelling salesman problem (MOTSP). In this work, we present PA-Net, a network that generates good approximations of the Pareto front for the multi-objective optimization problems. Our training framework is applicable to other multi-objective optimization problems; however, in this work, we focus on solving MOTSP.  Firstly, MOTSP is converted into a constrained optimization problem. We then train our network to solve this constrained problem using the Lagrangian relaxation and policy gradient. With PA-Net we are able to generate better quality Pareto fronts with fast inference times as compared to other learning based and classical methods. Finally, we present the application of PA-Net to find optimal visiting order in coverage planning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">PA-Net: a network that approximates Pareto Frontier for Multi Objective TSP problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LZVXOnSrD0Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aJORhCrlYqu" data-number="4718">
        <h4>
          <a href="https://openreview.net/forum?id=aJORhCrlYqu">
              ARMCMC:  Online Bayesian Density Estimation of Model Parameters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aJORhCrlYqu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedram_Agand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedram_Agand1">Pedram Agand</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?id=~Hamid_Taghirad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hamid_Taghirad1">Hamid Taghirad</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aJORhCrlYqu-details-765" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aJORhCrlYqu-details-765"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian, Probabilistic approaches, MCMC, Hunt Crossley, parameter identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although the Bayesian paradigm provides a rigorous framework to estimate the full probability distribution over unknown parameters,  its online  implementation can be challenging due to heavy computational costs. This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches. These shortcomings include: being solely able to account for Gaussian noise, being applicable to systems with linear in the parameters (LIP) constraint, or having requirements on persistence excitation (PE). In ARMCMC, we propose a variable jump distribution, which depends on a temporal forgetting factor.  This allows one to adjust the trade-off between exploitation and exploration, depending on whether there is an abrupt change to the parameter being estimated. We prove that ARMCMC requires fewer samples to achieve the same precision and reliability compared to conventional MCMC approaches.  We demonstrate our approach on two challenging benchmarks:  the estimation of parameters in a soft bending actuator and the Hunt-Crossley dynamic model. Our method shows at-least 70\% improvement in parameter point estimation accuracy and approximately 55\% reduction in tracking error of the value of interest compared to recursive least squares and conventional MCMC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=aJORhCrlYqu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Xd6T7cT7vwj" data-number="4673">
        <h4>
          <a href="https://openreview.net/forum?id=Xd6T7cT7vwj">
              Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Xd6T7cT7vwj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marcus_L%C3%A5ng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marcus_LÃ¥ng1">Marcus LÃ¥ng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Xd6T7cT7vwj-details-845" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xd6T7cT7vwj-details-845"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent studies have show that wide neural networks with orthogonal linear layers and Gaussian PoincarÃ© normalized activation functions avoid vanishing and exploding gradients for input vectors with the correct magnitude. This paper introduces a strengthening of the condition that the activation function must be Gaussian PoincarÃ© normalized which creates robustness to deviations from standard normal distribution in the pre-activations, thereby reducing the dependence on the requirement that the network is wide and that the input vector has the correct magnitude. In implicit representation learning this allows the training of deep networks of this type where the linear layers are no longer constrained to be orthogonal linear transformations. Networks of this type can be fitted to a reference image to 1/10th the mean square error achievable with previous methods. Herein is also given an improved positional encoding for implicit representation learning of two-dimensional images and a small-batch training procedure for fitting of neural networks to images which allows fitting in fewer epochs, leading to substantial improvement in training time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Similar to SIREN, but able to fit images to higher accuracy (PSNR=67 instead PSNR=50 for a specific reference image).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sHUFhv03qX_" data-number="4671">
        <h4>
          <a href="https://openreview.net/forum?id=sHUFhv03qX_">
              Q-Learning Scheduler for Multi-Task Learning through the use of Histogram of Task Uncertainty
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sHUFhv03qX_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kourosh_Meshgi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Meshgi2">Kourosh Meshgi</a>, <a href="https://openreview.net/profile?id=~Maryam_Sadat_Mirzaei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maryam_Sadat_Mirzaei1">Maryam Sadat Mirzaei</a>, <a href="https://openreview.net/profile?id=~Satoshi_Sekine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satoshi_Sekine1">Satoshi Sekine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sHUFhv03qX_-details-879" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sHUFhv03qX_-details-879"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Q-learning, Multi-Task Learning, MTL Scheduling, Histogram of Task Uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Simultaneous training of a multi-task learning network on different domains or tasks is not always straightforward. It could lead to inferior performance or generalization compared to the corresponding single-task networks. To maximally taking advantage of the benefits of multi-task learning, an effective training scheduling method is deemed necessary. Traditional schedulers follow a heuristic or prefixed strategy, ignoring the relation of the tasks, their sample complexities, and the state of the emergent shared features. We proposed a deep Q-Learning Scheduler (QLS) that monitors the state of the tasks and the shared features using a novel histogram of task uncertainty, and through trial-and-error, learns an optimal policy for task scheduling. Extensive experiments on multi-domain and multi-task settings with various task difficulty profiles have been conducted, the proposed method is benchmarked against other schedulers, its superior performance has been demonstrated, and results are discussed.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A deep Q-learning-based task scheduling method to improve multi-tasking learning based on a novel histogram of task uncertainty.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lTiW8Jet8t" data-number="4664">
        <h4>
          <a href="https://openreview.net/forum?id=lTiW8Jet8t">
              Efficient Ensembles of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lTiW8Jet8t" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Amrit_Nagarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amrit_Nagarajan1">Amrit Nagarajan</a>, <a href="https://openreview.net/profile?id=~Jacob_R._Stevens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_R._Stevens1">Jacob R. Stevens</a>, <a href="https://openreview.net/profile?id=~Anand_Raghunathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anand_Raghunathan1">Anand Raghunathan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lTiW8Jet8t-details-379" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lTiW8Jet8t-details-379"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) have enabled the power of deep learning to be applied to inputs beyond the Euclidean domain, with applications ranging from social networks and product recommendation engines to the life sciences. GNNs, like other classes of machine learning models, benefit from ensemble learning, wherein multiple models are combined to provide higher accuracy and robustness than single models. However, ensembles suffer from significantly higher inference processing and storage requirements, limiting their use in practical applications. In this work, we leverage the unique characteristics of GNNs to overcome these overheads, creating efficient ensemble GNNs that are faster than even single models at inference time. We observe that during message passing, nodes that are incorrectly classified (error nodes) also end up adversely affecting the representations of other nodes in their neighborhood. This error propagation also makes GNNs more difficult to approximate (e.g., through pruning) for efficient inference. We propose a technique to create ensembles of diverse models, and further propose Error Node Isolation (ENI), which prevents error nodes from sending messages to (and thereby influencing) other nodes. In addition to improving accuracy, ENI also leads to a significant reduction in the memory footprint and the number of arithmetic operations required to evaluate the computational graphs of all neighbors of error nodes. Remarkably, these savings outweigh even the overheads of using multiple models in the ensemble. A second key benefit of ENI is that it  enhances the resilience of GNNs to approximations. Consequently, we propose Edge Pruning and Network Pruning techniques that target both the input graph and the neural networks used to process the graph. Our experiments on GNNs for transductive and inductive node classification demonstrate that ensembles with ENI are simultaneously more accurate (by up to 4.6% and 3.8%) and  faster (by up to 2.8<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="113" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> and 5.7<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="114" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container>) when compared to the best-performing single models and ensembles without ENI, respectively. In addition, GNN ensembles with ENI are consistently more accurate than single models and ensembles without ENI when subject to pruning, leading to additional speedups of up to 5<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="115" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> with no loss in accuracy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ndffz5uo6H" data-number="4656">
        <h4>
          <a href="https://openreview.net/forum?id=Ndffz5uo6H">
              Updater-Extractor Architecture for Inductive World State Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ndffz5uo6H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arsenii_Kirillovich_Moskvichev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arsenii_Kirillovich_Moskvichev1">Arsenii Kirillovich Moskvichev</a>, <a href="https://openreview.net/profile?id=~James_A_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_A_Liu1">James A Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ndffz5uo6H-details-162" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ndffz5uo6H-details-162"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformers, long-term-memory, sequential processing, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Developing sequential models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure. Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Proposing a theoretically and practically justified way to introduce persistent world state representations into transformer architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ndffz5uo6H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ys-bh0Eer_" data-number="4654">
        <h4>
          <a href="https://openreview.net/forum?id=ys-bh0Eer_">
              Block Contextual MDPs for Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ys-bh0Eer_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Franziska_Meier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Franziska_Meier2">Franziska Meier</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joelle_Pineau1">Joelle Pineau</a>, <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amy_Zhang1">Amy Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ys-bh0Eer_-details-460" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ys-bh0Eer_-details-460"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, MDP, Block Contextual MDP, Continual Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In reinforcement learning (RL), when defining a Markov Decision Process (MDP), the environment dynamics is implicitly assumed to be stationary. This assumption of stationarity, while simplifying, can be unrealistic in many scenarios. In the continual reinforcement learning scenario, the sequence of tasks is another source of nonstationarity. In this work, we propose to examine this continual reinforcement learning setting through the block contextual MDP (BC-MDP) framework, which enables us to relax the assumption of stationarity. This framework challenges RL algorithms to handle both nonstationarity and rich observation settings and, by additionally leveraging smoothness properties, enables us to study generalization bounds for this setting. Finally, we take inspiration from adaptive control to propose a novel algorithm that addresses the challenges introduced by this more realistic BC-MDP setting, allows for zero-shot adaptation at evaluation time, and achieves strong performance on several nonstationary environments.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce the Lipschitz Block Contextual MDP framework for the continual RL setting and propose a representation learning algorithm that enables RL agents to generalize to non-stationary environments.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FeaitX_a5Av" data-number="4645">
        <h4>
          <a href="https://openreview.net/forum?id=FeaitX_a5Av">
              GSD: Generalized Stochastic Decoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FeaitX_a5Av" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ning_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ning_Gong1">Ning Gong</a>, <a href="https://openreview.net/profile?id=~Nianmin_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nianmin_Yao1">Nianmin Yao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FeaitX_a5Av-details-271" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FeaitX_a5Av-details-271"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Processing, Decoding Algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although substantial progress has been made in various text generation tasks, there remains a vast gap between current generations and human languages. One reason is that virtually all decoding methods currently developed are pragmatic to address the text degeneration problem, which exists in both deterministic and stochastic decoding algorithms. So, why text generated from these algorithms are divergent? What is the critical difference between these algorithms? Moreover, is it possible to design a generalized framework where existing decoding algorithms can be naturally connected, uniformly described, and mutually inspired?
        In this paper, we try to explore answers to these intriguing questions. Correctly, we propose a generalized decoding framework that can be used to describe and connect existing popular decoding algorithms. Based on the framework, we propose a novel implementation with a distinctive core from existing decoding algorithms. As far as we know, this is the first work trying to propose a generalized framework to bridge these decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms. Experiments show that text produced by our method is closest to the characteristics of human languages. Source code and the generated text can be accessed from https://github.com/ginoailab/gsd.git.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel work proposing a generalized framework to connect existing decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JgmY4TUgznC" data-number="4641">
        <h4>
          <a href="https://openreview.net/forum?id=JgmY4TUgznC">
              Few-Shot Multi-task Learning via Implicit regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JgmY4TUgznC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsung_Huh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongsung_Huh1">Dongsung Huh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JgmY4TUgznC-details-273" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JgmY4TUgznC-details-273"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Few Shot Learning, Learning Instability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern machine learning is highly data-intensive. Few-shot learning (FSL) aims to resolve this sample efficiency problem by learning from multiple tasks and quickly adapt to new tasks containing only a few samples. However,  FSL problems proves to be significantly more challenging and require more compute expensive process to optimize. In this work, we consider multi-task linear regression (MTLR) as a canonical problem for few-shot learning, and investigate the source of challenge of FSL. We find that the MTLR exhibits local minimum problems that are not present in single-task problem, and thus making the learning much more challenging. We also show that the problem can be resolved by  overparameterizing the  model by increasing both the width and depth of the linear network and initializing the weights with small values, exploiting the implicit regularization bias of gradient descent-based learning.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UzOEYQM-xTg" data-number="4635">
        <h4>
          <a href="https://openreview.net/forum?id=UzOEYQM-xTg">
              Robust Long-Tailed Learning under Label Noise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UzOEYQM-xTg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tong_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tong_Wei1">Tong Wei</a>, <a href="https://openreview.net/profile?id=~Jiang-Xin_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiang-Xin_Shi1">Jiang-Xin Shi</a>, <a href="https://openreview.net/profile?id=~Wei-Wei_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei-Wei_Tu1">Wei-Wei Tu</a>, <a href="https://openreview.net/profile?id=~Yu-Feng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Feng_Li1">Yu-Feng Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UzOEYQM-xTg-details-789" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UzOEYQM-xTg-details-789"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weakly-supervised learning, long-tailed learning, learning with noisy labels, semi-supervised learning, multi-label learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework,~\algo, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on both benchmark and real-world datasets demonstrate substantial improvement over many existing methods. For example, \algo\ outperforms baselines by more than 5\% in test accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UzOEYQM-xTg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a3NaSCJ20V" data-number="4634">
        <h4>
          <a href="https://openreview.net/forum?id=a3NaSCJ20V">
              Equivariant Grasp learning In Real Time
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a3NaSCJ20V" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xupeng_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xupeng_Zhu1">Xupeng Zhu</a>, <a href="https://openreview.net/profile?id=~Dian_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dian_Wang1">Dian Wang</a>, <a href="https://openreview.net/profile?id=~Ondrej_Biza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ondrej_Biza1">Ondrej Biza</a>, <a href="https://openreview.net/profile?id=~Robert_Platt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_Platt1">Robert Platt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a3NaSCJ20V-details-279" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a3NaSCJ20V-details-279"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotic Grasping, Equivariance, Reinforcement Leanring</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual grasp detection is a key problem in robotics where the agent must learn to model the grasp function, a mapping from an image of a scene onto a set of feasible grasp poses. In this paper, we recognize that the grasp function is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="116" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">SE</mi></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>-equivariant and that it can be modeled using an equivariant convolutional neural network. As a result, we are able to significantly improve the sample efficiency of grasp learning to the point where we can learn a good approximation of the grasp function within only 500 grasp experiences. This is fast enough that we can learn to grasp completely on a physical robot in about an hour. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="I13PP8-cdvz" data-number="4632">
        <h4>
          <a href="https://openreview.net/forum?id=I13PP8-cdvz">
              SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=I13PP8-cdvz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheng_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheng_Cheng1">Sheng Cheng</a>, <a href="https://openreview.net/profile?id=~Yi_Ren3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Ren3">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yezhou_Yang1">Yezhou Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#I13PP8-cdvz-details-308" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I13PP8-cdvz-details-308"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stroke-based representation, Spatial robustness, Robust feature learning, Novel pattern generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing end-to-end visual recognition models do not possess innate spatial invariance and are thus vulnerable to out-of-training attacks. This suggests the need of a better representation design. This paper follows existing cognitive studies to investigate a sketch representation that specify stroke information on vertices and inter-stroke information on edges. The resultant representation, combined with a graph neural network, achieves both high classification accuracy and high robustness against translation, rotation, and stroke-wise parametric and topological attacks thanks to the use of spatially invariant stroke features and GNN architecture. While prior studies demonstrated similar sketch representations for classification and generation, these attempts heavily relied on run-time statistical inference rather than more efficient bottom-up computation via GNN. The presented sketch representation poses good structured expression capability as it enables generation of sketches semantically different from the training dataset.  Lastly, we show SSR-GNNs are able to accomplish all tasks (classification, robust feature learning, and novel pattern generation), which shows that the representation is task-agnostic. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper presents a Stroke-based Sketch Representation with Graph Neural Networks which is spatially robust, with structured expression capability and is task-agnostic.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RSd79AULOu" data-number="4625">
        <h4>
          <a href="https://openreview.net/forum?id=RSd79AULOu">
              Fairness-aware Federated Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RSd79AULOu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhuozhuo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuozhuo_Tu1">Zhuozhuo Tu</a>, <a href="https://openreview.net/profile?id=~zhiqiang_xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~zhiqiang_xu1">zhiqiang xu</a>, <a href="https://openreview.net/profile?id=~Tairan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tairan_Huang1">Tairan Huang</a>, <a href="https://openreview.net/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dacheng_Tao1">Dacheng Tao</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RSd79AULOu-details-938" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RSd79AULOu-details-938"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Learning Theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized. In such a setting, naively minimizing an aggregate loss may introduce bias and disadvantage model performance on certain clients. To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set. By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable. We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees. We further prove generalization bounds for learning with this objective. Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new framework to address the fairness issues in federated learning and provide theoretical guarantees.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="edqz84cQ79T" data-number="4622">
        <h4>
          <a href="https://openreview.net/forum?id=edqz84cQ79T">
              Shaping latent representations using Self-Organizing Maps with Relevance Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=edqz84cQ79T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedro_Braga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedro_Braga1">Pedro Braga</a>, <a href="https://openreview.net/profile?email=hrm%40cin.ufpe.br" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="hrm@cin.ufpe.br">Heitor Medeiros</a>, <a href="https://openreview.net/profile?id=~Hansenclever_Bassani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hansenclever_Bassani1">Hansenclever Bassani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#edqz84cQ79T-details-450" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="edqz84cQ79T-details-450"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Clustering, Learning Prototypes, Topological Representations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work indicates that Deep Clustering (DC) methods are a viable option for unsupervised representations learning of visual features. By combining representation learning and clustering, traditional approaches have been shown to build latent representations that capture essential features of the data while preserving topological characteristics. In this sense, models based on Self-Organizing Maps models with relevance learning (SOMRL) were considered as they perform well in clustering besides being able to create a map that learns the relevance of each input dimension for each cluster, preserving the original relations and topology of the data. We hypothesize that this type of model can produce a more intuitive and disentangled representation in the latent space by promoting smoother transitions between cluster points over time. This work proposes a representation learning framework that combines a new gradient-based SOMRL model and autoencoders. The SOMRL learns the relevance weights for each input dimension of each cluster. It creates a tendency to separate the information into subspaces. To achieve this, we designed a new loss function term that weighs these learned relevances and provides an estimated unsupervised error to be used in combination with a reconstruction loss. The model is evaluated in terms of clustering performance and quality of the learned representations and then compared with start-of-the-art models, showing competitive results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work proposes a representation learning framework that combines a new Self-Organizing Maps with autoencoders to shape their latent spaces into cluster prototypes living in separate subspaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=edqz84cQ79T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eAEcdRkcMHh" data-number="4611">
        <h4>
          <a href="https://openreview.net/forum?id=eAEcdRkcMHh">
              HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N:M Sparsity and Integer Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eAEcdRkcMHh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minjia_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minjia_Zhang1">Minjia Zhang</a>, <a href="https://openreview.net/profile?id=~Connor_Holmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Connor_Holmes1">Connor Holmes</a>, <a href="https://openreview.net/profile?id=~Yuxiong_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuxiong_He1">Yuxiong He</a>, <a href="https://openreview.net/profile?id=~Bo_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Wu1">Bo Wu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eAEcdRkcMHh-details-165" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eAEcdRkcMHh-details-165"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Efficient Inference, N:M Sparsification, Quantization, Transformer networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks. However, the huge size of these models brings significant challenges to fine-tuning and online deployment due to latency and cost constraints. Recently, hardware manufacturers have released new architectures that support efficient N:M sparsity and low-precision integer computation for fast inferencing. In contrast to unstructured sparsity, N:M sparsity specifies that out of each chunk of N contiguous weight parameters, exactly M parameters are non-zero. Moreover, these architectures also support processing data with reduced precision, such as INT8. Prior work often considers inducing N:M sparsity and integer quantization in isolation or as independent pieces of a compression pipeline. However, there lacks a systematic investigation towards how N:M sparsity and integer quantization can be effectively combined to exploit the maximum degree of redundancy and enable even faster acceleration for pre-trained Transformer networks.
        
        In this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM). We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and
        solved in a unified manner. When evaluated across the GLUE suite of NLP benchmarks, our approach outperforms baselines that consider each of these problems independently, retaining 99.4\% accuracy of the dense baseline while being able to execute on newly released hardware effectively. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">HoloFormer is a unified and systematic approach to learn N:M sparsity and integer quantization for compressing pre-trained Transformer networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ad_F_z27pCx" data-number="4595">
        <h4>
          <a href="https://openreview.net/forum?id=ad_F_z27pCx">
              A Discussion On the Validity of Manifold Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ad_F_z27pCx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dai_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dai_Shi1">Dai Shi</a>, <a href="https://openreview.net/profile?id=~Andi_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andi_Han1">Andi Han</a>, <a href="https://openreview.net/profile?id=~Yi_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Guo3">Yi Guo</a>, <a href="https://openreview.net/profile?id=~Junbin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junbin_Gao1">Junbin Gao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 30 Sept 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ad_F_z27pCx-details-950" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ad_F_z27pCx-details-950"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Manifold learning, Dimensionality Reduction, Computational Geometry, Simplicial Complex</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition (orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any -simplex decomposition result on a -manifold.  However,  constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SK1nec-Ehd" data-number="4584">
        <h4>
          <a href="https://openreview.net/forum?id=SK1nec-Ehd">
              PulseImpute: A Novel Benchmark Task and Architecture for Imputation of Physiological Signals
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SK1nec-Ehd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Maxwell_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maxwell_Xu1">Maxwell Xu</a>, <a href="https://openreview.net/profile?id=~Alexander_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Moreno1">Alexander Moreno</a>, <a href="https://openreview.net/profile?id=~James_Matthew_Rehg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Matthew_Rehg1">James Matthew Rehg</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SK1nec-Ehd-details-701" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SK1nec-Ehd-details-701"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">missingness, imputation, mHealth, sensors, transformer, self-attention</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Providing care for patients with chronic diseases is one of the biggest drivers of the nationâ€™s rising healthcare costs, but many of these diseases are linked to mutable health behaviors. Mobile health (mHealth) biophysical sensors that continuously measure our current conditions provide the framework for a personalized guidance system for the maintenance of healthy behaviors. However, this physiological sensor data is plagued with missingness due to insecure attachments, wireless dropout, battery, and adherence issues. These issues cripple their rich diagnostic utility as well as their ability to enable temporally-precise interventions. While there is a sizable amount of research focusing on imputation methods, surprisingly, no works have addressed the patterns of missingness, quasi-periodic signal structure, and the between subject heterogeneity that characterizes physiological signals in mHealth applications. We present the PulseImpute Challenge, the first challenge dataset for physiological signal imputation which includes a large set of baselines' performances on realistic missingness models and data. Next, we demonstrate the potential to address this quasi-periodic structure and heterogeneity with our Dilated Convolution Bottleneck (DCB) Transformer, a transformer architecture with a self-attention mechanism that is able to attend to corresponding waveform features in quasi-periodic signals. By utilizing stacked dilated convolutions with bottleneck layers for query and key transformations, we visually demonstrate that the kernel similarity in the attention model gives high similarity to similar temporal features across quasi-periodic periods. We hope the release of our challenge task definitions and baseline implementations will spur the community to address this challenging and important problem. 
         </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present PulseImpute, a benchmarking challenge for the imputation of biophysical signals, and propose a novel self-attention module for attending over quasi-periodic signals.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dhLChxJwgMR" data-number="4578">
        <h4>
          <a href="https://openreview.net/forum?id=dhLChxJwgMR">
              HFSP: A Hardware-friendly Soft Pruning Framework for Vision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dhLChxJwgMR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhenglun_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhenglun_Kong1">Zhenglun Kong</a>, <a href="https://openreview.net/profile?id=~Peiyan_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peiyan_Dong1">Peiyan Dong</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaolong_Ma2">Xiaolong Ma</a>, <a href="https://openreview.net/profile?id=~Xin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Meng1">Xin Meng</a>, <a href="https://openreview.net/profile?id=~Mengshu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengshu_Sun1">Mengshu Sun</a>, <a href="https://openreview.net/profile?id=~Wei_Niu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Niu3">Wei Niu</a>, <a href="https://openreview.net/profile?id=~Bin_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Ren1">Bin Ren</a>, <a href="https://openreview.net/profile?id=~Minghai_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghai_Qin1">Minghai Qin</a>, <a href="https://openreview.net/profile?id=~Hao_Tang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Tang6">Hao Tang</a>, <a href="https://openreview.net/profile?id=~Yanzhi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanzhi_Wang3">Yanzhi Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dhLChxJwgMR-details-654" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dhLChxJwgMR-details-654"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Hardware-friendly, Soft Token Pruning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViT, and the related edge device deployment, we leverage the input token sparsity and propose a hardware-friendly soft pruning framework (HFSP), which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique to package the pruned tokens, which integrate the less informative tokens generated by the selector module into a package token, and participates in subsequent calculations rather than being discarded completely.  From a hardware standpoint, our framework is bound to the tradeoff between accuracy and specific hardware constraints through our proposed hardware-oriented progressive training, and all the operators embedded in the framework have been well-supported. Experimental results demonstrate that the proposed framework significantly reduces the computational costs of ViTs while maintaining comparable performance on image classification. For example, our method reduces the FLOPs of DeiT-S by over 42.6% while only sacrificing 0.46% top-1 accuracy. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. Code will be publicly released.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A vision transformer pruning framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dhLChxJwgMR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="znpOLJUYGcA" data-number="4568">
        <h4>
          <a href="https://openreview.net/forum?id=znpOLJUYGcA">
              Automatic Integration for Neural Temporal Point Process
          </a>
        
          
            <a href="https://openreview.net/pdf?id=znpOLJUYGcA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zihao_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zihao_Zhou1">Zihao Zhou</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rose_Yu1">Rose Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#znpOLJUYGcA-details-7" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="znpOLJUYGcA-details-7"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point process</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integration lies at the heart of the temporal point process. Due to the intrinsic mathematical difficulty of symbolic integration, neural temporal point process models either constrain the intensity function to an integrable functional form or apply certain numerical methods. However, the former type of model has limited expressive power, and the latter type of model suffers additional numerical errors and high computational costs. In this paper, we introduce automatic integration with neural point process models, a new paradigm for efficient, closed-form nonparametric inference of temporal point process characterized by any intensity function. We test the model against a variety of synthetic temporal point process datasets and show that the model can better capture inter-event intensity changes than state-of-the-art methods. We also identify certain model settings that would lead the MLE estimator for the temporal point process to be inconsistent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Leveraging automatic integration for more efficient and accurate recovery of temporal point process's intensity</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZV7MoEj44Et" data-number="4556">
        <h4>
          <a href="https://openreview.net/forum?id=ZV7MoEj44Et">
              Measuring the Effectiveness of Self-Supervised Learning using Calibrated Learning Curves
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZV7MoEj44Et" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrei_Atanov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrei_Atanov1">Andrei Atanov</a>, <a href="https://openreview.net/profile?id=~Shijian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shijian_Xu1">Shijian Xu</a>, <a href="https://openreview.net/profile?email=onur.beker%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="onur.beker@epfl.ch">Onur Beker</a>, <a href="https://openreview.net/profile?id=~Andrey_Filatov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrey_Filatov1">Andrey Filatov</a>, <a href="https://openreview.net/profile?id=~Amir_Zamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Zamir1">Amir Zamir</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZV7MoEj44Et-details-174" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZV7MoEj44Et-details-174"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Self-Supervised Learning, Transfer Learning, Metric, Evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning has witnessed remarkable progress in recent years, in particular with the introduction of augmentation-based contrastive methods. While a number of large-scale empirical studies on the performance of self-supervised pre-training have been conducted, there isn't yet an agreed upon set of control baselines, evaluation practices, and metrics to report. We identify this as an important angle of investigation and propose an evaluation standard that aims to quantify and communicate transfer learning performance in an informative yet accessible setup. This is done by baking in a number of key control baselines in the evaluation method, particularly the blind guess (quantifying the dataset bias), the scratch model (quantifying the architectural contribution), and the gold standard (quantifying the upper-bound). We further provide a number of experiments to demonstrate how the proposed evaluation can be employed in empirical studies of basic questions -- for example, whether the effectiveness of existing self-supervised learning methods is skewed towards image classification versus other tasks, such as dense pixel-wise predictions. 
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an evaluation standard for measuring the effectiveness of self-supervised learning based on incorporating important control baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxTR9Z2DW8R" data-number="4552">
        <h4>
          <a href="https://openreview.net/forum?id=UxTR9Z2DW8R">
              Reinforcement Learning State Estimation for High-Dimensional Nonlinear Systems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxTR9Z2DW8R" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Saviz_Mowlavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saviz_Mowlavi1">Saviz Mowlavi</a>, <a href="https://openreview.net/profile?id=~Mouhacine_Benosman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mouhacine_Benosman1">Mouhacine Benosman</a>, <a href="https://openreview.net/profile?id=~Saleh_Nabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saleh_Nabi1">Saleh Nabi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxTR9Z2DW8R-details-566" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxTR9Z2DW8R-details-566"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, partial differential equation, reduced order modeling, closure models, state prediction, state estimation, dynamic mode decomposition.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RL-ROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="V2WidtMGSRG" data-number="4516">
        <h4>
          <a href="https://openreview.net/forum?id=V2WidtMGSRG">
              Provable Identifiability of ReLU Neural Networks via Lasso Regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=V2WidtMGSRG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gen_Li2">Gen Li</a>, <a href="https://openreview.net/profile?id=~Ganghua_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ganghua_Wang1">Ganghua Wang</a>, <a href="https://openreview.net/profile?id=~Yuantao_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuantao_Gu1">Yuantao Gu</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jie_Ding2">Jie Ding</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#V2WidtMGSRG-details-155" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V2WidtMGSRG-details-155"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Lasso, nonlinear regression, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">LASSO regularization is a popular regression tool to enhance the prediction accuracy of statistical models by performing variable selection through the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="117" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> penalty, initially formulated for the linear model and its variants. In this paper, the territory of LASSO is extended to the neural network model, a fashionable and powerful nonlinear regression model. Specifically, given a neural network whose output <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="118" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container> depends only on a small subset of input <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="119" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-b mjx-i"><mjx-c class="mjx-c1D499 TEX-BI"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math></mjx-assistive-mml></mjx-container>, denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="120" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>â‹†</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, we prove that the LASSO estimator can stably reconstruct the neural network and identify <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="121" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>â‹†</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> when the number of samples scales logarithmically with the input dimension. This challenging regime has been well understood for linear models while barely studied for neural networks. Our theory lies in an extended Restricted Isometry Property (RIP)-based analysis framework for two-layer ReLU neural networks, which may be of independent interest to other LASSO or neural network settings. Based on the result, we further propose a neural network-based variable selection method. Experiments on simulated and real-world datasets show the promising performance of our variable selection approach compared with classical techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We theoretically show that the Lasso estimator can stably identify ReLU neural networks and then propose to use neural networks as vehicles to perform variable selection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=V2WidtMGSRG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Jh9VxCkrEZn" data-number="4515">
        <h4>
          <a href="https://openreview.net/forum?id=Jh9VxCkrEZn">
              Spatiotemporal Representation Learning on Time Series with Dynamic Graph ODEs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Jh9VxCkrEZn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ming_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming_Jin3">Ming Jin</a>, <a href="https://openreview.net/profile?id=~Yuan-Fang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan-Fang_Li1">Yuan-Fang Li</a>, <a href="https://openreview.net/profile?id=~Yu_Zheng5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zheng5">Yu Zheng</a>, <a href="https://openreview.net/profile?id=~Bin_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Yang4">Bin Yang</a>, <a href="https://openreview.net/profile?id=~Shirui_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shirui_Pan1">Shirui Pan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Jh9VxCkrEZn-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jh9VxCkrEZn-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Spatiotemporal representation learning on multivariate time series has received tremendous attention in forecasting traffic and energy data. Recent works either rely on complicated discrete neural architectures or graph priors, hindering their effectiveness and applications in the real world. In this paper, inspired by neural ordinary differential equations and graph structure learning, we propose a fully continuous model named Dynamic Graph ODE (DyG-ODE) to capture both long-range spatial and temporal dependencies to learn expressive representations on arbitrary multivariate time series data without being restricted by rigid preconditions (e.g., graph priors). For modeling the continuous dynamics of spatiotemporal clues, we design a simple yet powerful dynamic graph ODE by coupling the proposed spatial and temporal ODEs, which not only allows the model to obtain infinite spatial and temporal receptive fields but also reduces numerical errors and model complexity significantly. Our empirical evaluations demonstrate the superior effectiveness and efficiency of DyG-ODE on a number of benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a fully continuous model named DyG-ODE to learn expressive spatiotemporal representations on arbitrary multivariate time series data</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="recent-activity">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="https://openreview.net/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review, with legal nonprofit status through<!-- --> <a href="https://codeforscience.org/" target="_blank" rel="noopener noreferrer">Code for Science &amp; Society</a>. We gratefully acknowledge the support of the<!-- --> <a href="https://openreview.net/sponsors" target="_blank">OpenReview Sponsors</a>.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible. To submit a bug report or feature request, you can use the official OpenReview GitHub repository:<br><a href="https://github.com/openreview/openreview/issues/new/choose" target="_blank" rel="noreferrer">Report an issue</a></p><form><div class="form-group"><input type="email" id="feedback-from" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" id="feedback-subject" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea id="feedback-message" name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2022/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2022/Conference\n$(function() {\nvar args = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar group = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar document = null;\nvar window = null;\n\n// TODO: remove these vars when all old webfields have been archived\nvar model = {\n  tokenPayload: function() {\n    return { user: user }\n  }\n};\nvar controller = {\n  get: Webfield.get,\n  addHandler: function(name, funcMap) {\n    Object.values(funcMap).forEach(function(func) {\n      func();\n    });\n  },\n};\n\n$('#group-container').empty();\n// START GROUP CODE\n// ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar CONFERENCE_ID = 'ICLR.cc/2022/Conference';\nvar PARENT_GROUP_ID = 'ICLR.cc/2022';\nvar SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';\nvar REVIEWERS_NAME = 'Reviewers';\nvar AREA_CHAIRS_NAME = 'Area_Chairs';\nvar AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';\nvar REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';\nvar PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';\nvar AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';\nvar HEADER = {\"title\": \"The Tenth International Conference on Learning Representations \", \"subtitle\": \"ICLR 2022\", \"location\": \"Virtual\", \"date\": \"Apr 25 2022\", \"website\": \"https://iclr.cc/Conferences/2022\", \"instructions\": \"\", \"deadline\": \"Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0\", \"contact\": \"iclr2022pc@gmail.com\"};\nvar PUBLIC = true;\n\nvar WILDCARD_INVITATION = CONFERENCE_ID + '/.*';\nvar BUFFER = 0;  // deprecated\nvar PAGE_SIZE = 50;\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true,\n  showTags: false\n};\nvar commentDisplayOptions = {\n  pdfLink: false,\n  replyCount: true,\n  showContents: false,\n  showParent: true\n};\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderSubmissionButton();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\n// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/\nfunction load() {\n\n  var spotlightNotesP = $.Deferred().resolve([]);\n  var oralNotesP = $.Deferred().resolve([]);\n  var posterNotesP = $.Deferred().resolve([]);\n  \n  \n  var activityNotesP = $.Deferred().resolve([]);\n  var authorNotesP = $.Deferred().resolve([]);\n  var userGroupsP = $.Deferred().resolve([]);\n  var withdrawnNotesP = $.Deferred().resolve([]);\n  var deskRejectedNotesP = $.Deferred().resolve([]);\n  var submittedNotesP = $.Deferred().resolve([]);\n\n  if (PUBLIC) {\n      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      //   pageSize: PAGE_SIZE,\n      //   details: 'replyCount',\n      //   includeCount: true\n      // });\n\n    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Spotlight',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Oral',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Poster',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Submitted',\n      details: 'replyCount',\n      includeCount: true\n    });\n\n    if (WITHDRAWN_SUBMISSION_ID) {\n      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount',\n        includeCount: true\n      });\n    }\n\n    if (DESK_REJECTED_SUBMISSION_ID) {\n      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount,invitation,original',\n        includeCount: true\n      });\n    }\n  }\n\n  if (user \u0026\u0026 !_.startsWith(user.id, 'guest_')) {\n    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {\n      pageSize: PAGE_SIZE,\n      details: 'forumContent,invitation,writable'\n    });\n\n    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });\n\n    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.authorids': user.profile.id\n    });\n  }\n\n  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);\n}\n\n// Render functions\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction renderSubmissionButton() {\n  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})\n    .then(function(invitation) {\n      Webfield.ui.submissionButton(invitation, user, {\n        onNoteCreated: function() {\n          // Callback funtion to be run when a paper has successfully been submitted (required)\n          if (PUBLIC) {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'A list of all submissions will be available after the deadline.');\n          } else {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'The author console page for managing your submissions will be available soon.');\n          }\n\n          load().then(renderContent).then(function() {\n            $('.tabs-container a[href=\"#your-consoles\"]').click();\n          });\n        }\n      });\n    });\n}\n\nfunction renderConferenceTabs() {\n  var sections = [\n    {\n      heading: 'Your Consoles',\n      id: 'your-consoles',\n    }\n  ];\n\n  if (PUBLIC) {\n    // sections.push({\n    //   heading: 'All Submissions',\n    //   id: 'all-submissions',\n    // });\n    sections.push({\n      heading: 'Oral Presentations',\n      id: 'oral-submissions',\n    });\n    sections.push({\n      heading: 'Spotlight Presentations',\n      id: 'spotlight-submissions',\n    });\n    sections.push({\n      heading: 'Poster Presentations',\n      id: 'poster-submissions',\n    });\n    sections.push({\n      heading: 'Rejected Submissions',\n      id: 'submitted-submissions',\n    });\n    // if (WITHDRAWN_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Withdrawn Submissions',\n    //     id: 'withdrawn-submissions',\n    //   })\n    // }\n    // if (DESK_REJECTED_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Desk Rejected Submissions',\n    //     id: 'desk-rejected-submissions',\n    //   })\n    // }\n    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {\n      sections.push({\n        heading: 'Desk Rejected/Withdrawn Submissions',\n        id: 'desk-rejected-withdrawn-submissions',\n      })\n    }\n  }\n\n  sections.push({\n    heading: 'Recent Activity',\n    id: 'recent-activity',\n  }\n)\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortBy(_.uniq(allGroups));\n  var links = [];\n  uniqueGroups.forEach(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n    links.push(\n      [\n        '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n        '\u003c/li\u003e'\n      ].join('')\n    );\n  });\n\n  $('#your-consoles .submissions-list').append(links);\n}\n\nfunction renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {\n\n  // Your Consoles tab\n  if (userGroups.length || authorNotes.length) {\n\n    var $container = $('#your-consoles').empty();\n    $container.append('\u003cul class=\"list-unstyled submissions-list\"\u003e');\n\n    var allConsoles = [];\n    if (authorNotes.length) {\n      allConsoles.push(AUTHORS_ID);\n    }\n    userGroups.forEach(function(group) {\n      allConsoles.push(group.id);\n    });\n\n    // Render all console links for the user\n    createConsoleLinks(allConsoles);\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n\n  // Oral Papers tab\n  var oralNotesCount = oralNotes.count || 0;\n  oralNotes = oralNotes.notes || [];\n\n  $('#oral-submissions').empty();\n\n  if (oralNotesCount) {\n    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#oral-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(oralNotes, {\n      heading: null,\n      container: '#oral-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Oral',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);\n          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: oralNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Oral',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#oral-submissions\"]').parent().hide();\n  }\n\n  // Spotlight Papers tab\n  var spotlightNotesCount = spotlightNotes.count || 0;\n  spotlightNotes = spotlightNotes.notes || [];\n\n  $('#spotlight-submissions').empty();\n\n  if (spotlightNotesCount) {\n    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#spotlight-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(spotlightNotes, {\n      heading: null,\n      container: '#spotlight-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Spotlight',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);\n          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: spotlightNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Spotlight',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#spotlight-submissions\"]').parent().hide();\n  }\n\n  // Poster Papers tab\n  var posterNotesCount = posterNotes.count || 0;\n  posterNotes = posterNotes.notes || [];\n\n  $('#poster-submissions').empty();\n\n  if (posterNotesCount) {\n    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#poster-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(posterNotes, {\n      heading: null,\n      container: '#poster-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Poster',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);\n          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: posterNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Poster',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#poster-submissions\"]').parent().hide();\n  }\n\n  // Rejected Papers tab\n  var submittedNotesCount = submittedNotes.count || 0;\n  submittedNotes = submittedNotes.notes || [];\n\n  $('#submitted-submissions').empty();\n\n  if (submittedNotesCount) {\n    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#submitted-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(submittedNotes, {\n      heading: null,\n      container: '#submitted-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Submitted',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);\n          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: submittedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Submitted',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#submitted-submissions\"]').parent().hide();\n  }\n\n  // Activity Tab\n  if (activityNotes.length) {\n    var displayOptions = {\n      container: '#recent-activity',\n      user: user \u0026\u0026 user.profile,\n      showActionButtons: true\n    };\n\n    $(displayOptions.container).empty();\n\n    Webfield.ui.activityList(activityNotes, displayOptions);\n\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().hide();\n  }\n  \n  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))\n  \n  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);\n  if (removedNotesCount) {\n    $('#desk-rejected-withdrawn-submissions').empty();\n\n    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);\n    Webfield.ui.submissionList(removedNotesArray, {\n      heading: null,\n      container: '#desk-rejected-withdrawn-submissions',\n      search: {\n        enabled: false\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: removedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        if (offset \u003c deskRejectedNotes.count) {\n          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset\n          });\n        } else {\n          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset - roundedDeskRejectedNotes\n          });\n        }\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#desk-rejected-withdrawn-submissions\"]').parent().hide();\n  }\n\n  // var withdrawnNotesCount = withdrawnNotes.count || 0;\n  // if (withdrawnNotesCount) {\n  //   $('#withdrawn-submissions').empty();\n\n  //   var withdrawnNotesArray = withdrawnNotes.notes || [];\n  //   Webfield.ui.submissionList(withdrawnNotesArray, {\n  //     heading: null,\n  //     container: '#withdrawn-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: withdrawnNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#withdrawn-submissions\"]').parent().hide();\n  // }\n\n  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;\n  // if (deskRejectedNotesCount) {\n  //   $('#desk-rejected-submissions').empty();\n\n  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];\n  //   Webfield.ui.submissionList(deskRejectedNotesArray, {\n  //     heading: null,\n  //     container: '#desk-rejected-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: deskRejectedNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#desk-rejected-submissions\"]').parent().hide();\n  // }\n\n  $('#notes .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n// END GROUP CODE\n});\n//# sourceURL=webfieldCode.js","writable":false,"query":{"id":"ICLR.cc/2022/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2022/Conference"},"buildId":"v1.5.0-1-g2b07894","isFallback":false,"gip":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;">ICLR 2022 Conference | OpenReview</p></next-route-announcer><script>// Webfield Code for ICLR.cc/2022/Conference
$(function() {
var args = {"id":"ICLR.cc/2022/Conference"};
var group = {"id":"ICLR.cc/2022/Conference"};
var document = null;
var window = null;

// TODO: remove these vars when all old webfields have been archived
var model = {
  tokenPayload: function() {
    return { user: user }
  }
};
var controller = {
  get: Webfield.get,
  addHandler: function(name, funcMap) {
    Object.values(funcMap).forEach(function(func) {
      func();
    });
  },
};

$('#group-container').empty();
// START GROUP CODE
// ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var CONFERENCE_ID = 'ICLR.cc/2022/Conference';
var PARENT_GROUP_ID = 'ICLR.cc/2022';
var SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';
var REVIEWERS_NAME = 'Reviewers';
var AREA_CHAIRS_NAME = 'Area_Chairs';
var AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';
var REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';
var PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';
var AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';
var HEADER = {"title": "The Tenth International Conference on Learning Representations ", "subtitle": "ICLR 2022", "location": "Virtual", "date": "Apr 25 2022", "website": "https://iclr.cc/Conferences/2022", "instructions": "", "deadline": "Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0", "contact": "iclr2022pc@gmail.com"};
var PUBLIC = true;

var WILDCARD_INVITATION = CONFERENCE_ID + '/.*';
var BUFFER = 0;  // deprecated
var PAGE_SIZE = 50;

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true,
  showTags: false
};
var commentDisplayOptions = {
  pdfLink: false,
  replyCount: true,
  showContents: false,
  showParent: true
};

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }

  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderSubmissionButton();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/
function load() {

  var spotlightNotesP = $.Deferred().resolve([]);
  var oralNotesP = $.Deferred().resolve([]);
  var posterNotesP = $.Deferred().resolve([]);
  
  
  var activityNotesP = $.Deferred().resolve([]);
  var authorNotesP = $.Deferred().resolve([]);
  var userGroupsP = $.Deferred().resolve([]);
  var withdrawnNotesP = $.Deferred().resolve([]);
  var deskRejectedNotesP = $.Deferred().resolve([]);
  var submittedNotesP = $.Deferred().resolve([]);

  if (PUBLIC) {
      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      //   pageSize: PAGE_SIZE,
      //   details: 'replyCount',
      //   includeCount: true
      // });

    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Spotlight',
      details: 'replyCount',
      includeCount: true
    });
  
    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Oral',
      details: 'replyCount',
      includeCount: true
    });
  
    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Poster',
      details: 'replyCount',
      includeCount: true
    });
  
    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Submitted',
      details: 'replyCount',
      includeCount: true
    });

    if (WITHDRAWN_SUBMISSION_ID) {
      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount',
        includeCount: true
      });
    }

    if (DESK_REJECTED_SUBMISSION_ID) {
      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount,invitation,original',
        includeCount: true
      });
    }
  }

  if (user && !_.startsWith(user.id, 'guest_')) {
    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {
      pageSize: PAGE_SIZE,
      details: 'forumContent,invitation,writable'
    });

    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });

    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.authorids': user.profile.id
    });
  }

  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);
}

// Render functions
function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);

  Webfield.ui.spinner('#notes', { inline: true });
}

function renderSubmissionButton() {
  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})
    .then(function(invitation) {
      Webfield.ui.submissionButton(invitation, user, {
        onNoteCreated: function() {
          // Callback funtion to be run when a paper has successfully been submitted (required)
          if (PUBLIC) {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'A list of all submissions will be available after the deadline.');
          } else {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'The author console page for managing your submissions will be available soon.');
          }

          load().then(renderContent).then(function() {
            $('.tabs-container a[href="#your-consoles"]').click();
          });
        }
      });
    });
}

function renderConferenceTabs() {
  var sections = [
    {
      heading: 'Your Consoles',
      id: 'your-consoles',
    }
  ];

  if (PUBLIC) {
    // sections.push({
    //   heading: 'All Submissions',
    //   id: 'all-submissions',
    // });
    sections.push({
      heading: 'Oral Presentations',
      id: 'oral-submissions',
    });
    sections.push({
      heading: 'Spotlight Presentations',
      id: 'spotlight-submissions',
    });
    sections.push({
      heading: 'Poster Presentations',
      id: 'poster-submissions',
    });
    sections.push({
      heading: 'Rejected Submissions',
      id: 'submitted-submissions',
    });
    // if (WITHDRAWN_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Withdrawn Submissions',
    //     id: 'withdrawn-submissions',
    //   })
    // }
    // if (DESK_REJECTED_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Desk Rejected Submissions',
    //     id: 'desk-rejected-submissions',
    //   })
    // }
    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {
      sections.push({
        heading: 'Desk Rejected/Withdrawn Submissions',
        id: 'desk-rejected-withdrawn-submissions',
      })
    }
  }

  sections.push({
    heading: 'Recent Activity',
    id: 'recent-activity',
  }
)

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortBy(_.uniq(allGroups));
  var links = [];
  uniqueGroups.forEach(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }
    links.push(
      [
        '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
        '</li>'
      ].join('')
    );
  });

  $('#your-consoles .submissions-list').append(links);
}

function renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {

  // Your Consoles tab
  if (userGroups.length || authorNotes.length) {

    var $container = $('#your-consoles').empty();
    $container.append('<ul class="list-unstyled submissions-list">');

    var allConsoles = [];
    if (authorNotes.length) {
      allConsoles.push(AUTHORS_ID);
    }
    userGroups.forEach(function(group) {
      allConsoles.push(group.id);
    });

    // Render all console links for the user
    createConsoleLinks(allConsoles);

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }


  // Oral Papers tab
  var oralNotesCount = oralNotes.count || 0;
  oralNotes = oralNotes.notes || [];

  $('#oral-submissions').empty();

  if (oralNotesCount) {
    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#oral-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(oralNotes, {
      heading: null,
      container: '#oral-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Oral',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);
          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: oralNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Oral',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#oral-submissions"]').parent().hide();
  }

  // Spotlight Papers tab
  var spotlightNotesCount = spotlightNotes.count || 0;
  spotlightNotes = spotlightNotes.notes || [];

  $('#spotlight-submissions').empty();

  if (spotlightNotesCount) {
    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#spotlight-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(spotlightNotes, {
      heading: null,
      container: '#spotlight-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Spotlight',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);
          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: spotlightNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Spotlight',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#spotlight-submissions"]').parent().hide();
  }

  // Poster Papers tab
  var posterNotesCount = posterNotes.count || 0;
  posterNotes = posterNotes.notes || [];

  $('#poster-submissions').empty();

  if (posterNotesCount) {
    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#poster-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(posterNotes, {
      heading: null,
      container: '#poster-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Poster',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);
          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: posterNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Poster',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#poster-submissions"]').parent().hide();
  }

  // Rejected Papers tab
  var submittedNotesCount = submittedNotes.count || 0;
  submittedNotes = submittedNotes.notes || [];

  $('#submitted-submissions').empty();

  if (submittedNotesCount) {
    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#submitted-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(submittedNotes, {
      heading: null,
      container: '#submitted-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Submitted',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);
          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: submittedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Submitted',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#submitted-submissions"]').parent().hide();
  }

  // Activity Tab
  if (activityNotes.length) {
    var displayOptions = {
      container: '#recent-activity',
      user: user && user.profile,
      showActionButtons: true
    };

    $(displayOptions.container).empty();

    Webfield.ui.activityList(activityNotes, displayOptions);

    $('.tabs-container a[href="#recent-activity"]').parent().show();
  } else {
    $('.tabs-container a[href="#recent-activity"]').parent().hide();
  }
  
  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))
  
  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);
  if (removedNotesCount) {
    $('#desk-rejected-withdrawn-submissions').empty();

    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);
    Webfield.ui.submissionList(removedNotesArray, {
      heading: null,
      container: '#desk-rejected-withdrawn-submissions',
      search: {
        enabled: false
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: removedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        if (offset < deskRejectedNotes.count) {
          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset
          });
        } else {
          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset - roundedDeskRejectedNotes
          });
        }
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#desk-rejected-withdrawn-submissions"]').parent().hide();
  }

  // var withdrawnNotesCount = withdrawnNotes.count || 0;
  // if (withdrawnNotesCount) {
  //   $('#withdrawn-submissions').empty();

  //   var withdrawnNotesArray = withdrawnNotes.notes || [];
  //   Webfield.ui.submissionList(withdrawnNotesArray, {
  //     heading: null,
  //     container: '#withdrawn-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: withdrawnNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#withdrawn-submissions"]').parent().hide();
  // }

  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;
  // if (deskRejectedNotesCount) {
  //   $('#desk-rejected-submissions').empty();

  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];
  //   Webfield.ui.submissionList(deskRejectedNotesArray, {
  //     heading: null,
  //     container: '#desk-rejected-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: deskRejectedNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#desk-rejected-submissions"]').parent().hide();
  // }

  $('#notes .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();
// END GROUP CODE
});
//# sourceURL=webfieldCode.js</script><script src="./ICLR2022_poster_files/login-6d19e702f9bd1dcc.js.download"></script><script src="./ICLR2022_poster_files/privacy-276f217f8a2a3840.js.download"></script><script src="./ICLR2022_poster_files/terms-e2f16e0dce69665f.js.download"></script><script src="./ICLR2022_poster_files/sponsors-987fb223230996d5.js.download"></script><script src="./ICLR2022_poster_files/contact-2c775b351d0a5421.js.download"></script><script src="./ICLR2022_poster_files/venues-458c67c21955226a.js.download"></script><script src="./ICLR2022_poster_files/about-77a61c48f23a1315.js.download"></script><script src="./ICLR2022_poster_files/index-55b7d6149a41ddec.js.download"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>