<!DOCTYPE html>
<!-- saved from url=(0074)https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://openreview.net/favicon.ico"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><script async="" src="./ICLR2022_poster_6_files/js"></script><script>window.dataLayer = window.dataLayer || [];
function gtag() { dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-108703919-1', {
  page_path: window.location.pathname + window.location.search,
  transport_type: 'beacon'
});</script><title>ICLR 2022 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta property="og:title" content="ICLR 2022 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta name="next-head-count" content="14"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preload" href="./ICLR2022_poster_6_files/ed513f7ce02040ba.css" as="style"><link rel="stylesheet" href="./ICLR2022_poster_6_files/ed513f7ce02040ba.css" data-n-g=""><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./ICLR2022_poster_6_files/polyfills-5cd94c89d3acac5f.js.download"></script><script src="./ICLR2022_poster_6_files/webpack-363f7b452897525e.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/framework-79bce4a3a540b080.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/main-aaf30a81beffaaa5.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/_app-dd140e4fb7437f25.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/6253-86ab2843b0544962.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/group-18de1916b352abe5.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/_buildManifest.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/_ssgManifest.js.download" defer=""></script><script src="./ICLR2022_poster_6_files/_middlewareManifest.js.download" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap">@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4DRG.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZtyH.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNb4Q.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFlYA.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARPQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARGQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARDQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4AROQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARBQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARNQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARMQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARCQ_mu72Bi.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyOzW1IPriezag.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyHzW1IPriezag.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyCzW1IPriezag.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyPzW1IPriezag.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyAzW1IPriezag.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyMzW1IPriezag.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyNzW1IPriezag.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyDzW1IPrie.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr4TRASf6M7VBj.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6TRASf6M7VBj.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6zRASf6M7VBj.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5TRASf6M7Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVadyBx2pqPIif.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVYNyBx2pqPIif.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZdyBx2pqPIif.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVaNyBx2pqPIif.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZ9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVa9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVatyBx2pqPIif.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZNyBx2pqPA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><script src="./ICLR2022_poster_6_files/tex-chtml-full.js.download" async="" crossorigin="anonymous"></script><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/index-55b7d6149a41ddec.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/about-77a61c48f23a1315.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/venues-458c67c21955226a.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/contact-2c775b351d0a5421.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/sponsors-987fb223230996d5.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/terms-e2f16e0dce69665f.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/privacy-276f217f8a2a3840.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_6_files/login-6d19e702f9bd1dcc.js.download"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-stretchy-v.mjx-c28 mjx-beg mjx-c::before {
  content: "\239B";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c28 mjx-ext mjx-c::before {
  content: "\239C";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c28 mjx-end mjx-c::before {
  content: "\239D";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c28 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c28 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-stretchy-v.mjx-c29 mjx-beg mjx-c::before {
  content: "\239E";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c29 mjx-ext mjx-c::before {
  content: "\239F";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c29 mjx-end mjx-c::before {
  content: "\23A0";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c29 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c29 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c1D44C.TEX-I::before {
  padding: 0.683em 0.763em 0 0;
  content: "Y";
}

mjx-c.mjx-c1D44B.TEX-I::before {
  padding: 0.683em 0.852em 0 0;
  content: "X";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c2218::before {
  padding: 0.444em 0.5em 0 0;
  content: "\2218";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D435.TEX-I::before {
  padding: 0.683em 0.759em 0 0;
  content: "B";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c1D70C.TEX-I::before {
  padding: 0.442em 0.517em 0.216em 0;
  content: "\3C1";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c5E::before {
  padding: 0.694em 0.5em 0 0;
  content: "^";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c2225::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D41B.TEX-B::before {
  padding: 0.694em 0.639em 0.006em 0;
  content: "b";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c1D714.TEX-I::before {
  padding: 0.443em 0.622em 0.011em 0;
  content: "\3C9";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c7E::before {
  padding: 0.318em 0.5em 0 0;
  content: "~";
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c1D45E.TEX-I::before {
  padding: 0.442em 0.46em 0.194em 0;
  content: "q";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c3F.TEX-MI::before {
  padding: 0.716em 0.551em 0 0;
  content: "?";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c398::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "\398";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c1D715::before {
  padding: 0.715em 0.566em 0.022em 0;
  content: "\2202";
}

mjx-c.mjx-c2F.TEX-S1::before {
  padding: 0.85em 0.578em 0.349em 0;
  content: "/";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-c2229::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2229";
}

mjx-c.mjx-c41::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c4E::before {
  padding: 0.683em 0.75em 0 0;
  content: "N";
}

mjx-c.mjx-c44::before {
  padding: 0.683em 0.764em 0 0;
  content: "D";
}

mjx-c.mjx-c49::before {
  padding: 0.683em 0.361em 0 0;
  content: "I";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c2DC.TEX-S1::before {
  padding: 0.722em 0.556em 0 0;
  content: "\2DC";
}

mjx-c.mjx-c28.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: "(";
}

mjx-c.mjx-c29.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: ")";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c53::before {
  padding: 0.705em 0.556em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c45::before {
  padding: 0.68em 0.681em 0 0;
  content: "E";
}

mjx-c.mjx-c1D499.TEX-BI::before {
  padding: 0.452em 0.659em 0.008em 0;
  content: "x";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c22C6::before {
  padding: 0.486em 0.5em 0 0;
  content: "\22C6";
}
</style></head><body data-new-gr-c-s-check-loaded="14.1068.0" data-gr-ext-installed=""><div id="__next" data-reactroot=""><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off" autocorrect="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login?redirect=%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%23poster-submissions&amp;noprompt=true">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2022"><img class="icon" src="./ICLR2022_poster_6_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2022</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>The Tenth International Conference on Learning Representations </h1>
<h3>ICLR 2022</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Virtual
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> Apr 25 2022
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/Conferences/2022" title="The Tenth International Conference on Learning Representations  Homepage" target="_blank">https://iclr.cc/Conferences/2022</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2022pc@gmail.com" target="_blank">iclr2022pc@gmail.com</a>
      </span>
  </h4>

<div class="description">
    <p class="no-margin">Please see the venue website for more information.</p>
  <p>Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0</p>
</div>
</div><div id="invitation" style="display: block;"></div><div id="notes">
<div class="tabs-container " style=""><div class="mobile-full-width">
  <ul class="nav nav-tabs" role="tablist">
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
          Your Consoles
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oral-submissions" aria-controls="oral-submissions" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true" aria-expanded="false">
          Oral Presentations
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#spotlight-submissions" aria-controls="spotlight-submissions" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true" aria-expanded="false">
          Spotlight Presentations
        </a>
      </li>
      <li role="presentation" class="active">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions" aria-controls="poster-submissions" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true" aria-expanded="true">
          Poster Presentations
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#submitted-submissions" aria-controls="submitted-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
          Rejected Submissions
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#desk-rejected-withdrawn-submissions" aria-controls="desk-rejected-withdrawn-submissions" role="tab" data-toggle="tab" data-tab-index="5" data-modify-history="true">
          Desk Rejected/Withdrawn Submissions
        </a>
      </li>
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#recent-activity" aria-controls="recent-activity" role="tab" data-toggle="tab" data-tab-index="6" data-modify-history="true">
          Recent Activity
        </a>
      </li>
  </ul>
</div>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade" id="oral-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="NMEceG4v69Y" data-number="224">
        <h4>
          <a href="https://openreview.net/forum?id=NMEceG4v69Y">
              CycleMLP: A MLP-like Architecture for Dense Prediction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMEceG4v69Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shoufa_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoufa_Chen1">Shoufa Chen</a>, <a href="https://openreview.net/profile?id=~Enze_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enze_Xie1">Enze Xie</a>, <a href="https://openreview.net/profile?id=~Chongjian_GE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjian_GE1">Chongjian GE</a>, <a href="https://openreview.net/profile?id=~Runjian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runjian_Chen1">Runjian Chen</a>, <a href="https://openreview.net/profile?id=~Ding_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ding_Liang1">Ding Liang</a>, <a href="https://openreview.net/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMEceG4v69Y-details-500" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMEceG4v69Y-details-500"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">MLP, Dense Prediction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope
        with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A versatile MLP-like architecture for both recognition and dense prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMEceG4v69Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0xiJLKH-ufZ" data-number="222">
        <h4>
          <a href="https://openreview.net/forum?id=0xiJLKH-ufZ">
              Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0xiJLKH-ufZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fan_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fan_Bao1">Fan Bao</a>, <a href="https://openreview.net/profile?id=~Chongxuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongxuan_Li1">Chongxuan Li</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>, <a href="https://openreview.net/profile?id=~Bo_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhang2">Bo Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0xiJLKH-ufZ-details-491" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0xiJLKH-ufZ-details-491"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">diffusion probabilistic models, generative models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an analytic framework of estimating the optimal reverse variance in DPMs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0xiJLKH-ufZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uSE03demja" data-number="208">
        <h4>
          <a href="https://openreview.net/forum?id=uSE03demja">
              RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uSE03demja" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pingchuan_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pingchuan_Ma3">Pingchuan Ma</a>, <a href="https://openreview.net/profile?id=~Tao_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Du1">Tao Du</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Wojciech_Matusik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Matusik2">Wojciech Matusik</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uSE03demja-details-17" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uSE03demja-details-17"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">differentiable rendering, differentiable simulation, system identification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel approach to address the problem of identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3wU2UX0voE" data-number="154">
        <h4>
          <a href="https://openreview.net/forum?id=3wU2UX0voE">
              The Information Geometry of Unsupervised Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3wU2UX0voE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3wU2UX0voE-details-575" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3wU2UX0voE-details-575"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised skill learning, reward-free RL, mutual information, DIAYN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that mutual information skill learning is optimal in one sense but not optimal in another sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=3wU2UX0voE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="pMQwKL1yctf" data-number="86">
        <h4>
          <a href="https://openreview.net/forum?id=pMQwKL1yctf">
              Language modeling via stochastic processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=pMQwKL1yctf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rose_E_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rose_E_Wang1">Rose E Wang</a>, <a href="https://openreview.net/profile?id=~Esin_Durmus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Esin_Durmus1">Esin Durmus</a>, <a href="https://openreview.net/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#pMQwKL1yctf-details-408" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pMQwKL1yctf-details-408"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">contrastive learning, language modelling, stochastic processes</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a language model that implicitly plans via a latent stochastic process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=pMQwKL1yctf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>

<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class=" active " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="3">
          <span>›</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="2">
          <span>»</span>
      </li>
  </ul>
</nav>

</div>
    <div role="tabpanel" class="tab-pane fade" id="spotlight-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="dFbKQaRk15w" data-number="693">
        <h4>
          <a href="https://openreview.net/forum?id=dFbKQaRk15w">
              Equivariant Subgraph Aggregation Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dFbKQaRk15w" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Beatrice_Bevilacqua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beatrice_Bevilacqua1">Beatrice Bevilacqua</a>, <a href="https://openreview.net/profile?id=~Fabrizio_Frasca1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabrizio_Frasca1">Fabrizio Frasca</a>, <a href="https://openreview.net/profile?id=~Derek_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Lim1">Derek Lim</a>, <a href="https://openreview.net/profile?id=~Balasubramaniam_Srinivasan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balasubramaniam_Srinivasan1">Balasubramaniam Srinivasan</a>, <a href="https://openreview.net/profile?id=~Chen_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Cai1">Chen Cai</a>, <a href="https://openreview.net/profile?id=~Gopinath_Balamurugan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gopinath_Balamurugan1">Gopinath Balamurugan</a>, <a href="https://openreview.net/profile?id=~Michael_M._Bronstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_M._Bronstein1">Michael M. Bronstein</a>, <a href="https://openreview.net/profile?id=~Haggai_Maron1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haggai_Maron1">Haggai Maron</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dFbKQaRk15w-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dFbKQaRk15w-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Expressive power, Equivariance, Weisfeiler-Leman</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a provably expressive graph learning framework based on representing graphs as multisets of subgraphs and processing them with an equivariant architecture.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="D78Go4hVcxO" data-number="676">
        <h4>
          <a href="https://openreview.net/forum?id=D78Go4hVcxO">
              How Do Vision Transformers Work?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=D78Go4hVcxO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Namuk_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Namuk_Park1">Namuk Park</a>, <a href="https://openreview.net/profile?id=~Songkuk_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songkuk_Kim1">Songkuk Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#D78Go4hVcxO-details-939" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D78Go4hVcxO-details-939"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">vision transformer, self-attention, multi-head self-attention, loss landscape</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that (1) multi-head self-attentions (MSAs) for computer vision flatten the loss landscapes, (2) MSAs are low-pass filters as opposed to Convs, and (3) MSAs at the end of a stage significantly improve the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=D78Go4hVcxO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kZ0UYdhqkNY" data-number="670">
        <h4>
          <a href="https://openreview.net/forum?id=kZ0UYdhqkNY">
              Variational methods for simulation-based inference
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kZ0UYdhqkNY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Manuel_Gl%C3%B6ckler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manuel_Glöckler1">Manuel Glöckler</a>, <a href="https://openreview.net/profile?id=~Michael_Deistler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Deistler1">Michael Deistler</a>, <a href="https://openreview.net/profile?id=~Jakob_H._Macke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_H._Macke1">Jakob H. Macke</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kZ0UYdhqkNY-details-943" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kZ0UYdhqkNY-details-943"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">likelihood-free inference, simulation-based inference, variational inference, neural density estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We combine likelihood-estimation with variational inference to achieve a scalable approach for simulation-based inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JprM0p-q0Co" data-number="590">
        <h4>
          <a href="https://openreview.net/forum?id=JprM0p-q0Co">
              Tackling the Generative Learning Trilemma with Denoising Diffusion GANs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JprM0p-q0Co" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhisheng_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhisheng_Xiao1">Zhisheng Xiao</a>, <a href="https://openreview.net/profile?id=~Karsten_Kreis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karsten_Kreis1">Karsten Kreis</a>, <a href="https://openreview.net/profile?id=~Arash_Vahdat3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arash_Vahdat3">Arash Vahdat</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JprM0p-q0Co-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JprM0p-q0Co-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">To reduce the number of sampling steps in diffusion models, we propose to model the denoising distribution with conditional GANs. We show our model tackles the generative learning trilemma &amp; achieves high sample quality, diversity &amp; fast sampling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="yKIAXjkJc2F" data-number="562">
        <h4>
          <a href="https://openreview.net/forum?id=yKIAXjkJc2F">
              Imbedding Deep Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=yKIAXjkJc2F" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrew_Corbett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Corbett1">Andrew Corbett</a>, <a href="https://openreview.net/profile?id=~Dmitry_Kangin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Kangin1">Dmitry Kangin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#yKIAXjkJc2F-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yKIAXjkJc2F-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural ODEs, Optimal Control, Deep Neural Networks, Invariant Imbedding</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.
        Our new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Invariant imbedding solution for (Bolza) optimal control problem derived and proved to yield new architectures of imbedded deep neural networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=yKIAXjkJc2F&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZkC8wKoLbQ7" data-number="560">
        <h4>
          <a href="https://openreview.net/forum?id=ZkC8wKoLbQ7">
              Understanding and Preventing Capacity Loss in Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZkC8wKoLbQ7" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Clare_Lyle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Clare_Lyle1">Clare Lyle</a>, <a href="https://openreview.net/profile?id=~Mark_Rowland1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Rowland1">Mark Rowland</a>, <a href="https://openreview.net/profile?id=~Will_Dabney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Will_Dabney1">Will Dabney</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZkC8wKoLbQ7-details-199" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZkC8wKoLbQ7-details-199"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.
        We identify a key mechanism by which this occurs in agents using neural networks as function approximators: \textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.
        We demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that RL agents experience representation collapse in sparse reward environments and propose an auxiliary task that prevents this from happening and outperforms the state of the art on the Atari benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZkC8wKoLbQ7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="1JDiK_TbV4S" data-number="550">
        <h4>
          <a href="https://openreview.net/forum?id=1JDiK_TbV4S">
              Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=1JDiK_TbV4S" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cian_Eastwood1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cian_Eastwood1">Cian Eastwood</a>, <a href="https://openreview.net/profile?id=~Ian_Mason1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Mason1">Ian Mason</a>, <a href="https://openreview.net/profile?id=~Chris_Williams1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Williams1">Chris Williams</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#1JDiK_TbV4S-details-316" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1JDiK_TbV4S-details-316"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transfer learning, dataset shift, unsupervised domain adaptation, source-free domain adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We identify a type of domain shift which can be resolved by restoring the *same* features and address it in the source-free setting by using softly-binned histograms to cheaply and flexibly align the marginal feature distributions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=1JDiK_TbV4S&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lnEaqbTJIRz" data-number="544">
        <h4>
          <a href="https://openreview.net/forum?id=lnEaqbTJIRz">
              The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lnEaqbTJIRz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yoav_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoav_Levine1">Yoav Levine</a>, <a href="https://openreview.net/profile?id=~Noam_Wies1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noam_Wies1">Noam Wies</a>, <a href="https://openreview.net/profile?id=~Daniel_Jannai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jannai1">Daniel Jannai</a>, <a href="https://openreview.net/profile?email=dan.nav%40mail.huji.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="dan.nav@mail.huji.ac.il">Dan Navon</a>, <a href="https://openreview.net/profile?id=~Yedid_Hoshen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yedid_Hoshen3">Yedid Hoshen</a>, <a href="https://openreview.net/profile?id=~Amnon_Shashua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amnon_Shashua1">Amnon Shashua</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lnEaqbTJIRz-details-890" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lnEaqbTJIRz-details-890"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Language Modeling, Pretraining, Self-attention, Transformers, Expressivity, Separation Rank, Sentence Embeddings</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.	This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove that pertained LMs model stronger dependencies between sentences that were shown in same training example, thus indicating benefits of better informed "pretraining example design"</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AUGBfDIV9rL" data-number="521">
        <h4>
          <a href="https://openreview.net/forum?id=AUGBfDIV9rL">
              Emergent Communication at Scale
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AUGBfDIV9rL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rahma_Chaabouni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rahma_Chaabouni1">Rahma Chaabouni</a>, <a href="https://openreview.net/profile?id=~Florian_Strub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Strub1">Florian Strub</a>, <a href="https://openreview.net/profile?id=~Florent_Altch%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florent_Altché1">Florent Altché</a>, <a href="https://openreview.net/profile?id=~Eugene_Tarassov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Tarassov1">Eugene Tarassov</a>, <a href="https://openreview.net/profile?id=~Corentin_Tallec2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Corentin_Tallec2">Corentin Tallec</a>, <a href="https://openreview.net/profile?id=~Elnaz_Davoodi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elnaz_Davoodi2">Elnaz Davoodi</a>, <a href="https://openreview.net/profile?id=~Kory_Wallace_Mathewson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kory_Wallace_Mathewson1">Kory Wallace Mathewson</a>, <a href="https://openreview.net/profile?id=~Olivier_Tieleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Tieleman1">Olivier Tieleman</a>, <a href="https://openreview.net/profile?id=~Angeliki_Lazaridou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angeliki_Lazaridou1">Angeliki Lazaridou</a>, <a href="https://openreview.net/profile?id=~Bilal_Piot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bilal_Piot1">Bilal Piot</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AUGBfDIV9rL-details-133" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AUGBfDIV9rL-details-133"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">emergent communication, multi-agent reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques.
        We also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work argues the importance of scaling up the emergent communication framework and investigates the impact of three scaling up aspects, namely the dataset, task complexity, and population size.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vds4SNooOe" data-number="496">
        <h4>
          <a href="https://openreview.net/forum?id=vds4SNooOe">
              Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vds4SNooOe" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jingchao_Ni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingchao_Ni1">Jingchao Ni</a>, <a href="https://openreview.net/profile?id=~Wei_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Cheng1">Wei Cheng</a>, <a href="https://openreview.net/profile?email=zchen%40nec-labs.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zchen@nec-labs.com">Zhengzhang Chen</a>, <a href="https://openreview.net/profile?email=takayoshi.asakura%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="takayoshi.asakura@nec.com">Takayoshi Asakura</a>, <a href="https://openreview.net/profile?email=tomoya-s%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomoya-s@nec.com">Tomoya Soma</a>, <a href="https://openreview.net/profile?email=kato%40renascience.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="kato@renascience.co.jp">Sho Kato</a>, <a href="https://openreview.net/profile?id=~Haifeng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haifeng_Chen1">Haifeng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vds4SNooOe-details-592" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vds4SNooOe-details-592"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep learning, represenation learning, generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on "coarse" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a "coarsely" pre-trained model to new tasks with a few "finer-grained" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a training framework characterized by a novel superclass conditional Gaussian mixture (SCGM) based generative model for learning fine-grained representations for cross-granularity adaptation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vds4SNooOe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-ApAkox5mp" data-number="427">
        <h4>
          <a href="https://openreview.net/forum?id=-ApAkox5mp">
              SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-ApAkox5mp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaccharie_Ramzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zaccharie_Ramzi1">Zaccharie Ramzi</a>, <a href="https://openreview.net/profile?id=~Florian_Mannel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Mannel1">Florian Mannel</a>, <a href="https://openreview.net/profile?id=~Shaojie_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaojie_Bai1">Shaojie Bai</a>, <a href="https://openreview.net/profile?id=~Jean-Luc_Starck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Luc_Starck1">Jean-Luc Starck</a>, <a href="https://openreview.net/profile?id=~Philippe_Ciuciu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Ciuciu1">Philippe Ciuciu</a>, <a href="https://openreview.net/profile?id=~Thomas_Moreau2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Moreau2">Thomas Moreau</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-ApAkox5mp-details-904" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-ApAkox5mp-details-904"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit models, bi-level optimization, quasi-newton methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models~(DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach in many settings, ranging from hyperparameter optimization to large Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the computational cost of the backward pass by up to two orders of magnitude. All this is achieved while retaining the excellent performance of the original models in hyperparameter optimization and on CIFAR, and giving encouraging and competitive results on ImageNet.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Use the approximate Jacobian matrix computed in quasi-Newton methods to perform the inversion needed in the training of implicit models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=-ApAkox5mp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ccWaPGl9Hq" data-number="411">
        <h4>
          <a href="https://openreview.net/forum?id=ccWaPGl9Hq">
              Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ccWaPGl9Hq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiawei_Huang3">Jiawei Huang</a>, <a href="https://openreview.net/profile?id=~Jinglin_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinglin_Chen2">Jinglin Chen</a>, <a href="https://openreview.net/profile?id=~Li_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Zhao1">Li Zhao</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?id=~Nan_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nan_Jiang2">Nan Jiang</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ccWaPGl9Hq-details-746" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ccWaPGl9Hq-details-746"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning theory, deployment efficiency, linear MDP</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an ''optimization with constraints'' perspective: we are interested in exploring an MDP and obtaining a near-optimal policy within minimal \emph{deployment complexity}, whereas in each deployment the policy can sample a large batch of data. Using finite-horizon linear MDPs as a concrete structural model, we reveal the fundamental limit in achieving deployment efficiency by establishing information-theoretic lower bounds, and provide algorithms that achieve the optimal deployment efficiency. Moreover, our formulation for DE-RL is flexible and can serve as a building block for other practically relevant settings; we give ''Safe DE-RL'' and ''Sample-Efficient DE-RL'' as two examples, which may be worth future investigation.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a formal theoretical formulation for depolyment-efficient reinforcement learning; establish lower bounds for deployment complexity and study near-optimal deployment-efficient algorithms in linear MDP setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ccWaPGl9Hq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="A3HHaEdqAJL" data-number="344">
        <h4>
          <a href="https://openreview.net/forum?id=A3HHaEdqAJL">
              Task Relatedness-Based Generalization Bounds for Meta Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=A3HHaEdqAJL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiechao_Guan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiechao_Guan2">Jiechao Guan</a>, <a href="https://openreview.net/profile?id=~Zhiwu_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwu_Lu1">Zhiwu Lu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 02 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#A3HHaEdqAJL-details-586" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="A3HHaEdqAJL-details-586"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Supposing the $n$ training tasks and the new task are sampled from the same environment, traditional meta learning theory derives an error bound on the expected loss over the new task in terms of the empirical training loss, uniformly over the set of all hypothesis spaces. However, there is still little research on how the relatedness of these tasks can affect the full utilization of all $mn$ training data (with $m$ examples per task). In this paper, we propose to address this problem by defining a new notion of task relatedness according to the existence of the bijective transformation between two tasks. A novel generalization bound of $\mathcal{O}(\frac{1}{\sqrt{mn}})$ for meta learning is thus derived by exploiting the proposed task relatedness. Moreover, when investigating a special branch of meta learning that involves representation learning with deep neural networks, we establish spectrally-normalized bounds for both classification and regression problems. Finally, we demonstrate that the relatedness requirement between two tasks is satisfied when the sample space possesses the completeness and separability properties, validating the rationality and applicability of our proposed task-relatedness measure.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="pFyXqxChZc" data-number="314">
        <h4>
          <a href="https://openreview.net/forum?id=pFyXqxChZc">
              IntSGD: Adaptive Floatless Compression of Stochastic Gradients
          </a>
        
          
            <a href="https://openreview.net/pdf?id=pFyXqxChZc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konstantin_Mishchenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantin_Mishchenko1">Konstantin Mishchenko</a>, <a href="https://openreview.net/profile?id=~Bokun_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bokun_Wang2">Bokun Wang</a>, <a href="https://openreview.net/profile?id=~Dmitry_Kovalev2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Kovalev2">Dmitry Kovalev</a>, <a href="https://openreview.net/profile?id=~Peter_Richt%C3%A1rik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Richtárik1">Peter Richtárik</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#pFyXqxChZc-details-634" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pFyXqxChZc-details-634"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">optimization, distributed optimization, compression, theory, parallel training, switchML</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the provably convergent and computationally cheap IntSGD algorithm for efficient distributed machine learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=pFyXqxChZc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="iLHOIDsPv1P" data-number="313">
        <h4>
          <a href="https://openreview.net/forum?id=iLHOIDsPv1P">
              PAC-Bayes Information Bottleneck
          </a>
        
          
            <a href="https://openreview.net/pdf?id=iLHOIDsPv1P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zifeng_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zifeng_Wang3">Zifeng Wang</a>, <a href="https://openreview.net/profile?id=~Shao-Lun_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shao-Lun_Huang3">Shao-Lun Huang</a>, <a href="https://openreview.net/profile?id=~Ercan_Engin_Kuruoglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ercan_Engin_Kuruoglu1">Ercan Engin Kuruoglu</a>, <a href="https://openreview.net/profile?id=~Jimeng_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimeng_Sun3">Jimeng Sun</a>, <a href="https://openreview.net/profile?id=~Xi_Chen21" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_Chen21">Xi Chen</a>, <a href="https://openreview.net/profile?id=~Yefeng_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yefeng_Zheng2">Yefeng Zheng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#iLHOIDsPv1P-details-885" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iLHOIDsPv1P-details-885"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">information bottleneck, representation learning, generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, over-parameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel PAC-Bayes bound guided information bottleneck for understanding and enhancing deep representation learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=iLHOIDsPv1P&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="jXKKDEi5vJt" data-number="249">
        <h4>
          <a href="https://openreview.net/forum?id=jXKKDEi5vJt">
              Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing
          </a>
        
          
            <a href="https://openreview.net/pdf?id=jXKKDEi5vJt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sai_Praneeth_Karimireddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sai_Praneeth_Karimireddy1">Sai Praneeth Karimireddy</a>, <a href="https://openreview.net/profile?id=~Lie_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lie_He1">Lie He</a>, <a href="https://openreview.net/profile?id=~Martin_Jaggi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Jaggi1">Martin Jaggi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">25 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#jXKKDEi5vJt-details-923" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jXKKDEi5vJt-details-923"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated learning, Distributed learning, Byzantine robust optimization, Heterogeneity (Non-IID)</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Byzantine-robust distributed learning with heterogeneous data distribution</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=jXKKDEi5vJt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AvcfxqRy4Y" data-number="227">
        <h4>
          <a href="https://openreview.net/forum?id=AvcfxqRy4Y">
              Understanding the Role of Self Attention for Efficient Speech Recognition
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AvcfxqRy4Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kyuhong_Shim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyuhong_Shim1">Kyuhong Shim</a>, <a href="https://openreview.net/profile?id=~Jungwook_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jungwook_Choi1">Jungwook Choi</a>, <a href="https://openreview.net/profile?id=~Wonyong_Sung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonyong_Sung1">Wonyong Sung</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AvcfxqRy4Y-details-887" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AvcfxqRy4Y-details-887"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformer, self attention, speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct roles: phonetic and linguistic localization. Especially, we show by experiments that phonetic localization in the lower layers extracts phonologically meaningful features from speech and reduces the phonetic variance in the utterance for proper linguistic localization in the upper layers. From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved. To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze the role of self attention in Transformer-based speech recognition and present a practical technique to design a model that accelerates the inference and improve the performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=AvcfxqRy4Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="8WawVDdKqlL" data-number="215">
        <h4>
          <a href="https://openreview.net/forum?id=8WawVDdKqlL">
              Label Encoding for Regression Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=8WawVDdKqlL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Deval_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deval_Shah1">Deval Shah</a>, <a href="https://openreview.net/profile?id=~Zi_Yu_Xue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zi_Yu_Xue1">Zi Yu Xue</a>, <a href="https://openreview.net/profile?id=~Tor_Aamodt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tor_Aamodt1">Tor Aamodt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#8WawVDdKqlL-details-471" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8WawVDdKqlL-details-471"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Regression, Label encoding, Output codes</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can improve accuracy by utilizing well-studied binary classification algorithms. We introduce binary-encoded labels (BEL), which generalizes the application of binary classification to regression by providing a framework for considering arbitrary multi-bit values when encoding target values. We identify desirable properties of suitable encoding and decoding functions used for the conversion between real-valued and binary-encoded labels based on theoretical and empirical study. These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings. BEL can be combined with off-the-shelf task-specific feature extractors and trained end-to-end. We propose a series of sample encoding, decoding, and training loss functions for BEL and demonstrate they result in lower error than direct regression and specialized approaches while being suitable for a diverse set of regression problems, network architectures, and evaluation metrics. BEL achieves state-of-the-art accuracies for several regression benchmarks. Code is available at https://github.com/ubc-aamodt-group/BEL_regression.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose binary-encoded labels (BEL) which improves regression by generalizing the application of binary classification. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=8WawVDdKqlL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zNHzqZ9wrRB" data-number="202">
        <h4>
          <a href="https://openreview.net/forum?id=zNHzqZ9wrRB">
              Equivariant Transformers for Neural Network based Molecular Potentials
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zNHzqZ9wrRB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Philipp_Th%C3%B6lke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Thölke1">Philipp Thölke</a>, <a href="https://openreview.net/profile?id=~Gianni_De_Fabritiis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gianni_De_Fabritiis1">Gianni De Fabritiis</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zNHzqZ9wrRB-details-224" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zNHzqZ9wrRB-details-224"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Molecular Modeling, Quantum Chemistry, Attention, Transformers</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields. In this work we propose TorchMD-NET, a novel equivariant Transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1, and many QM9 targets in both accuracy and computational efficiency. Through an extensive attention weight analysis, we gain valuable insights into the black box predictor and show differences in the learned representation of conformers versus conformations sampled from molecular dynamics or normal modes. Furthermore, we highlight the importance of datasets including off-equilibrium conformations for the evaluation of molecular potentials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel equivariant Transformer architecture for the prediction of molecular potentials and provide insights into the molecular representation through extensive analysis of the model's attention weights.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="9XhPLAjjRB" data-number="172">
        <h4>
          <a href="https://openreview.net/forum?id=9XhPLAjjRB">
              SGD Can Converge to Local Maxima
          </a>
        
          
            <a href="https://openreview.net/pdf?id=9XhPLAjjRB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Ziyin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liu_Ziyin1">Liu Ziyin</a>, <a href="https://openreview.net/profile?email=botao.li95%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="botao.li95@gmail.com">Botao Li</a>, <a href="https://openreview.net/profile?id=~James_B_Simon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_B_Simon1">James B Simon</a>, <a href="https://openreview.net/profile?id=~Masahito_Ueda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masahito_Ueda1">Masahito Ueda</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#9XhPLAjjRB-details-715" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9XhPLAjjRB-details-715"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stochastic gradient descent, saddle points, convergence, amsgrad, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that it can be common for SGD to converge to saddle points and maxima.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="H0oaWl6THa" data-number="158">
        <h4>
          <a href="https://openreview.net/forum?id=H0oaWl6THa">
              Hybrid Local SGD for Federated Learning with Heterogeneous Communications
          </a>
        
          
            <a href="https://openreview.net/pdf?id=H0oaWl6THa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuanxiong_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanxiong_Guo1">Yuanxiong Guo</a>, <a href="https://openreview.net/profile?id=~Ying_Sun5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Sun5">Ying Sun</a>, <a href="https://openreview.net/profile?email=ruihu2017%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruihu2017@gmail.com">Rui Hu</a>, <a href="https://openreview.net/profile?id=~Yanmin_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanmin_Gong1">Yanmin Gong</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 02 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#H0oaWl6THa-details-807" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H0oaWl6THa-details-807"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated Learning, Communication Efficiency, Heterogeneity, Local SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive when each device needs to communicate with the remote server repeatedly for many times over bandwidth-limited networks. In light of both device-to-device (D2D) and device-to-server (D2S) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best balance between model accuracy and runtime. We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings. We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2S communications in HL-SGD can largely speed up the training time of federated learning. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EAy7C1cgE1L" data-number="64">
        <h4>
          <a href="https://openreview.net/forum?id=EAy7C1cgE1L">
              Increasing the Cost of Model Extraction with Calibrated Proof of Work
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EAy7C1cgE1L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Adam_Dziedzic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Dziedzic1">Adam Dziedzic</a>, <a href="https://openreview.net/profile?id=~Muhammad_Ahmad_Kaleem1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhammad_Ahmad_Kaleem1">Muhammad Ahmad Kaleem</a>, <a href="https://openreview.net/profile?id=~Yu_Shen_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Shen_Lu1">Yu Shen Lu</a>, <a href="https://openreview.net/profile?id=~Nicolas_Papernot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Papernot1">Nicolas Papernot</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EAy7C1cgE1L-details-726" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EAy7C1cgE1L-details-726"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model extraction, model stealing, model functionality stealing, proof-of-work, adversarial machine learning, trustworthy machine learning, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose to make model extraction more difficult by requiring users to complete a callibrated proof-of-work before they can read predictions from a machine learning model exposed via a public API.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=EAy7C1cgE1L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HFmAukZ-k-2" data-number="34">
        <h4>
          <a href="https://openreview.net/forum?id=HFmAukZ-k-2">
              Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HFmAukZ-k-2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marten_Lienen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marten_Lienen1">Marten Lienen</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HFmAukZ-k-2-details-248" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HFmAukZ-k-2-details-248"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spatio-temporal, finite, elements, forecasting, continuous, partial, differential, equation, PDE, graph, gnn, time-series</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A continuous-time graph neural network model for spatio-temporal forecasting that can structurally incorporate prior knowledge</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="BnQhMqDfcKG" data-number="4">
        <h4>
          <a href="https://openreview.net/forum?id=BnQhMqDfcKG">
              Probabilistic Implicit Scene Completion
          </a>
        
          
            <a href="https://openreview.net/pdf?id=BnQhMqDfcKG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsu_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongsu_Zhang1">Dongsu Zhang</a>, <a href="https://openreview.net/profile?id=~Changwoon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changwoon_Choi1">Changwoon Choi</a>, <a href="https://openreview.net/profile?id=~Inbum_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Inbum_Park1">Inbum Park</a>, <a href="https://openreview.net/profile?id=~Young_Min_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Young_Min_Kim1">Young Min Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#BnQhMqDfcKG-details-723" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BnQhMqDfcKG-details-723"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">3D shape completion, 3D generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a scalable generative model for multi-modal completion of 3D scenes in implicit representation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7l1IjZVddDW" data-number="1">
        <h4>
          <a href="https://openreview.net/forum?id=7l1IjZVddDW">
              Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7l1IjZVddDW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qiang_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Meng1">Qiang Meng</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feng_Zhou8">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~Hainan_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hainan_Ren1">Hainan Ren</a>, <a href="https://openreview.net/profile?id=~Tianshu_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianshu_Feng1">Tianshu Feng</a>, <a href="https://openreview.net/profile?id=~Guochao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guochao_Liu1">Guochao Liu</a>, <a href="https://openreview.net/profile?id=~Yuanqing_Lin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanqing_Lin3">Yuanqing Lin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7l1IjZVddDW-details-296" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7l1IjZVddDW-details-296"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>



<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class=" active " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="5">
          <span>›</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="4">
          <span>»</span>
      </li>
  </ul>
</nav>



</div>
    <div role="tabpanel" class="tab-pane fade active in" id="poster-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="upnDJ7itech" data-number="3063">
        <h4>
          <a href="https://openreview.net/forum?id=upnDJ7itech">
              Knowledge Infused Decoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=upnDJ7itech" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ruibo_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruibo_Liu1">Ruibo Liu</a>, <a href="https://openreview.net/profile?id=~Guoqing_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guoqing_Zheng1">Guoqing Zheng</a>, <a href="https://openreview.net/profile?id=~Shashank_Gupta3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shashank_Gupta3">Shashank Gupta</a>, <a href="https://openreview.net/profile?id=~Radhika_Gaonkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Radhika_Gaonkar1">Radhika Gaonkar</a>, <a href="https://openreview.net/profile?id=~Chongyang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongyang_Gao1">Chongyang Gao</a>, <a href="https://openreview.net/profile?id=~Soroush_Vosoughi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soroush_Vosoughi1">Soroush Vosoughi</a>, <a href="https://openreview.net/profile?id=~Milad_Shokouhi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Milad_Shokouhi1">Milad Shokouhi</a>, <a href="https://openreview.net/profile?id=~Ahmed_Hassan_Awadallah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Hassan_Awadallah1">Ahmed Hassan Awadallah</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#upnDJ7itech-details-871" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="upnDJ7itech-details-871"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">natural language, decoding, reinforcement learning, knowledge integration, generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence. they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications.
        
        We present Knowledge Infused Decoding (KID)---a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new decoding algorithm for language model generation, to obtain better performance in knowledge-intensive tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="N1WI0vJLER" data-number="3062">
        <h4>
          <a href="https://openreview.net/forum?id=N1WI0vJLER">
              Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences
          </a>
        
          
            <a href="https://openreview.net/pdf?id=N1WI0vJLER" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Euhyun_Moon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Euhyun_Moon1">Euhyun Moon</a>, <a href="https://openreview.net/profile?id=~Eric_C_Cyr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_C_Cyr1">Eric C Cyr</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#N1WI0vJLER-details-985" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N1WI0vJLER-details-985"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GRU, MGRIT, parallel-in-time, distributed machine learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Parallelizing Gated Recurrent Unit (GRU) is a challenging task, as the training procedure of GRU is inherently sequential. Prior efforts to parallelize GRU have largely focused on conventional parallelization strategies such as data-parallel and model-parallel training algorithms. However, when the given sequences are very long, existing approaches are still inevitably performance limited in terms of both training time and model accuracy. In this paper, we present a novel parallel training scheme (called  parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and trains the sub-sequences on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset, where each video is an image sequence, demonstrate that a new parallel training scheme of GRU achieves up to $6.5 \times$ speedup over a serial approach. As efficiency of our new parallelization strategy is associated with the sequence length, our parallel GRU algorithm achieves significant performance improvement as the length of sequence increases. Further, the proposed approach can be applied simultaneously with batch and other forms of model parallelism.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper presents a novel parallel-in-time training scheme for GRU networks based on a MGRIT solver.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=N1WI0vJLER&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="73MEhZ0anV" data-number="3054">
        <h4>
          <a href="https://openreview.net/forum?id=73MEhZ0anV">
              QUERY EFFICIENT DECISION BASED SPARSE ATTACKS AGAINST BLACK-BOX DEEP LEARNING MODELS
          </a>
        
          
            <a href="https://openreview.net/pdf?id=73MEhZ0anV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Viet_Vo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Viet_Vo1">Viet Vo</a>, <a href="https://openreview.net/profile?id=~Ehsan_M_Abbasnejad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ehsan_M_Abbasnejad1">Ehsan M Abbasnejad</a>, <a href="https://openreview.net/profile?email=damith.ranasinghe%40adelaide.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="damith.ranasinghe@adelaide.edu.au">Damith Ranasinghe</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#73MEhZ0anV-details-358" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="73MEhZ0anV-details-358"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">decision-based attacks, sparse attacks, evolution algorithms, vision transformer, convolutional neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as Machine Learning as a Service (MLaaS), particularly $sparse~attacks$. The realization of sparse attacks in black-box settings demonstrates that machine learning models are more vulnerable than we believe. Because, these attacks aim to $minimize~the~number~of~perturbed~pixels$—measured by $l_0$ norm—required to mislead a model by $solely$ observing the decision ($the~predicted~label$) returned to a model query; the so-called $decision-based~setting$. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm—$SparseEvo$—for the problem and evaluate it against both convolutional deep neural networks and $vision~transformers$. Notably, vision transformers are yet to be investigated under a decision-based attack setting. SparseEvo requires significantly fewer queries than the state-of-the-art sparse attack $Pointwise$ for both untargeted and targeted attacks. The attack algorithm, although conceptually simple, is competitive with only a limited query budget against the state-of-the-art gradient-based $white-box$ attacks in standard computer vision tasks such as $ImageNet$. Importantly, the query efficient SparseEvo, along with decision-based attacks, in general, raises new questions regarding the safety of deployed systems and poses new directions to study and understand the robustness of machine learning models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gJLEXy3ySpu" data-number="3051">
        <h4>
          <a href="https://openreview.net/forum?id=gJLEXy3ySpu">
              Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gJLEXy3ySpu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jinyuan_Jia2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinyuan_Jia2">Jinyuan Jia</a>, <a href="https://openreview.net/profile?id=~Binghui_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binghui_Wang2">Binghui Wang</a>, <a href="https://openreview.net/profile?id=~Xiaoyu_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyu_Cao1">Xiaoyu Cao</a>, <a href="https://openreview.net/profile?id=~Hongbin_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongbin_Liu2">Hongbin Liu</a>, <a href="https://openreview.net/profile?id=~Neil_Zhenqiang_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Zhenqiang_Gong1">Neil Zhenqiang Gong</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gJLEXy3ySpu-details-443" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gJLEXy3ySpu-details-443"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\ell_0$-norm adversarial perturbation is easy to interpret and can be implemented in the physical world. Therefore, certifying  robustness of top-$k$ predictions against $\ell_0$-norm adversarial perturbation is important. However, existing studies either focused on certifying $\ell_0$-norm robustness of top-$1$ predictions or  $\ell_2$-norm robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our approach is based on randomized smoothing, which builds a provably robust classifier from an arbitrary classifier via randomizing an input. Our major theoretical contribution is an almost tight $\ell_0$-norm certified robustness guarantee for top-$k$ predictions. We empirically evaluate our method on CIFAR10 and ImageNet. For instance, our method can build a classifier that achieves a certified top-3 accuracy of 69.2\% on ImageNet when an attacker can arbitrarily perturb 5 pixels of a testing image. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this work, we derive the certified robustness against $\ell_0$-norm adversarial perturbation for top-$k$ prediction.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Vjki79-619-" data-number="3046">
        <h4>
          <a href="https://openreview.net/forum?id=Vjki79-619-">
              Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Vjki79-619-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arthur_da_Cunha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_da_Cunha1">Arthur da Cunha</a>, <a href="https://openreview.net/profile?id=~Emanuele_Natale1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emanuele_Natale1">Emanuele Natale</a>, <a href="https://openreview.net/profile?id=~Laurent_Viennot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurent_Viennot1">Laurent Viennot</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Vjki79-619--details-694" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Vjki79-619--details-694"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">lottery ticket hypothesis, convolutional neural network, network pruning, random subset sum, random neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs).
        In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove the lottery ticket hypothesis for convolutional neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Vjki79-619-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Vog_3GXsgmb" data-number="3042">
        <h4>
          <a href="https://openreview.net/forum?id=Vog_3GXsgmb">
              Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Vog_3GXsgmb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chengping_Rao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengping_Rao1">Chengping Rao</a>, <a href="https://openreview.net/profile?id=~Pu_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pu_Ren1">Pu Ren</a>, <a href="https://openreview.net/profile?id=~Yang_Liu52" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu52">Yang Liu</a>, <a href="https://openreview.net/profile?id=~Hao_Sun4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Sun4">Hao Sun</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 08 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">42 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Vog_3GXsgmb-details-699" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Vog_3GXsgmb-details-699"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Data-driven equation discovery, dynamical system modeling, physics-encoded learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There have been growing interests in leveraging experimental measurements to discover the underlying partial differential equations (PDEs) that govern complex physical phenomena. Although past research attempts have achieved great success in data-driven PDE discovery, the robustness of the existing methods cannot be guaranteed when dealing with low-quality measurement data. To overcome this challenge, we propose a novel physics-encoded discrete learning framework for discovering spatiotemporal PDEs from scarce and noisy data. The general idea is to (1) firstly introduce a novel deep convolutional-recurrent networks, which can encode prior physics knowledge (e.g., known terms, assumed PDE structure, initial/boundary conditions, etc.) while remaining flexible on representation capability, to accurately reconstruct high-fidelity data, and (2) then perform sparse regression with the reconstructed data to identify the analytical form of the governing PDEs. We validate our proposed framework on three high-dimensional PDE systems. The effectiveness and superiority of the proposed method over baselines are demonstrated.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work seeks to solve the data-driven governing equation discovery problem with a novel physics-encoded learning framework.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KJztlfGPdwW" data-number="3038">
        <h4>
          <a href="https://openreview.net/forum?id=KJztlfGPdwW">
              Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KJztlfGPdwW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rui_Yang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Yang8">Rui Yang</a>, <a href="https://openreview.net/profile?id=~Yiming_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiming_Lu1">Yiming Lu</a>, <a href="https://openreview.net/profile?id=~Wenzhe_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenzhe_Li2">Wenzhe Li</a>, <a href="https://openreview.net/profile?id=~Hao_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Sun3">Hao Sun</a>, <a href="https://openreview.net/profile?id=~Meng_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meng_Fang1">Meng Fang</a>, <a href="https://openreview.net/profile?id=~Yali_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yali_Du1">Yali Du</a>, <a href="https://openreview.net/profile?id=~Xiu_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiu_Li1">Xiu Li</a>, <a href="https://openreview.net/profile?id=~Lei_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Han1">Lei Han</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">40 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KJztlfGPdwW-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KJztlfGPdwW-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Goal-conditioned reinforcement learning, offline reinforcement learning, goal-conditioned supervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Solving goal-conditioned tasks with sparse rewards using self-supervised learning is promising because of its simplicity and stability over current reinforcement learning (RL) algorithms. A recent work, called Goal-Conditioned Supervised Learning (GCSL), provides a new learning framework by iteratively relabeling and imitating self-generated experiences. In this paper, we revisit the theoretical property of GCSL --- optimizing a lower bound of the goal reaching objective, and extend GCSL as a novel offline goal-conditioned RL algorithm. The proposed method is named Weighted GCSL (WGCSL), in which we introduce an advanced compound weight consisting of three parts (1) discounted weight for goal relabeling, (2) goal-conditioned exponential advantage weight, and (3) best-advantage weight. Theoretically, WGCSL is proved to optimize an equivalent lower bound of the goal-conditioned RL objective and generates monotonically improved policies via an iterated scheme. The monotonic property holds for any behavior policies, and therefore WGCSL can be applied to both online and offline settings. To evaluate algorithms in the offline goal-conditioned RL setting, we provide a benchmark including a range of point and simulated robot domains. Experiments in the introduced benchmark demonstrate that WGCSL can consistently outperform GCSL and existing state-of-the-art offline methods in the fully offline goal-conditioned setting.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We revisit GCSL's theoretical foundation and present a simple but effective algorithm for offline goal-conditioned RL via weighted supervised learning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="P1QUVhOtEFP" data-number="3034">
        <h4>
          <a href="https://openreview.net/forum?id=P1QUVhOtEFP">
              Topologically Regularized Data Embeddings
          </a>
        
          
            <a href="https://openreview.net/pdf?id=P1QUVhOtEFP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Robin_Vandaele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robin_Vandaele1">Robin Vandaele</a>, <a href="https://openreview.net/profile?id=~Bo_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Kang1">Bo Kang</a>, <a href="https://openreview.net/profile?id=~Jefrey_Lijffijt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jefrey_Lijffijt1">Jefrey Lijffijt</a>, <a href="https://openreview.net/profile?id=~Tijl_De_Bie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tijl_De_Bie1">Tijl De Bie</a>, <a href="https://openreview.net/profile?id=~Yvan_Saeys1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yvan_Saeys1">Yvan Saeys</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#P1QUVhOtEFP-details-803" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="P1QUVhOtEFP-details-803"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Embedding, Dimensionality Reduction, Topological Data Analysis, Persistent Homology, Optimization, Regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised feature learning often finds low-dimensional embeddings that capture the structure of complex data.  For tasks for which prior expert topological knowledge is available, incorporating this into the learned representation may lead to higher quality embeddings. For example, this may help one to embed the data into a given number of clusters, or to accommodate for noise that prevents one from deriving the distribution of the data over the model directly, which can then be learned more effectively. However, a general tool for integrating different prior topological knowledge into embeddings is lacking. Although differentiable topology layers have been recently developed that can (re)shape embeddings into prespecified topological models, they have two important limitations for representation learning, which we address in this paper. First, the currently suggested topological losses fail to represent simple models such as clusters and flares in a natural manner. Second, these losses neglect all original structural (such as neighborhood) information in the data that is useful for learning. We overcome these limitations by introducing a new set of topological losses, and proposing their usage as a way for topologically regularizing data embeddings to naturally represent a prespecified model. We include thorough experiments on synthetic and real data that highlight the usefulness and versatility of this approach, with applications ranging from modeling high-dimensional single-cell data, to graph embedding.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A method for incorporating expert prior topological knowledge into data embeddings.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=P1QUVhOtEFP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oh4TirnfSem" data-number="3033">
        <h4>
          <a href="https://openreview.net/forum?id=oh4TirnfSem">
              PF-GNN: Differentiable particle filtering based approximation of universal graph representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oh4TirnfSem" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mohammed_Haroon_Dupty1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammed_Haroon_Dupty1">Mohammed Haroon Dupty</a>, <a href="https://openreview.net/profile?id=~Yanfei_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanfei_Dong1">Yanfei Dong</a>, <a href="https://openreview.net/profile?id=~Wee_Sun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wee_Sun_Lee1">Wee Sun Lee</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oh4TirnfSem-details-612" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oh4TirnfSem-details-612"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Graph representation learning, Expressive GNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of $\textit{Individualization and refinement}$ (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search-tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings ( i.e. embeddings) by sampling multiple paths from root to leaves of the search-tree. To learn more discriminative representations, we guide the sampling process with $\textit{particle filter}$ updates, a principled approach for sequential state estimation. Our algorithm is end-to-end differentiable, can be applied with any GNN as backbone and learns richer graph representations with only linear increase in runtime. Experimental evaluation shows that our approach consistently outperforms leading GNN models on both synthetic benchmarks for isomorphism detection as well as real-world datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Increasing the expressive power of Graph Neural Networks by using techniques from exact isomorphism solvers with a particle filtering approach.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AMpki9kp8Cn" data-number="3031">
        <h4>
          <a href="https://openreview.net/forum?id=AMpki9kp8Cn">
              Nonlinear ICA Using Volume-Preserving Transformations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AMpki9kp8Cn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaojiang_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojiang_Yang1">Xiaojiang Yang</a>, <a href="https://openreview.net/profile?id=~Yi_Wang24" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wang24">Yi Wang</a>, <a href="https://openreview.net/profile?id=~Jiacheng_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiacheng_Sun1">Jiacheng Sun</a>, <a href="https://openreview.net/profile?id=~Xing_Zhang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xing_Zhang6">Xing Zhang</a>, <a href="https://openreview.net/profile?id=~Shifeng_Zhang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shifeng_Zhang5">Shifeng Zhang</a>, <a href="https://openreview.net/profile?id=~Zhenguo_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenguo_Li1">Zhenguo Li</a>, <a href="https://openreview.net/profile?id=~Junchi_Yan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junchi_Yan2">Junchi Yan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AMpki9kp8Cn-details-371" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AMpki9kp8Cn-details-371"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Independent Component Analysis, Nonlinear ICA, Identifiability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Nonlinear ICA is a fundamental problem in machine learning, aiming to identify the underlying independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. Recent works prove that if the sources have some particular structures (e.g. temporal structure), they are theoretically identifiable even if the mixing function is arbitrary. However, in many cases such restrictions on the sources are difficult to satisfy or even verify, hence it inhibits the applicability of the proposed methods. Different from these works, we propose a general framework for nonlinear ICA, in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. We provide an insightful proof of the identifiability of the proposed framework. We implement the framework by volume-preserving Flow-based models, and verify our theory by experiments on artificial data and synthesized images. Moreover, results on real-world images indicate that our framework can disentangle interpretable features.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a general framework for nonlinear ICA, in which the mixing function is restricted to a volume-preserving transformation, and establish two novel identifiability theorems and provide insightful proofs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="18Ys0-PzyPI" data-number="3030">
        <h4>
          <a href="https://openreview.net/forum?id=18Ys0-PzyPI">
              Online Ad Hoc Teamwork under Partial Observability
          </a>
        
          
            <a href="https://openreview.net/pdf?id=18Ys0-PzyPI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pengjie_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengjie_Gu1">Pengjie Gu</a>, <a href="https://openreview.net/profile?id=~Mengchen_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengchen_Zhao1">Mengchen Zhao</a>, <a href="https://openreview.net/profile?email=haojianye%40huawei.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="haojianye@huawei.com">Jianye Hao</a>, <a href="https://openreview.net/profile?id=~Bo_An2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_An2">Bo An</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#18Ys0-PzyPI-details-150" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="18Ys0-PzyPI-details-150"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">coordination, reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, fixed and predefined teammates' types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vwLLQ-HwqhZ" data-number="3026">
        <h4>
          <a href="https://openreview.net/forum?id=vwLLQ-HwqhZ">
              Continual Normalization: Rethinking Batch Normalization for Online Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vwLLQ-HwqhZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Quang_Pham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quang_Pham1">Quang Pham</a>, <a href="https://openreview.net/profile?id=~Chenghao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenghao_Liu1">Chenghao Liu</a>, <a href="https://openreview.net/profile?id=~Steven_HOI1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_HOI1">Steven HOI</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vwLLQ-HwqhZ-details-762" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vwLLQ-HwqhZ-details-762"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Continual Learning, Batch Normalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation will be made publicly available upon acceptance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A negative effect of BN in online continual learning and a simple strategy to alleviate it.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vwLLQ-HwqhZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SHbhHHfePhP" data-number="3025">
        <h4>
          <a href="https://openreview.net/forum?id=SHbhHHfePhP">
              Equivariant Graph Mechanics Networks with Constraints
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SHbhHHfePhP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wenbing_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenbing_Huang1">Wenbing Huang</a>, <a href="https://openreview.net/profile?id=~Jiaqi_Han2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaqi_Han2">Jiaqi Han</a>, <a href="https://openreview.net/profile?id=~Yu_Rong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Rong1">Yu Rong</a>, <a href="https://openreview.net/profile?id=~Tingyang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tingyang_Xu1">Tingyang Xu</a>, <a href="https://openreview.net/profile?id=~Fuchun_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fuchun_Sun1">Fuchun Sun</a>, <a href="https://openreview.net/profile?id=~Junzhou_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junzhou_Huang2">Junzhou Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">36 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SHbhHHfePhP-details-614" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SHbhHHfePhP-details-614"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained.
        Current methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. 
        In this paper, we propose Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message passing in GMN, we have developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments  support the advantages of GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on the simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=SHbhHHfePhP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vfsRB5MImo9" data-number="3023">
        <h4>
          <a href="https://openreview.net/forum?id=vfsRB5MImo9">
              Towards Continual Knowledge Learning of Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vfsRB5MImo9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Joel_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joel_Jang1">Joel Jang</a>, <a href="https://openreview.net/profile?id=~Seonghyeon_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seonghyeon_Ye1">Seonghyeon Ye</a>, <a href="https://openreview.net/profile?id=~Sohee_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sohee_Yang1">Sohee Yang</a>, <a href="https://openreview.net/profile?id=~Joongbo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joongbo_Shin1">Joongbo Shin</a>, <a href="https://openreview.net/profile?id=~Janghoon_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Janghoon_Han1">Janghoon Han</a>, <a href="https://openreview.net/profile?id=~Gyeonghun_KIM1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gyeonghun_KIM1">Gyeonghun KIM</a>, <a href="https://openreview.net/profile?id=~Stanley_Jungkyu_Choi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stanley_Jungkyu_Choi2">Stanley Jungkyu Choi</a>, <a href="https://openreview.net/profile?id=~Minjoon_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjoon_Seo1">Minjoon Seo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vfsRB5MImo9-details-284" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vfsRB5MImo9-details-284"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continual learning, knowledge acquisition, catastrophic forgetting, large language models, pretraining, natural language processing</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel continual learning formulation named Continual Knowledge Learning which allows large language models to constantly obtain new and updated knowledge while mitigating forgetting of previous learned time-invariant knowledge.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="nf3A0WZsXS5" data-number="3016">
        <h4>
          <a href="https://openreview.net/forum?id=nf3A0WZsXS5">
              Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns
          </a>
        
          
            <a href="https://openreview.net/pdf?id=nf3A0WZsXS5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhijian_Yang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhijian_Yang2">Zhijian Yang</a>, <a href="https://openreview.net/profile?email=junhao.wen89%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="junhao.wen89@gmail.com">Junhao Wen</a>, <a href="https://openreview.net/profile?id=~Christos_Davatzikos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christos_Davatzikos1">Christos Davatzikos</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#nf3A0WZsXS5-details-791" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nf3A0WZsXS5-details-791"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Representation Learning, Disease-related imaging patterns, Alzheimer's disease, MRI, GAN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A plethora of machine learning methods have been applied to imaging data, enabling the construction of clinically relevant imaging signatures of neurological and neuropsychiatric diseases. Oftentimes, such methods don't explicitly model the heterogeneity of disease effects, or approach it via nonlinear models that are not interpretable. Moreover, unsupervised methods may parse heterogeneity that is driven by nuisance confounding factors that affect brain structure or function, rather than heterogeneity relevant to a pathology of interest. On the other hand, semi-supervised clustering methods seek to derive a dichotomous subtype membership, ignoring the truth that disease heterogeneity spatially and temporally extends along a continuum.  To address the aforementioned limitations, herein, we propose a novel method, termed Surreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using cross-sectional imaging data, Surreal-GAN dissects underlying disease-related heterogeneity under the principle of semi-supervised clustering (cluster mappings from normal control to patient), proposes a continuously dimensional representation, and infers the disease severity of patients at individual level along each dimension. The model first learns a transformation function from normal control (CN) domain to the patient (PT) domain with latent variables controlling transformation directions. An inverse mapping function together with regularization on function continuity, pattern orthogonality and monotonicity was also imposed to make sure that the transformation function captures necessarily meaningful imaging patterns with clinical significance. We first validated the model through extensive semi-synthetic experiments, and then demonstrate its potential in capturing biologically plausible imaging patterns in Alzheimer's disease (AD).</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We proposed a novel method, Surreal-GAN, to derive low dimensional representation of disease-related patterns from neuroimaging data.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TfhfZLQ2EJO" data-number="3012">
        <h4>
          <a href="https://openreview.net/forum?id=TfhfZLQ2EJO">
              SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TfhfZLQ2EJO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jongjin_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongjin_Park1">Jongjin Park</a>, <a href="https://openreview.net/profile?id=~Younggyo_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Younggyo_Seo1">Younggyo Seo</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>, <a href="https://openreview.net/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Kimin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kimin_Lee1">Kimin Lee</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TfhfZLQ2EJO-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TfhfZLQ2EJO-details-370"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">preference-based reinforcement learning, human-in-the-loop reinforcement learning, deep reinforcement learning, semi-supervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor’s preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present SURF, a semi-supervised reward learning algorithm with data augmentation for feedback-efficient preference-based RL.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TfhfZLQ2EJO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ItkxLQU01lD" data-number="3011">
        <h4>
          <a href="https://openreview.net/forum?id=ItkxLQU01lD">
              Convergent Graph Solvers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ItkxLQU01lD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Junyoung_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junyoung_Park1">Junyoung Park</a>, <a href="https://openreview.net/profile?id=~Jinhyun_Choo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinhyun_Choo1">Jinhyun Choo</a>, <a href="https://openreview.net/profile?id=~Jinkyoo_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinkyoo_Park1">Jinkyoo Park</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ItkxLQU01lD-details-534" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ItkxLQU01lD-details-534"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph, Graph Neural Network, Fixed point, Implicit model, Implicit function theorem, Convergent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. The forward propagation of CGS proceeds in three steps: (1) constructing the input-dependent linear contracting iterative maps, (2) computing the fixed points of the iterative maps, and (3) decoding the fixed points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems,  irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_F9xpOrqyX9" data-number="3007">
        <h4>
          <a href="https://openreview.net/forum?id=_F9xpOrqyX9">
              Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_F9xpOrqyX9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Junhyun_Nam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junhyun_Nam1">Junhyun Nam</a>, <a href="https://openreview.net/profile?id=~Jaehyung_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaehyung_Kim1">Jaehyung Kim</a>, <a href="https://openreview.net/profile?id=~Jaeho_Lee3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaeho_Lee3">Jaeho Lee</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_F9xpOrqyX9-details-144" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_F9xpOrqyX9-details-144"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">worst-group loss minimization, spurious correlation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The paradigm of worst-group loss minimization has shown its promise in avoiding to learn spurious correlations, but requires costly additional supervision on spurious attributes. To resolve this, recent works focus on developing weaker forms of supervision---e.g., hyperparameters discovered with a small number of validation samples with spurious attribute annotation---but none of the methods retain comparable performance to methods using full supervision on the spurious attribute. In this paper, instead of searching for weaker supervisions, we ask: Given access to a fixed number of samples with spurious attribute annotations, what is the best achievable worst-group loss if we ''fully exploit'' them? To this end, we propose a pseudo-attribute-based algorithm, coined Spread Spurious Attribute (SSA), for improving the worst-group accuracy. In particular, we leverage samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo-attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst-group loss. Our experiments on various benchmark datasets show that our algorithm consistently outperforms the baseline methods using the same number of validation samples with spurious attribute annotations. We also demonstrate that the proposed SSA can achieve comparable performances to methods using full (100%) spurious attribute supervision, by using a much smaller number of annotated samples---from 0.6% and up to 1.5%, depending on the dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Using a small amount of attribute annotated samples for training can boost worst-group performance in the presence of spurious correlation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_F9xpOrqyX9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="06Wy2BtxXrz" data-number="2998">
        <h4>
          <a href="https://openreview.net/forum?id=06Wy2BtxXrz">
              Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=06Wy2BtxXrz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yaoxin_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaoxin_Wu2">Yaoxin Wu</a>, <a href="https://openreview.net/profile?id=~Wen_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wen_Song1">Wen Song</a>, <a href="https://openreview.net/profile?id=~Zhiguang_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiguang_Cao1">Zhiguang Cao</a>, <a href="https://openreview.net/profile?id=~Jie_Zhang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Zhang9">Jie Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 26 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#06Wy2BtxXrz-details-299" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="06Wy2BtxXrz-details-299"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Conditional Variational Autoencoder, Stochastic Integer Programming, Scenario Reduction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances. Specifically, we design a graph convolutional network based encoder to embed each scenario with the deterministic part of its instance (i.e. context) into a low-dimensional latent space, from which a decoder reconstructs the scenario from its latent representation conditioned on the context. Such a design effectively captures the dependencies of the scenarios on their corresponding instances. We apply the trained encoder to two tasks in typical SIP solving, i.e. scenario reduction and objective prediction. Experiments on two SIP problems show that the learned latent representation significantly boosts the solving performance to attain high-quality solutions in short computational time, and generalizes fairly well to problems of larger sizes or with more scenarios.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper provides a CVAE based method to learn scenario representations for solving stochastic integer programs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7grkzyj89A_" data-number="2996">
        <h4>
          <a href="https://openreview.net/forum?id=7grkzyj89A_">
              Generalization Through the Lens of Leave-One-Out Error
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7grkzyj89A_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gregor_Bachmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gregor_Bachmann1">Gregor Bachmann</a>, <a href="https://openreview.net/profile?id=~Thomas_Hofmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Hofmann1">Thomas Hofmann</a>, <a href="https://openreview.net/profile?id=~Aurelien_Lucchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aurelien_Lucchi1">Aurelien Lucchi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 01 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7grkzyj89A_-details-905" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7grkzyj89A_-details-905"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings~\citep{nagarajan2019uniform}. In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning.
        Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=7grkzyj89A_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="VPjw9KPWRSK" data-number="2992">
        <h4>
          <a href="https://openreview.net/forum?id=VPjw9KPWRSK">
              Self-Supervised Inference in State-Space Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=VPjw9KPWRSK" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~David_Ruhe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Ruhe1">David Ruhe</a>, <a href="https://openreview.net/profile?id=~Patrick_Forr%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Forré1">Patrick Forré</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#VPjw9KPWRSK-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VPjw9KPWRSK-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervision, inference, state-space model, Kalman filter, recurrent neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. It comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=VPjw9KPWRSK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SwIp410B6aQ" data-number="2983">
        <h4>
          <a href="https://openreview.net/forum?id=SwIp410B6aQ">
              On the Role of Neural Collapse in Transfer Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SwIp410B6aQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tomer_Galanti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomer_Galanti1">Tomer Galanti</a>, <a href="https://openreview.net/profile?id=~Andr%C3%A1s_Gy%C3%B6rgy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~András_György2">András György</a>, <a href="https://openreview.net/profile?id=~Marcus_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcus_Hutter1">Marcus Hutter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SwIp410B6aQ-details-810" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SwIp410B6aQ-details-810"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper, we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IpctgL7khPp" data-number="2977">
        <h4>
          <a href="https://openreview.net/forum?id=IpctgL7khPp">
              Information-theoretic Online Memory Selection for Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IpctgL7khPp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shengyang_Sun4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyang_Sun4">Shengyang Sun</a>, <a href="https://openreview.net/profile?id=~Daniele_Calandriello1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniele_Calandriello1">Daniele Calandriello</a>, <a href="https://openreview.net/profile?id=~Huiyi_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huiyi_Hu1">Huiyi Hu</a>, <a href="https://openreview.net/profile?id=~Ang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ang_Li1">Ang Li</a>, <a href="https://openreview.net/profile?id=~Michalis_Titsias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michalis_Titsias1">Michalis Titsias</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IpctgL7khPp-details-204" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IpctgL7khPp-details-204"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Task-free continual learning, replay memory, information theoretic, reservoir sampling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \textit{surprise} and the \textit{learnability} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of \textit{the timing to update the memory}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present information-theoretic algorithms to tackle the online memory selection problem in task-free and data imbalanced continual learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XHUxf5aRB3s" data-number="2975">
        <h4>
          <a href="https://openreview.net/forum?id=XHUxf5aRB3s">
              Dealing with Non-Stationarity in MARL via Trust-Region Decomposition
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XHUxf5aRB3s" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wenhao_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhao_Li2">Wenhao Li</a>, <a href="https://openreview.net/profile?id=~Xiangfeng_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangfeng_Wang1">Xiangfeng Wang</a>, <a href="https://openreview.net/profile?id=~Bo_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Jin1">Bo Jin</a>, <a href="https://openreview.net/profile?id=~Junjie_Sheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junjie_Sheng1">Junjie Sheng</a>, <a href="https://openreview.net/profile?id=~Hongyuan_Zha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongyuan_Zha1">Hongyuan Zha</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XHUxf5aRB3s-details-299" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XHUxf5aRB3s-details-299"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nonstationarity, Trust-Region Methods, Multi-Agent Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\delta$-$stationarity$ measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XHUxf5aRB3s&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kF9DZQQrU0w" data-number="2974">
        <h4>
          <a href="https://openreview.net/forum?id=kF9DZQQrU0w">
              Information Bottleneck: Exact Analysis of (Quantized) Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kF9DZQQrU0w" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Stephan_Sloth_Lorenzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Sloth_Lorenzen1">Stephan Sloth Lorenzen</a>, <a href="https://openreview.net/profile?id=~Christian_Igel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Igel1">Christian Igel</a>, <a href="https://openreview.net/profile?id=~Mads_Nielsen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mads_Nielsen2">Mads Nielsen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kF9DZQQrU0w-details-511" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kF9DZQQrU0w-details-511"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">information bottleneck, quantization, neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases  during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that  different ways of binning  when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures.
        To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. 
        In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate the information bottleneck in quantized neural networks, allowing us to compute the exact mutual information and provide an analysis free from estimation artifacts.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=kF9DZQQrU0w&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XLxhEjKNbXj" data-number="2965">
        <h4>
          <a href="https://openreview.net/forum?id=XLxhEjKNbXj">
              GLASS: GNN with Labeling Tricks for Subgraph Representation Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XLxhEjKNbXj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiyuan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiyuan_Wang1">Xiyuan Wang</a>, <a href="https://openreview.net/profile?id=~Muhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhan_Zhang1">Muhan Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">25 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XLxhEjKNbXj-details-447" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XLxhEjKNbXj-details-447"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite the remarkable achievements of Graph Neural Networks (GNNs) on graph representation learning, few works have tried to use them to predict properties of subgraphs in the whole graph. The existing state-of-the-art method SubGNN introduces an overly complicated subgraph-level GNN model which synthesizes three artificial channels each of which has two carefully designed subgraph-level message passing modules, yet only slightly outperforms a plain GNN which performs node-level message passing and then pools node embeddings within the subgraph. By analyzing SubGNN and plain GNNs, we find that the key for subgraph representation learning might be to distinguish nodes inside and outside the subgraph. With this insight, we propose an expressive and scalable labeling trick, namely max-zero-one, to enhance plain GNNs for subgraph tasks. The resulting model is called GLASS (GNN with LAbeling trickS for Subgraph). We theoretically characterize GLASS's expressive power. Compared with SubGNN, GLASS is more expressive, more scalable, and easier to implement. Experiments on eight benchmark datasets show that GLASS outperforms the strongest baseline by $14.8\%$ on average. And ablation analysis shows that our max-zero-one labeling trick can boost the performance of a plain GNN by up to $105\%$ in maximum, which illustrates the effectiveness of labeling trick on subgraph tasks. Furthermore, training a GLASS model only takes $37\%$ time needed for a SubGNN on average. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XLxhEjKNbXj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DnG75_KyHjX" data-number="2964">
        <h4>
          <a href="https://openreview.net/forum?id=DnG75_KyHjX">
              MoReL: Multi-omics Relational Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DnG75_KyHjX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arman_Hasanzadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arman_Hasanzadeh1">Arman Hasanzadeh</a>, <a href="https://openreview.net/profile?id=~Ehsan_Hajiramezanali1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ehsan_Hajiramezanali1">Ehsan Hajiramezanali</a>, <a href="https://openreview.net/profile?id=~Nick_Duffield1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nick_Duffield1">Nick Duffield</a>, <a href="https://openreview.net/profile?id=~Xiaoning_Qian2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoning_Qian2">Xiaoning Qian</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DnG75_KyHjX-details-61" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DnG75_KyHjX-details-61"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">relational learning, data integration, multi-view learning, Bayesian generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efficiently infer a multi-partite graph encoding molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-specific side information, either with graph-structured or unstructured data in different views, but also increases the model flexibility with the distribution-based regularization. This allows efficient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="BwPaPxwgyQb" data-number="2961">
        <h4>
          <a href="https://openreview.net/forum?id=BwPaPxwgyQb">
              Provable Learning-based Algorithm For Sparse Recovery
          </a>
        
          
            <a href="https://openreview.net/pdf?id=BwPaPxwgyQb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xinshi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinshi_Chen1">Xinshi Chen</a>, <a href="https://openreview.net/profile?id=~Haoran_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoran_Sun2">Haoran Sun</a>, <a href="https://openreview.net/profile?id=~Le_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Le_Song1">Le Song</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">28 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#BwPaPxwgyQb-details-827" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BwPaPxwgyQb-details-827"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">learning to learn, sparse parameter estimation, learning to optimize, algorithm unrolling, generalization bound</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but their performances rely on choosing the correct hyperparameters. Besides, hand-designed algorithms do not fully exploit the particular problem distribution of interest. In this work, we propose a deep learning method for algorithm learning called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm). PLISA is designed by unrolling a classic path-following algorithm for sparse recovery, with some components being more flexible and learnable. We theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations. The techniques could potentially be applied to analyze other learning-based algorithms in the literature.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="jJOjjiZHy3h" data-number="2957">
        <h4>
          <a href="https://openreview.net/forum?id=jJOjjiZHy3h">
              Defending Against Image Corruptions Through Adversarial Augmentations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=jJOjjiZHy3h" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dan_Andrei_Calian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Andrei_Calian1">Dan Andrei Calian</a>, <a href="https://openreview.net/profile?id=~Florian_Stimberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Stimberg1">Florian Stimberg</a>, <a href="https://openreview.net/profile?id=~Olivia_Wiles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivia_Wiles1">Olivia Wiles</a>, <a href="https://openreview.net/profile?id=~Sylvestre-Alvise_Rebuffi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sylvestre-Alvise_Rebuffi1">Sylvestre-Alvise Rebuffi</a>, <a href="https://openreview.net/profile?id=~Andr%C3%A1s_Gy%C3%B6rgy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~András_György2">András György</a>, <a href="https://openreview.net/profile?id=~Timothy_A_Mann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_A_Mann1">Timothy A Mann</a>, <a href="https://openreview.net/profile?id=~Sven_Gowal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sven_Gowal2">Sven Gowal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">34 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#jJOjjiZHy3h-details-777" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jJOjjiZHy3h-details-777"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robustness, adversarial training, image corruptions</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on Lp-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against Lp-norm bounded perturbations on both CIFAR-10 and ImageNet.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Our theoretically-supported method finds adversarial examples by optimizing over the weights of pre-trained autoencoders, and yields classifiers with improved robustness to image corruptions.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Zf4ZdI4OQPV" data-number="2950">
        <h4>
          <a href="https://openreview.net/forum?id=Zf4ZdI4OQPV">
              Attacking deep networks with surrogate-based adversarial black-box methods is easy
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Zf4ZdI4OQPV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nicholas_A._Lord1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_A._Lord1">Nicholas A. Lord</a>, <a href="https://openreview.net/profile?id=~Romain_Mueller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Romain_Mueller1">Romain Mueller</a>, <a href="https://openreview.net/profile?id=~Luca_Bertinetto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luca_Bertinetto1">Luca Bertinetto</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Zf4ZdI4OQPV-details-798" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zf4ZdI4OQPV-details-798"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial attacks, black-box attacks, network robustness, network analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a search which uses the surrogate network's class-score gradients, with no need for other priors or heuristics. The guiding assumption of the algorithm is that the studied networks are in a fundamental sense learning similar functions, and that a transfer attack from one to the other should thus be fairly "easy". This assumption is validated by the extremely low query counts and failure rates achieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a ResNet-152 as the surrogate yields a median query count of 6 at a success rate of 99.9%. Code is available at https://github.com/fiveai/GFCS.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a simple and extremely effective score- and surrogate-based black-box adversarial attack which uses a specific gradient/Jacobian transfer strategy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Lm8T39vLDTE" data-number="2941">
        <h4>
          <a href="https://openreview.net/forum?id=Lm8T39vLDTE">
              Autoregressive Diffusion Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Lm8T39vLDTE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Emiel_Hoogeboom1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emiel_Hoogeboom1">Emiel Hoogeboom</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>, <a href="https://openreview.net/profile?id=~Jasmijn_Bastings1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jasmijn_Bastings1">Jasmijn Bastings</a>, <a href="https://openreview.net/profile?id=~Ben_Poole1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Poole1">Ben Poole</a>, <a href="https://openreview.net/profile?id=~Rianne_van_den_Berg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rianne_van_den_Berg1">Rianne van den Berg</a>, <a href="https://openreview.net/profile?id=~Tim_Salimans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Salimans1">Tim Salimans</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Lm8T39vLDTE-details-794" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Lm8T39vLDTE-details-794"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">diffusion, autoregressive models, lossless compression</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new model class for discrete variables encompassing order agnostic autoregressive models and absorbing discrete diffusion.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="H94a1_Pyr-6" data-number="2939">
        <h4>
          <a href="https://openreview.net/forum?id=H94a1_Pyr-6">
              Auto-scaling Vision Transformers without Training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=H94a1_Pyr-6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wuyang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wuyang_Chen1">Wuyang Chen</a>, <a href="https://openreview.net/profile?id=~Wei_Huang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Huang6">Wei Huang</a>, <a href="https://openreview.net/profile?id=~Xianzhi_Du4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xianzhi_Du4">Xianzhi Du</a>, <a href="https://openreview.net/profile?id=~Xiaodan_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Song1">Xiaodan Song</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>, <a href="https://openreview.net/profile?id=~Denny_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denny_Zhou1">Denny Zhou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 28 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#H94a1_Pyr-6-details-966" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H94a1_Pyr-6-details-966"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">vision transformer, neural architecture search, training-free search, efficient training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner. Specifically, we first design a "seed" ViT topology by leveraging a training-free search process. This extremely fast search is fulfilled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the "seed" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a unified framework, As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-to-end model design and scaling process costs only 12 hours on one V100 GPU. Our code is available at https://github.com/VITA-Group/AsViT.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We automate the design and scaling of vision transformers without any training, achieving state-of-the-art performance on ImageNet classification and COCO object detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=H94a1_Pyr-6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KPEFXR1HdIo" data-number="2937">
        <h4>
          <a href="https://openreview.net/forum?id=KPEFXR1HdIo">
              Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KPEFXR1HdIo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Deshan_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deshan_Gong1">Deshan Gong</a>, <a href="https://openreview.net/profile?id=~Zhanxing_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhanxing_Zhu1">Zhanxing Zhu</a>, <a href="https://openreview.net/profile?id=~Andrew_J._Bulpitt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_J._Bulpitt1">Andrew J. Bulpitt</a>, <a href="https://openreview.net/profile?id=~He_Wang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~He_Wang6">He Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KPEFXR1HdIo-details-288" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KPEFXR1HdIo-details-288"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming relatively simple structures and forces. Their granularity is intrinsically coarse and therefore incapable of modelling complex physical phenomena. Fine-grained models are still to be developed to incorporate sophisticated material structures and force interactions with gradient-based learning. Following this motivation, we propose a new differentiable fabrics model for composite materials such as cloths, where we dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, we propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, we demonstrate our model's $\textit{explicability}$ in learning meaningful physical parameters, $\textit{versatility}$ in incorporating complex physical structures and heterogeneous materials, $\textit{data-efficiency}$ in learning, and $\textit{high-fidelity}$ in capturing subtle dynamics.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=KPEFXR1HdIo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6y2KBh-0Fd9" data-number="2934">
        <h4>
          <a href="https://openreview.net/forum?id=6y2KBh-0Fd9">
              Revisiting flow generative models for Out-of-distribution detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6y2KBh-0Fd9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dihong_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dihong_Jiang1">Dihong Jiang</a>, <a href="https://openreview.net/profile?id=~Sun_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sun_Sun1">Sun Sun</a>, <a href="https://openreview.net/profile?id=~Yaoliang_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaoliang_Yu1">Yaoliang Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6y2KBh-0Fd9-details-933" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6y2KBh-0Fd9-details-933"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">flow models, out-of-distribution detection, random projection, distribution comparison</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work,  we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection.  Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Te5ytkqsnl" data-number="2929">
        <h4>
          <a href="https://openreview.net/forum?id=Te5ytkqsnl">
              Missingness Bias in Model Debugging
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Te5ytkqsnl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Saachi_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saachi_Jain1">Saachi Jain</a>, <a href="https://openreview.net/profile?id=~Hadi_Salman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hadi_Salman1">Hadi Salman</a>, <a href="https://openreview.net/profile?id=~Eric_Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Wong1">Eric Wong</a>, <a href="https://openreview.net/profile?id=~Pengchuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengchuan_Zhang1">Pengchuan Zhang</a>, <a href="https://openreview.net/profile?id=~Vibhav_Vineet5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vibhav_Vineet5">Vibhav Vineet</a>, <a href="https://openreview.net/profile?id=~Sai_Vemprala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sai_Vemprala1">Sai Vemprala</a>, <a href="https://openreview.net/profile?id=~Aleksander_Madry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksander_Madry1">Aleksander Madry</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Te5ytkqsnl-details-236" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Te5ytkqsnl-details-236"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model debugging, vision transformers, missingness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate how current missingness approximations for model debugging can impose undesirable biases on the model predictions and hinder our ability to debug models, and we show how transformer-based architectures can side-step these issues.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GQd7mXSPua" data-number="2905">
        <h4>
          <a href="https://openreview.net/forum?id=GQd7mXSPua">
              Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GQd7mXSPua" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jeffrey_Ryan_Willette1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeffrey_Ryan_Willette1">Jeffrey Ryan Willette</a>, <a href="https://openreview.net/profile?id=~Hae_Beom_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hae_Beom_Lee1">Hae Beom Lee</a>, <a href="https://openreview.net/profile?id=~Juho_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juho_Lee2">Juho Lee</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GQd7mXSPua-details-172" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GQd7mXSPua-details-172"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">calibration, meta-learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance sensitive model are used to construct feature covariance matrices which are used in deterministic uncertainty estimation or OOD detection. However, in cases where there is a distribution over tasks, these methods result in covariances which are sub-optimal, as they may not leverage all of the meta information which can be shared among tasks. With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices. Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which is well calibrated under a distributional dataset shift. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel meta learning algorithm which learns low rank covariance factors, and utilizes an energy-based inference to achieve a calibrated prediction. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=GQd7mXSPua&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aD7uesX1GF_" data-number="2904">
        <h4>
          <a href="https://openreview.net/forum?id=aD7uesX1GF_">
              Conditional Object-Centric Learning from Video
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aD7uesX1GF_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Thomas_Kipf2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Kipf2">Thomas Kipf</a>, <a href="https://openreview.net/profile?id=~Gamaleldin_Fathy_Elsayed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gamaleldin_Fathy_Elsayed1">Gamaleldin Fathy Elsayed</a>, <a href="https://openreview.net/profile?id=~Aravindh_Mahendran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aravindh_Mahendran2">Aravindh Mahendran</a>, <a href="https://openreview.net/profile?id=~Austin_Stone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Austin_Stone1">Austin Stone</a>, <a href="https://openreview.net/profile?id=~Sara_Sabour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sara_Sabour1">Sara Sabour</a>, <a href="https://openreview.net/profile?id=~Georg_Heigold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georg_Heigold1">Georg Heigold</a>, <a href="https://openreview.net/profile?id=~Rico_Jonschkowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rico_Jonschkowski1">Rico Jonschkowski</a>, <a href="https://openreview.net/profile?id=~Alexey_Dosovitskiy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexey_Dosovitskiy1">Alexey Dosovitskiy</a>, <a href="https://openreview.net/profile?id=~Klaus_Greff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Klaus_Greff1">Klaus Greff</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aD7uesX1GF_-details-505" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aD7uesX1GF_-details-505"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=aD7uesX1GF_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="f2OYVDyfIB" data-number="2891">
        <h4>
          <a href="https://openreview.net/forum?id=f2OYVDyfIB">
              Scale Efficiently: Insights from Pretraining and Finetuning Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=f2OYVDyfIB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yi_Tay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Tay1">Yi Tay</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Jinfeng_Rao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinfeng_Rao1">Jinfeng Rao</a>, <a href="https://openreview.net/profile?id=~William_Fedus2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Fedus2">William Fedus</a>, <a href="https://openreview.net/profile?id=~Samira_Abnar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samira_Abnar1">Samira Abnar</a>, <a href="https://openreview.net/profile?id=~Hyung_Won_Chung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyung_Won_Chung1">Hyung Won Chung</a>, <a href="https://openreview.net/profile?id=~Sharan_Narang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sharan_Narang1">Sharan Narang</a>, <a href="https://openreview.net/profile?id=~Dani_Yogatama2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dani_Yogatama2">Dani Yogatama</a>, <a href="https://openreview.net/profile?id=~Ashish_Vaswani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashish_Vaswani1">Ashish Vaswani</a>, <a href="https://openreview.net/profile?id=~Donald_Metzler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Donald_Metzler1">Donald Metzler</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#f2OYVDyfIB-details-321" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="f2OYVDyfIB-details-321"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformers, attention, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Scaling laws for upstream and downstream tasks </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ow1C7s3UcY" data-number="2889">
        <h4>
          <a href="https://openreview.net/forum?id=Ow1C7s3UcY">
              Vitruvion: A Generative Model of Parametric CAD Sketches
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ow1C7s3UcY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ari_Seff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ari_Seff1">Ari Seff</a>, <a href="https://openreview.net/profile?id=~Wenda_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenda_Zhou1">Wenda Zhou</a>, <a href="https://openreview.net/profile?id=~Nick_Richardson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nick_Richardson1">Nick Richardson</a>, <a href="https://openreview.net/profile?id=~Ryan_P_Adams1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryan_P_Adams1">Ryan P Adams</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ow1C7s3UcY-details-794" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ow1C7s3UcY-details-794"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generative modeling, CAD, transformers, design, geometric constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We build a generative model for parametric CAD sketches and use it to perform autocompletion and hand drawing conversion tasks relevant to design.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XJiajt89Omg" data-number="2882">
        <h4>
          <a href="https://openreview.net/forum?id=XJiajt89Omg">
              Space-Time Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XJiajt89Omg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Samar_Hadou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samar_Hadou1">Samar Hadou</a>, <a href="https://openreview.net/profile?email=kanac%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kanac@seas.upenn.edu">Charilaos I Kanatsoulis</a>, <a href="https://openreview.net/profile?id=~Alejandro_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alejandro_Ribeiro1">Alejandro Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XJiajt89Omg-details-195" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XJiajt89Omg-details-195"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ST-GNNs, GNNs, stability, graph-time perturbations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce space-time graph neural network (ST-GNN), a novel GNN architecture, tailored to jointly process the underlying space-time topology of time-varying network data. The cornerstone of our proposed architecture is the composition of time and graph convolutional filters followed by pointwise nonlinear activation functions. We introduce a generic definition of convolution operators that mimic the diffusion process of signals over its underlying support. On top of this definition, we propose space-time graph convolutions that are built upon a composition of time and graph shift operators.  We prove that ST-GNNs with multivariate integral Lipschitz filters are stable to small perturbations in the underlying graphs as well as small perturbations in the time domain caused by time warping. Our analysis shows that small variations in the network topology and time evolution of a system does not significantly affect the performance of ST-GNNs. Numerical experiments with decentralized control systems showcase the effectiveness and stability of the proposed ST-GNNs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce space-time graph neural network (ST-GNN) tailored to jointly process the underlying space-time topology of time-varying network data, and we show its stability to perturbations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bjy5Zb2fo2" data-number="2879">
        <h4>
          <a href="https://openreview.net/forum?id=bjy5Zb2fo2">
              Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bjy5Zb2fo2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jason_McEwen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_McEwen1">Jason McEwen</a>, <a href="https://openreview.net/profile?email=christophergrwallis%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="christophergrwallis@gmail.com">Christopher Wallis</a>, <a href="https://openreview.net/profile?id=~Augustine_N._Mavor-Parker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Augustine_N._Mavor-Parker1">Augustine N. Mavor-Parker</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bjy5Zb2fo2-details-796" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bjy5Zb2fo2-details-796"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Convolutional neural networks (CNNs) constructed natively on the sphere have been developed recently and shown to be highly effective for the analysis of spherical data.  While an efficient framework has been formulated, spherical CNNs are nevertheless highly computationally demanding; typically they cannot scale beyond spherical signals of thousands of pixels.  We develop scattering networks constructed natively on the sphere that provide a powerful representational space for spherical data.  Spherical scattering networks are computationally scalable and exhibit rotational equivariance, while their representational space is invariant to isometries and provides efficient and stable signal representations.  By integrating scattering networks as an additional type of layer in the generalized spherical CNN framework, we show how they can be leveraged to scale spherical CNNs to the high-resolution data typical of many practical applications, with spherical signals of many tens of megapixels and beyond.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Scaling rotationally equivariant spherical CNNs to high-resolution data through spherical scattering networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xkjqJYqRJy" data-number="2867">
        <h4>
          <a href="https://openreview.net/forum?id=xkjqJYqRJy">
              Bayesian Neural Network Priors Revisited
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xkjqJYqRJy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vincent_Fortuin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vincent_Fortuin1">Vincent Fortuin</a>, <a href="https://openreview.net/profile?id=~Adri%C3%A0_Garriga-Alonso1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrià_Garriga-Alonso1">Adrià Garriga-Alonso</a>, <a href="https://openreview.net/profile?id=~Sebastian_W._Ober1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_W._Ober1">Sebastian W. Ober</a>, <a href="https://openreview.net/profile?id=~Florian_Wenzel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Wenzel1">Florian Wenzel</a>, <a href="https://openreview.net/profile?id=~Gunnar_Ratsch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunnar_Ratsch1">Gunnar Ratsch</a>, <a href="https://openreview.net/profile?id=~Richard_E_Turner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_E_Turner1">Richard E Turner</a>, <a href="https://openreview.net/profile?id=~Mark_van_der_Wilk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_van_der_Wilk1">Mark van der Wilk</a>, <a href="https://openreview.net/profile?id=~Laurence_Aitchison1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurence_Aitchison1">Laurence Aitchison</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xkjqJYqRJy-details-377" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xkjqJYqRJy-details-377"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian deep learning, Bayesian neural networks, Priors</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Using BNN priors that are not isotropic Gaussians can improve performance and reduce the cold posterior effect.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6NePxZwfae" data-number="2861">
        <h4>
          <a href="https://openreview.net/forum?id=6NePxZwfae">
              Goal-Directed Planning via Hindsight Experience Replay
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6NePxZwfae" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lorenzo_Moro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lorenzo_Moro1">Lorenzo Moro</a>, <a href="https://openreview.net/profile?id=~Amarildo_Likmeta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amarildo_Likmeta1">Amarildo Likmeta</a>, <a href="https://openreview.net/profile?id=~Enrico_Prati1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enrico_Prati1">Enrico Prati</a>, <a href="https://openreview.net/profile?id=~Marcello_Restelli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcello_Restelli1">Marcello Restelli</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6NePxZwfae-details-615" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6NePxZwfae-details-615"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Goal-Directed Planning, Monte Carlo Tree Search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. It has been extended from complex continuous domains through function approximators to bias the search of the planning tree in AlphaZero. Nonetheless, these algorithms still struggle with control problems with sparse rewards, such as goal-directed domains, where a positive reward is awarded only when reaching a goal state. In this work, we recast AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks. We perform a thorough empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper presents an extension of AlphaZero to tackle sparse reward goal-based tasks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EMigfE6ZeS" data-number="2859">
        <h4>
          <a href="https://openreview.net/forum?id=EMigfE6ZeS">
              Hybrid Random Features
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EMigfE6ZeS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Krzysztof_Marcin_Choromanski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krzysztof_Marcin_Choromanski1">Krzysztof Marcin Choromanski</a>, <a href="https://openreview.net/profile?id=~Han_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Lin1">Han Lin</a>, <a href="https://openreview.net/profile?id=~Haoxian_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoxian_Chen2">Haoxian Chen</a>, <a href="https://openreview.net/profile?id=~Arijit_Sehanobish1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arijit_Sehanobish1">Arijit Sehanobish</a>, <a href="https://openreview.net/profile?id=~Yuanzhe_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanzhe_Ma1">Yuanzhe Ma</a>, <a href="https://openreview.net/profile?id=~Deepali_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepali_Jain1">Deepali Jain</a>, <a href="https://openreview.net/profile?id=~Jake_Varley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jake_Varley1">Jake Varley</a>, <a href="https://openreview.net/profile?id=~Andy_Zeng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andy_Zeng1">Andy Zeng</a>, <a href="https://openreview.net/profile?id=~Michael_S_Ryoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_S_Ryoo1">Michael S Ryoo</a>, <a href="https://openreview.net/profile?id=~Valerii_Likhosherstov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valerii_Likhosherstov2">Valerii Likhosherstov</a>, <a href="https://openreview.net/profile?id=~Dmitry_Kalashnikov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Kalashnikov1">Dmitry Kalashnikov</a>, <a href="https://openreview.net/profile?id=~Vikas_Sindhwani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikas_Sindhwani1">Vikas Sindhwani</a>, <a href="https://openreview.net/profile?id=~Adrian_Weller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Weller1">Adrian Weller</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">31 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EMigfE6ZeS-details-317" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EMigfE6ZeS-details-317"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random features, softmax kernel, attention mechanism, compositional kernels</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi &amp; Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner’s Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts.  We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new class of random feature methods for softmax and Gaussian kernel estimation that are adaptable to provide particularly accurate approximation in the desired regions of interest.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=EMigfE6ZeS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="figzpGMrdD" data-number="2857">
        <h4>
          <a href="https://openreview.net/forum?id=figzpGMrdD">
              Pretrained Language Model in Continual Learning: A Comparative Study
          </a>
        
          
            <a href="https://openreview.net/pdf?id=figzpGMrdD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tongtong_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tongtong_Wu1">Tongtong Wu</a>, <a href="https://openreview.net/profile?id=~Massimo_Caccia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Massimo_Caccia1">Massimo Caccia</a>, <a href="https://openreview.net/profile?id=~Zhuang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuang_Li1">Zhuang Li</a>, <a href="https://openreview.net/profile?id=~Yuan-Fang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan-Fang_Li1">Yuan-Fang Li</a>, <a href="https://openreview.net/profile?email=gqi%40seu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="gqi@seu.edu.cn">Guilin Qi</a>, <a href="https://openreview.net/profile?id=~Gholamreza_Haffari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gholamreza_Haffari1">Gholamreza Haffari</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#figzpGMrdD-details-4" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="figzpGMrdD-details-4"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Continual Learning, Pre-trained Language Model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 veins of CL methods on 3 benchmarks in 2 typical incremental settings. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=figzpGMrdD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XVPqLyNxSyh" data-number="2856">
        <h4>
          <a href="https://openreview.net/forum?id=XVPqLyNxSyh">
              Salient ImageNet: How to discover spurious features in Deep Learning?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XVPqLyNxSyh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sahil_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sahil_Singla1">Sahil Singla</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XVPqLyNxSyh-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XVPqLyNxSyh-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">interpretability, failure explanation, debugging, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks can be unreliable in the real world especially when they heavily use {\it spurious} features for their predictions. Focusing on image classifications, we define {\it core features} as the set of visual features that are always a part of the object definition while {\it spurious features} are the ones that are likely to {\it co-occur} with the object but not a part of it (e.g., attribute ``fingers" for class ``band aid"). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a {\it general} framework to discover a subset of spurious and core visual features used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or core \textit{visual features} used in model predictions, we identify spurious or core \textit{neural features} (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations {\it generalize} extremely well to many more images {\it without} any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the {\it Salient Imagenet} dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, indicating the standard accuracy alone is not sufficient to fully assess model' performance specially in safety-critical applications. Code is available at \url{https://github.com/singlasahil14/salient_imagenet}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A scalable framework for discovering spurious features of deep neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XVPqLyNxSyh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="9wOQOgNe-w" data-number="2855">
        <h4>
          <a href="https://openreview.net/forum?id=9wOQOgNe-w">
              Differentiable DAG Sampling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=9wOQOgNe-w" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Bertrand_Charpentier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bertrand_Charpentier2">Bertrand Charpentier</a>, <a href="https://openreview.net/profile?id=~Simon_Kibler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Kibler1">Simon Kibler</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#9wOQOgNe-w-details-730" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9wOQOgNe-w-details-730"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DAG, Differentiable, Sampling, Probabilistic model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new differentiable probabilistic model over DAGs (DP-DAG). DP-DAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG learning from observational data which combines DP-DAG with variational inference. Hence,VI-DP-DAG approximates the posterior probability over DAG edges given the observed data. VI-DP-DAG is guaranteed to output a valid DAG at any time during training and does not require any complex augmented Lagrangian optimization scheme in contrast to existing differentiable DAG learning approaches. In our extensive experiments, we compare VI-DP-DAG to other differentiable DAG learning baselines on synthetic and real datasets. VI-DP-DAG significantly improves DAG structure and causal mechanism learning while training faster than competitors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SS8F6tFX3-" data-number="2851">
        <h4>
          <a href="https://openreview.net/forum?id=SS8F6tFX3-">
              Evaluating Model-Based Planning and Planner Amortization for Continuous Control
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SS8F6tFX3-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arunkumar_Byravan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arunkumar_Byravan1">Arunkumar Byravan</a>, <a href="https://openreview.net/profile?id=~Leonard_Hasenclever1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonard_Hasenclever1">Leonard Hasenclever</a>, <a href="https://openreview.net/profile?id=~Piotr_Trochim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piotr_Trochim1">Piotr Trochim</a>, <a href="https://openreview.net/profile?id=~Mehdi_Mirza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mehdi_Mirza1">Mehdi Mirza</a>, <a href="https://openreview.net/profile?id=~Alessandro_Davide_Ialongo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_Davide_Ialongo1">Alessandro Davide Ialongo</a>, <a href="https://openreview.net/profile?id=~Yuval_Tassa2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuval_Tassa2">Yuval Tassa</a>, <a href="https://openreview.net/profile?id=~Jost_Tobias_Springenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jost_Tobias_Springenberg1">Jost Tobias Springenberg</a>, <a href="https://openreview.net/profile?id=~Abbas_Abdolmaleki3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abbas_Abdolmaleki3">Abbas Abdolmaleki</a>, <a href="https://openreview.net/profile?id=~Nicolas_Heess1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Heess1">Nicolas Heess</a>, <a href="https://openreview.net/profile?id=~Josh_Merel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Josh_Merel1">Josh Merel</a>, <a href="https://openreview.net/profile?id=~Martin_Riedmiller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Riedmiller1">Martin Riedmiller</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SS8F6tFX3--details-840" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SS8F6tFX3--details-840"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Model-based Reinforcement Learning, Planning, Robotics, Model Predictive Control, Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There is a widespread intuition that model-based control methods should be able to surpass the data efficiency of model-free approaches. In this paper we attempt to evaluate this intuition on various challenging locomotion tasks. We take a hybrid approach, combining model predictive control (MPC) with a learned model and model-free policy learning; the learned policy serves as a proposal for MPC. We show that MPC with learned proposals and models (trained on the fly or transferred from related tasks) can significantly improve performance and data efficiency with respect to model-free methods. However, we find that well-tuned model-free agents are strong baselines even for high DoF control problems. Finally, we show that it is possible to distil a model-based planner into a policy that amortizes the planning computation without any loss of performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We combine MPC with model-free RL and evaluate on continuous control tasks from scratch and in transfer settings; our results show that model-free RL is a strong baseline in single task settings and model-based methods shine in multi-goal tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=SS8F6tFX3-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xKZ4K0lTj_" data-number="2845">
        <h4>
          <a href="https://openreview.net/forum?id=xKZ4K0lTj_">
              Hierarchical Few-Shot Imitation with Skill Transition Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xKZ4K0lTj_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kourosh_Hakhamaneshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kourosh_Hakhamaneshi1">Kourosh Hakhamaneshi</a>, <a href="https://openreview.net/profile?id=~Ruihan_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruihan_Zhao1">Ruihan Zhao</a>, <a href="https://openreview.net/profile?id=~Albert_Zhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Zhan1">Albert Zhan</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Laskin1">Michael Laskin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xKZ4K0lTj_-details-540" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xKZ4K0lTj_-details-540"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">behavioral priors, skill extraction, imitation learning, few-shot learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new algorithm (FIST) which extracts skills from offline data and adapts them in few-shot to solve unseen complex long-horizon tasks by utilizing an inverse skill dynamics model and semi-parametric imitation. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xKZ4K0lTj_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="g2LCQwG7Of" data-number="2835">
        <h4>
          <a href="https://openreview.net/forum?id=g2LCQwG7Of">
              End-to-End Learning of Probabilistic Hierarchies on Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=g2LCQwG7Of" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daniel_Z%C3%BCgner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Zügner1">Daniel Zügner</a>, <a href="https://openreview.net/profile?id=~Bertrand_Charpentier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bertrand_Charpentier2">Bertrand Charpentier</a>, <a href="https://openreview.net/profile?id=~Morgane_Ayle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Morgane_Ayle1">Morgane Ayle</a>, <a href="https://openreview.net/profile?id=~Sascha_Geringer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sascha_Geringer1">Sascha Geringer</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#g2LCQwG7Of-details-330" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g2LCQwG7Of-details-330"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">hierarchical clustering, graphs, networks, graph mining, network mining, graph custering</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a novel probabilistic model over hierarchies on graphs obtained by continuous relaxation of tree-based hierarchies. We draw connections to Markov chain theory, enabling us to perform hierarchical clustering by efficient end-to-end optimization of relaxed versions of quality metrics such as Dasgupta cost or Tree-Sampling Divergence (TSD). 
        We show that our model learns rich, high-quality hierarchies present in 11 real world graphs, including a large  graph with 2.3M nodes. Our model consistently outperforms recent as well as strong traditional baselines such as average linkage. 
        Our model also obtains strong results on link prediction despite not being trained on this task, highlighting the quality of the hierarchies discovered by our model.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">End-to-end gradient-based hierarchical clustering on graphs by exploiting Markov chain theory leads to state-of-the-art results.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=g2LCQwG7Of&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>





<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class=" active " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">›</a>
      </li>
      <li class="  right-arrow" data-page-number="18">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">»</a>
      </li>
  </ul>
</nav>





</div>
    <div role="tabpanel" class="tab-pane fade  " id="submitted-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="youe3QQepVB" data-number="4714">
        <h4>
          <a href="https://openreview.net/forum?id=youe3QQepVB">
              Generative Modeling for Multitask Visual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=youe3QQepVB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhipeng_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhipeng_Bao1">Zhipeng Bao</a>, <a href="https://openreview.net/profile?id=~Yu-Xiong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Xiong_Wang1">Yu-Xiong Wang</a>, <a href="https://openreview.net/profile?id=~Martial_Hebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martial_Hebert1">Martial Hebert</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#youe3QQepVB-details-679" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="youe3QQepVB-details-679"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, especially in the low-data regimes, and our model consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a general multi-task oriented generative modeling (MGM) framework that introduces generative models to facilitate multi-task learning and it consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2aC0_RxkBL_" data-number="4707">
        <h4>
          <a href="https://openreview.net/forum?id=2aC0_RxkBL_">
              Where is the bottleneck in long-tailed classification?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2aC0_RxkBL_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaid_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zaid_Khan1">Zaid Khan</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yun_Fu1">Yun Fu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2aC0_RxkBL_-details-584" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2aC0_RxkBL_-details-584"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, bias, long tailed learning, imbalanced learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A commonly held belief in deep-learning based long-tailed classiﬁcation is that the representations learned from long-tailed data are ”good enough” and the performance bottleneck is the classiﬁcation head atop the representation learner. We design experiments to investigate this folk wisdom, and ﬁnd that representations learned from long-tailed data distributions substantially differ from the representations learned from ”normal” data distributions. We show that the long-tailed representations are volatile and brittle with respect to the true data distribution. Compared to the representations learned from the true, balanced distributions, long-tailed representations fail to localize tail classes and display vastly worse inter-class separation and intra-class compactness when unseen samples from the true data distribution are embedded into the feature space. We provide an explanation for why data augmentation helps long-tailed classiﬁcation despite leaving the dataset imbalance unchanged — it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate how learning from long-tailed distributions harms representations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="voEpzgY8gsT" data-number="4704">
        <h4>
          <a href="https://openreview.net/forum?id=voEpzgY8gsT">
              Additive Poisson Process: Learning Intensity of Higher-Order Interaction in Poisson Processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=voEpzgY8gsT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Simon_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Luo1">Simon Luo</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Zhou9">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~lamiae_azizi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~lamiae_azizi1">lamiae azizi</a>, <a href="https://openreview.net/profile?id=~Mahito_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mahito_Sugiyama1">Mahito Sugiyama</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#voEpzgY8gsT-details-110" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="voEpzgY8gsT-details-110"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Poisson Process, Log-Linear Model, Energy-Based Model, Generalized Additive Models, Information Geometry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Additive Poisson Process (APP), a novel framework that can model the higher-order interaction effects of the intensity functions in Poisson processes using projections into lower-dimensional space. Our model combines the techniques in information geometry to model higher-order interactions on a statistical manifold and in generalized additive models to use lower-dimensional projections to overcome the effects from the curse of dimensionality. Our approach solves a convex optimization problem by minimizing the KL divergence from a sample distribution in lower-dimensional projections to the distribution modeled by an intensity function in the Poisson process. Our empirical results show that our model is able to use samples observed in the lower dimensional space to estimate the higher-order intensity function with extremely sparse observations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An efficient technique that uses a log-linear model on a partial order structure to approximate a high-dimensional intensity functions in a Poisson Process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=voEpzgY8gsT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qfLJBJf_DnH" data-number="4701">
        <h4>
          <a href="https://openreview.net/forum?id=qfLJBJf_DnH">
              Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qfLJBJf_DnH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Laureline_Logiaco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Laureline_Logiaco1">Laureline Logiaco</a>, <a href="https://openreview.net/profile?id=~G_Sean_Escola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~G_Sean_Escola1">G Sean Escola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qfLJBJf_DnH-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qfLJBJf_DnH-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuroscience, dynamical systems, thalamocortical architecture, motor preparation, continual learning, hierarchical continuous motor control, out-of-distribution generalization, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of learning dynamics that can produce hierarchically organized continuous outputs consisting of the flexible chaining of re-usable motor ‘motifs’ from which complex behavior is generated. Can a motif library be efficiently and extendably learned without interference between motifs, and can these motifs be chained in arbitrary orders without first learning the corresponding motif transitions during training? This requires (i) parameter updates while learning a new motif that do not interfere with the parameters used for the previously acquired ones; and (ii) successful motif generation when starting from the network states reached at the end of any of the other motifs, even if these states were not present during training (a case of out-of-distribution generalization). We meet the first requirement by designing recurrent neural networks (RNNs) with specific architectures that segregate motif-dependent parameters (as customary in continual learning works), and try a standard method to address the second by training with random initial states. We find that these standard RNNs are very unreliable during zero-shot transfer to motif chaining. We then use insights from the motor thalamocortical circuit, featuring a specific module that shapes motif transitions. We develop a method to constrain the RNNs to function similarly to the thalamocortical circuit during motif transitions, while preserving the large expressivity afforded by gradient-based training of non-analytically tractable RNNs. We then show that this thalamocortical inductive bias not only acts in synergy with gradient-descent RNN training to improve accuracy during in-training-distribution motif production, but also leads to zero-shot transfer to new motif chains with no performance cost. Besides proposing an efficient, robust and flexible RNN architecture, our results shed new light on the function of motor preparation in the brain.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Motor preparation in nonlinear RNNs supports robust chaining of accurate continuous motor motifs in never-experienced orders.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rF5UoZFrsF4" data-number="4697">
        <h4>
          <a href="https://openreview.net/forum?id=rF5UoZFrsF4">
              VUT: Versatile UI Transformer for Multimodal Multi-Task User Interface Modeling 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rF5UoZFrsF4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Li2">Yang Li</a>, <a href="https://openreview.net/profile?id=~Gang_Li13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Li13">Gang Li</a>, <a href="https://openreview.net/profile?id=~Xin_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Zhou3">Xin Zhou</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rF5UoZFrsF4-details-15" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rF5UoZFrsF4-details-15"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">User Interface Modeling, Multimodal input, Multi-task learning, Transformer, Layout Detection, Language Grounding, Image Captioning, Screen Summarization, Tappability Prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT has achieved accuracy either on par with or exceeding the accuracy when the model is trained for individual tasks separately.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The work addresses unique challenges of multimodal multi-task learning of distinct tasks for user interface modeling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3M3t3tUbA2Y" data-number="4686">
        <h4>
          <a href="https://openreview.net/forum?id=3M3t3tUbA2Y">
              DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3M3t3tUbA2Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fei_Deng1">Fei Deng</a>, <a href="https://openreview.net/profile?id=~Ingook_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingook_Jang1">Ingook Jang</a>, <a href="https://openreview.net/profile?id=~Sungjin_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungjin_Ahn1">Sungjin Ahn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3M3t3tUbA2Y-details-757" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3M3t3tUbA2Y-details-757"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction
        often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions. To address this issue, previous work has proposed to contrastively learn the latent representations and its temporal dynamics, but showed inconsistent performance, often worse than Dreamer. Although, in computer vision, an alternative prototypical approach has often shown to be more accurate and robust, it is elusive how this approach can be combined best with the temporal dynamics learning in MBRL. In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal. Similar to SwAV, by encouraging uniform cluster assignment across the batch, we implicitly push apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details. We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model is consistently better than the previous models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dy8gq-LuckD" data-number="4680">
        <h4>
          <a href="https://openreview.net/forum?id=Dy8gq-LuckD">
              Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dy8gq-LuckD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Wu1">Nan Wu</a>, <a href="https://openreview.net/profile?id=~Stanislaw_Kamil_Jastrzebski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stanislaw_Kamil_Jastrzebski1">Stanislaw Kamil Jastrzebski</a>, <a href="https://openreview.net/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyunghyun_Cho1">Kyunghyun Cho</a>, <a href="https://openreview.net/profile?id=~Krzysztof_J._Geras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krzysztof_J._Geras1">Krzysztof J. Geras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dy8gq-LuckD-details-620" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dy8gq-LuckD-details-620"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-modal learning, deep neural networks, multi-view learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities. We observe empirically that such behavior hurts its overall generalization. We validate our hypothesis by estimating the gain on the accuracy when the model has access to an additional modality. We refer to this gain as the conditional utilization rate of the modality. In the experiments, we consistently observe an imbalance in conditional utilization rate between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. We thus propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the model’s generalization on three datasets: Colored MNIST (Kim et al., 2019), Princeton ModelNet40 (Wu et al., 2015), and NVIDIA Dynamic Hand Gesture Dataset (Molchanov et al., 2016).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UeE41VsK1KJ" data-number="4677">
        <h4>
          <a href="https://openreview.net/forum?id=UeE41VsK1KJ">
              Subjective Learning for Open-Ended Data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UeE41VsK1KJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tianren_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianren_Zhang1">Tianren Zhang</a>, <a href="https://openreview.net/profile?id=~Yizhou_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yizhou_Jiang1">Yizhou Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Su1">Xin Su</a>, <a href="https://openreview.net/profile?id=~Shangqi_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shangqi_Guo2">Shangqi Guo</a>, <a href="https://openreview.net/profile?id=~Feng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Chen1">Feng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UeE41VsK1KJ-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UeE41VsK1KJ-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Open-ended data, machine learning, supervised learning, data conflict</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional supervised learning typically assumes that the learning task can be solved by learning a single function since the data is sampled from a fixed distribution. However, this assumption is invalid in open-ended environments where no task-level data partitioning is available. In this paper, we present a novel supervised learning framework of learning from open-ended data, which is modeled as data implicitly sampled from multiple domains with the data in each domain obeying a domain-specific target function. Since different domains may possess distinct target functions, open-ended data inherently requires multiple functions to capture all its input-output relations, rendering training a single global model problematic. To address this issue, we devise an Open-ended Supervised Learning (OSL) framework, of which the key component is a subjective function that allocates the data among multiple candidate models to resolve the "conflict'' between the data from different domains, exhibiting a natural hierarchy. We theoretically analyze the learnability and the generalization error of OSL, and empirically validate its efficacy in both open-ended regression and classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We formalize the problem of learning from open-ended data that implicitly comes from multiple domains and inherently requires multiple functions to fully capture its input-output relations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UeE41VsK1KJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2RNpZ8S4alJ" data-number="4675">
        <h4>
          <a href="https://openreview.net/forum?id=2RNpZ8S4alJ">
              KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2RNpZ8S4alJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alireza_Rezazadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Rezazadeh1">Alireza Rezazadeh</a>, <a href="https://openreview.net/profile?email=cchoi%40umn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="cchoi@umn.edu">Changhyun Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2RNpZ8S4alJ-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2RNpZ8S4alJ-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object-centric representation is an essential abstraction for physical reasoning and forward prediction. Most existing approaches learn this representation through extensive supervision (e.g, object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network)---an end-to-end unsupervised framework to reason about object interactions in complex systems based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, and novel object geometries. Experiments demonstrate the effectiveness of our model to accurately perform forward prediction and learn plannable object-centric representations which can also be used in downstream model-based control tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="an_ndI09oVZ" data-number="4667">
        <h4>
          <a href="https://openreview.net/forum?id=an_ndI09oVZ">
              Deep banach space kernels
          </a>
        
          
            <a href="https://openreview.net/pdf?id=an_ndI09oVZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mrityunjay_Bhardwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mrityunjay_Bhardwaj1">Mrityunjay Bhardwaj</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#an_ndI09oVZ-details-653" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="an_ndI09oVZ-details-653"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RKBS, RKHS, concatenated kernel learning, representation learning, deep learning, MLMKL, Deep Gaussian Processes, gaussian processes, kernel machines</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recent success of deep learning has encouraged many researchers to explore the deep/concatenated variants of classical kernel methods. Some of which includes MLMKL, DGP and DKL. Although, These methods have proven to be quite useful in various real-world settings. They still suffer from the limitations of only utilizing kernels from Hilbert spaces. In this paper, we address these shortcomings by introducing a new class of concatenated kernel learning methods that use the kernels from the reproducing kernel Banach spaces(RKBSs). These spaces turned out to be one of the most general spaces where a reproducing Kernel exists. We propose a framework of construction for these Deep RKBS models and then provide a representer theorem for regularized learning problems. We also describe the relationship with its deep RKHS variant as well as standard Deep Gaussian Processes. In the end, we construct and implement a two-layer deep RKBS model and demonstrate it on a range of machine learning tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">a new class of deep kernel methods which uses kernels from reproducing kernel banach spaces (RKBS).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0Kj5mhn6sw" data-number="4650">
        <h4>
          <a href="https://openreview.net/forum?id=0Kj5mhn6sw">
              Gesture2Vec: Clustering Gestures using  Representation Learning Methods for Co-speech Gesture Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0Kj5mhn6sw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Payam_Jome_Yazdian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_Jome_Yazdian1">Payam Jome Yazdian</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?email=angelica%40sfu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="angelica@sfu.ca">Angelica Lim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0Kj5mhn6sw-details-898" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0Kj5mhn6sw-details-898"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation learning, gesture generation, vector quantization, machine translation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans. Similarly, the co-speech gesture is a key ingredient in human-agent interaction including both virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. Furthermore, we use a machine translation model that takes input text and translates it into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated quantized gestures from the input text as an input to the autoencoder’s decoder to produce gesture sequences. The resulting gestures can be applied to both virtual agents and humanoid robots. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0Kj5mhn6sw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WXy4C-RjET" data-number="4644">
        <h4>
          <a href="https://openreview.net/forum?id=WXy4C-RjET">
              Logit Attenuating Weight Normalization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WXy4C-RjET" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aman_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aman_Gupta1">Aman Gupta</a>, <a href="https://openreview.net/profile?id=~Rohan_Ramanath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Ramanath1">Rohan Ramanath</a>, <a href="https://openreview.net/profile?id=~Jun_Shi4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jun_Shi4">Jun Shi</a>, <a href="https://openreview.net/profile?id=~Anika_Ramachandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anika_Ramachandran1">Anika Ramachandran</a>, <a href="https://openreview.net/profile?id=~SIROU_ZHU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SIROU_ZHU1">SIROU ZHU</a>, <a href="https://openreview.net/profile?id=~Mingzhou_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mingzhou_Zhou1">Mingzhou Zhou</a>, <a href="https://openreview.net/profile?id=~Sathiya_Keerthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sathiya_Keerthi1">Sathiya Keerthi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WXy4C-RjET-details-185" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WXy4C-RjET-details-185"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning, gradient methods, stochastic optimization, generalization gap, imagenet, adam, large batch training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over-parameterized deep networks trained using gradient-based optimizers is a popular way of solving classification and ranking problems. Without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that, even with weight decay (WD) and normal hyper-parameter tuning, adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain.
        
        An alternative to WD for improving a network's adaptivity is to directly control the magnitude of the weights and hence the logits. We propose a method called Logit Attenuating Weight Normalization (LAWN), that can be stacked onto any gradient-based optimizer.  LAWN initially starts off training in a free (unregularized) mode and, after some initial epochs, it constrains the weight norms of layers, thereby controlling the logits and improving adaptivity. This is a new regularization approach that does not use WD anywhere; instead, the number of initial free epochs becomes the new hyper-parameter. The resulting LAWN variant of adaptive optimizers gives a solid lift to generalization performance, making their performance equal or even exceed SGD's performance on benchmark image classification and recommender datasets. Another important feature is that LAWN also greatly improves the adaptive optimizers when used with large batch sizes.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An optimizer for deep learning called Logit Attenuating Weight Normalization (LAWN) for superior generalization performance and scaling to large batches</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WXy4C-RjET&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxBH9j8IE_H" data-number="4643">
        <h4>
          <a href="https://openreview.net/forum?id=UxBH9j8IE_H">
              Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxBH9j8IE_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~BITHIKA_PAL1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BITHIKA_PAL1">BITHIKA PAL</a>, <a href="https://openreview.net/profile?id=~Arindam_Biswas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arindam_Biswas1">Arindam Biswas</a>, <a href="https://openreview.net/profile?id=~Pabitra_Mitra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pabitra_Mitra1">Pabitra Mitra</a>, <a href="https://openreview.net/profile?id=~BISWAJIT_BASU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BISWAJIT_BASU1">BISWAJIT BASU</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxBH9j8IE_H-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxBH9j8IE_H-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Neural Networks, Network Pruning, Ramanujan Graphs, Eigenvalue bounds, Spectral Gap</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A Ramanujan graph perspective to explain the lottery ticket hypothesis</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a0SRWViFYW" data-number="4642">
        <h4>
          <a href="https://openreview.net/forum?id=a0SRWViFYW">
              Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a0SRWViFYW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Patrick_R._Johnstone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_R._Johnstone1">Patrick R. Johnstone</a>, <a href="https://openreview.net/profile?id=~Jonathan_Eckstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Eckstein1">Jonathan Eckstein</a>, <a href="https://openreview.net/profile?id=~Thomas_Flynn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Flynn1">Thomas Flynn</a>, <a href="https://openreview.net/profile?id=~Shinjae_Yoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinjae_Yoo1">Shinjae Yoo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a0SRWViFYW-details-897" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a0SRWViFYW-details-897"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">convex optimization, min-max games, saddle-point problems, first-order stochastic methods, proximal methods, operator splitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new, stochastic variant of the projective splitting (PS) family of algorithms for monotone inclusion problems.  It can solve min-max and noncooperative game formulations arising in applications such as robust ML without the convergence issues associated with gradient descent-ascent, the current de facto standard approach in ML applications.  Our proposal is the first version of PS able to use stochastic gradient oracles. It can solve min-max games while handling multiple constraints and nonsmooth regularizers via projection and proximal operators. Unlike other stochastic splitting methods that can solve such problems, our method does not rely on a product-space reformulation of the original problem. We prove almost-sure convergence of the iterates to the solution and a convergence rate for the expected residual.  By working with monotone inclusions rather than variational inequalities, our analysis avoids the drawbacks of measuring convergence through the restricted gap function. We close with numerical experiments on a distributionally robust sparse logistic regression problem.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a stochastic splitting method that can easily handle min-max problems with multiple regularizers and constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=a0SRWViFYW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_K6rwRjW9WO" data-number="4637">
        <h4>
          <a href="https://openreview.net/forum?id=_K6rwRjW9WO">
              RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_K6rwRjW9WO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Victor_Quintas-Martinez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Quintas-Martinez1">Victor Quintas-Martinez</a>, <a href="https://openreview.net/profile?id=~Victor_Chernozhukov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Chernozhukov1">Victor Chernozhukov</a>, <a href="https://openreview.net/profile?id=~Vasilis_Syrgkanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vasilis_Syrgkanis1">Vasilis Syrgkanis</a>, <a href="https://openreview.net/profile?id=~Whitney_Newey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Whitney_Newey1">Whitney Newey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_K6rwRjW9WO-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_K6rwRjW9WO-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="75" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.281em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></mjx-assistive-mml></mjx-container>-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, that is derived based on a functional-specific theoretical derivation of what is known as the influence function and which leads to properties such as double robustness and Neyman orthogonality. We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method solely requires value query oracle access to the linear functional. We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Reisz representer and regression loss, while sharing representation layers for the two functions. We also propose a random forest method which learns a locally linear representation of the Reisz function. Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We implement an automatic debiasing procedure for causal and policy effects based on automatically learning their corresponding Riesz representation, using Neural Nets and Random Forests.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_K6rwRjW9WO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Mo9R9oqzPo" data-number="4636">
        <h4>
          <a href="https://openreview.net/forum?id=Mo9R9oqzPo">
              New Definitions and Evaluations for Saliency Methods: Staying Intrinsic and Sound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Mo9R9oqzPo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arushi_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arushi_Gupta1">Arushi Gupta</a>, <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Dingli_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dingli_Yu1">Dingli Yu</a>, <a href="https://openreview.net/profile?id=~Kaifeng_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaifeng_Lyu2">Kaifeng Lyu</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sanjeev_Arora1">Sanjeev Arora</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Mo9R9oqzPo-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mo9R9oqzPo-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">saliency, masking based methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Saliency methods seek to provide human-interpretable explanations for the output of machine learning model on a given input. A plethora of saliency methods exist, as well as an extensive literature on their justifications/criticisms/evaluations. This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans. It tries to introduce methods and evaluations for masked-based saliency methods that are {\em intrinsic} --- use just the training dataset and the trained net, and do not use separately trained nets, distractor distributions, human evaluations or annotations. Since a mask can be seen as a "certificate" justifying the net's answer, we introduce notions of {\em completeness} and {\em soundness} (the latter being the new contribution) motivated by logical proof systems. These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.V. regularization, upscaling). </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mo9R9oqzPo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GthNKCqdDg" data-number="4623">
        <h4>
          <a href="https://openreview.net/forum?id=GthNKCqdDg">
              Selective Token Generation for Few-shot Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GthNKCqdDg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daejin_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daejin_Jo1">Daejin Jo</a>, <a href="https://openreview.net/profile?id=~Taehwan_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Taehwan_Kwon1">Taehwan Kwon</a>, <a href="https://openreview.net/profile?id=~Sungwoong_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungwoong_Kim2">Sungwoong Kim</a>, <a href="https://openreview.net/profile?id=~Eun-Sol_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eun-Sol_Kim1">Eun-Sol Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GthNKCqdDg-details-571" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GthNKCqdDg-details-571"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Generation, Reinforcement Learning, Few-shot Learning, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among these transfer learning algorithms from PLMs, additive learning that incorporates a task-specific adapter on top of the fixed PLM has been popularly used to alleviate the severe overfitting problem in the few-shot setting. However, this added task-specific adapter is generally trained by maximum likelihood estimation that can easily suffer from the so-called exposure bias problem, especially in sequential text generation. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) for few-shot natural language generation (NLG) tasks. In particular, we propose to use a selective token generation between the transformer-based PLM and the task-specific adapter during both training and inference. This output token selection between the two generators allows the adapter to take into account only on the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, in order to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="INO8hGXD2M" data-number="4614">
        <h4>
          <a href="https://openreview.net/forum?id=INO8hGXD2M">
              Adversarial Distributions Against Out-of-Distribution Detectors
          </a>
        
          
            <a href="https://openreview.net/pdf?id=INO8hGXD2M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sangwoong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sangwoong_Yoon1">Sangwoong Yoon</a>, <a href="https://openreview.net/profile?id=~Jinwon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinwon_Choi1">Jinwon Choi</a>, <a href="https://openreview.net/profile?id=~Yonghyeon_LEE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonghyeon_LEE1">Yonghyeon LEE</a>, <a href="https://openreview.net/profile?id=~Yung-Kyun_Noh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yung-Kyun_Noh1">Yung-Kyun Noh</a>, <a href="https://openreview.net/profile?id=~Frank_C._Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_C._Park1">Frank C. Park</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#INO8hGXD2M-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="INO8hGXD2M-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">out-of-distribution detection, outlier detection, adversarial attack, model evaluation, markov chain monte carlo</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution. As an outlier may deviate from the training distribution in unexpected ways, an ideal OOD detector should be able to detect all types of outliers. However, current evaluation protocols test a detector over OOD datasets that cover only a small fraction of all possible outliers, leading to overly optimistic views of OOD detector performance.  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers.  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector. Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel evaluation method for out-of-distribution detectors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eOdSD0B5TE" data-number="4606">
        <h4>
          <a href="https://openreview.net/forum?id=eOdSD0B5TE">
              On the Implicit Biases of Architecture &amp; Gradient Descent
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eOdSD0B5TE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jeremy_Bernstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeremy_Bernstein1">Jeremy Bernstein</a>, <a href="https://openreview.net/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisong_Yue1">Yisong Yue</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eOdSD0B5TE-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eOdSD0B5TE-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalisation, function space, PAC-Bayes, NNGP, orthants, margin</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Do neural networks generalise because of bias in the functions returned by gradient descent, or bias already present in the network architecture? <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="76" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">¿</mjx-utext><mjx-c class="mjx-c1D443 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45E TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">é</mjx-utext><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c3F TEX-MI"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">¿Por qué no los dos?</mtext></math></mjx-assistive-mml></mjx-container> This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent. To measure the implicit bias of architecture, new technical tools are developed to both <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="77" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">analytically bound</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="78" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">consistently estimate</mtext></math></mjx-assistive-mml></mjx-container> the average test error of the neural network--Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-Pérez et al. (2019) and underscoring the importance of architecture. Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior. This highlights a curious fact: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="79" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">minimum a posteriori</mtext></math></mjx-assistive-mml></mjx-container> functions can generalise best, and gradient descent can select for those functions. In summary, new technical tools suggest a nuanced portrait of generalisation involving both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">New technical tools suggest a nuanced portrait of generalisation that involves both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=eOdSD0B5TE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vPK-G5HbnWg" data-number="4603">
        <h4>
          <a href="https://openreview.net/forum?id=vPK-G5HbnWg">
              PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vPK-G5HbnWg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zehao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zehao_Dong1">Zehao Dong</a>, <a href="https://openreview.net/profile?id=~Muhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhan_Zhang1">Muhan Zhang</a>, <a href="https://openreview.net/profile?id=~Fuhai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuhai_Li1">Fuhai Li</a>, <a href="https://openreview.net/profile?id=~Yixin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yixin_Chen1">Yixin Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vPK-G5HbnWg-details-659" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vPK-G5HbnWg-details-659"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DAG encoder, graph neural network, Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optimization of directed acyclic graph (DAG) structures has many applications, such as neural architecture search (NAS) and probabilistic graphical model learning. Encoding DAGs into real vectors is a dominant component in most neural-network-based DAG optimization frameworks. Currently, most popular DAG encoders use an asynchronous message passing scheme which sequentially processes nodes according to the dependency between nodes in a DAG. That is, a node must not be processed until all its predecessors are processed. As a result, they are inherently not parallelizable. In this work, we propose a Parallelizable Attention-based Computation structure Encoder (PACE) that processes nodes simultaneously and encodes DAGs in parallel. We demonstrate the superiority of PACE through  encoder-dependent optimization subroutines that search the optimal DAG structure based on the learned DAG embeddings. Experiments show that PACE not only improves the effectiveness over previous sequential DAG encoders with a significantly boosted training and inference speed, but also generates smooth latent (DAG encoding) spaces that are beneficial to downstream optimization subroutines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper introduces a novel DAG encoder based on Transformer to encode the computation structure defined by DAGs in a fully parallelizable manner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vPK-G5HbnWg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hEiwVblq4P" data-number="4602">
        <h4>
          <a href="https://openreview.net/forum?id=hEiwVblq4P">
              Proper Straight-Through Estimator: Breaking symmetry promotes convergence to true minimum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hEiwVblq4P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shinya_Gongyo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinya_Gongyo1">Shinya Gongyo</a>, <a href="https://openreview.net/profile?id=~Kohta_Ishikawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kohta_Ishikawa1">Kohta Ishikawa</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hEiwVblq4P-details-792" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hEiwVblq4P-details-792"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">quantization, binary network, low bit network, Straight through estimator, STE</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the quantized network, its gradient shows either vanishing or diverging. The network thus cannot be learned by the standard back-propagation, so that an alternative approach called Straight Through Estimator (STE), which replaces the part of the gradient with a simple differentiable function, is used. While STE is known to work well for learning the quantized network empirically, it has not been established theoretically. A recent study by Yin et. al. (2019) has provided theoretical support for STE. However, its justification is still limited to the model in the one-hidden layer network with the binary activation where  Gaussian generates the input data, and the true labels are output from the teacher network with the same binary network architecture. In this paper, we discuss the effectiveness of STEs in more general situations without assuming the shape of the input distribution and the labels. By considering the scale symmetry of the network and specific properties of the STEs, we find that STE with clipped Relu is superior to STEs with identity function and vanilla Relu. The clipped Relu STE, which breaks the scale symmetry, may pick up one of the local minima degenerated in scales, while the identity STE and vanilla Relu STE, which keep the scale symmetry, may not pick it up. To confirm this observation, we further present an analysis of a simple misspecified model as an example. We find that all the stationary points are identical with the vanishing points of the cRelu STE gradient, while some of them are not identical with the vanishing points of the identity and Relu STE.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We discuss breaking symmetry embedded in the network by Straight through estimators enhances the possibility of convergence to the true minimum.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="W6BpshgRi0q" data-number="4599">
        <h4>
          <a href="https://openreview.net/forum?id=W6BpshgRi0q">
              Ask2Mask: Guided Data Selection for Masked Speech Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=W6BpshgRi0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Murali_Karthick_Baskar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Murali_Karthick_Baskar1">Murali Karthick Baskar</a>, <a href="https://openreview.net/profile?id=~Andrew_Rosenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Rosenberg1">Andrew Rosenberg</a>, <a href="https://openreview.net/profile?id=~Bhuvana_Ramabhadran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhuvana_Ramabhadran2">Bhuvana Ramabhadran</a>, <a href="https://openreview.net/profile?id=~Yu_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zhang2">Yu Zhang</a>, <a href="https://openreview.net/profile?email=pedro%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="pedro@google.com">Pedro Moreno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#W6BpshgRi0q-details-97" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W6BpshgRi0q-details-97"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM), Data selection, Self-supervision, ASR, Speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work,  we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training.  ATM employs an external ASR model or \textit{scorer} to weight unsupervised input samples in two different ways: 1) A fine-grained data selection is performed by masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. 2) ATM is further extended to focus at utterance-level by weighting the final MSM loss with the utterance-level confidence score.  We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and AMI (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6\% relative) while still yielding modest improvements under matched conditions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Data selection Approach for masked speech model to focus on relevant samples to learn meaningful speech representations</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZWykq5n4zx" data-number="4598">
        <h4>
          <a href="https://openreview.net/forum?id=ZWykq5n4zx">
              Boosting the Confidence of Near-Tight Generalization Bounds for Uniformly Stable Randomized Algorithms
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZWykq5n4zx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaotong_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaotong_Yuan1">Xiaotong Yuan</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZWykq5n4zx-details-544" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZWykq5n4zx-details-544"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uniform stability, Randomized learning algorithms, Bagging, Generalization bounds, Stochastic gradient methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">High probability generalization bounds of uniformly stable learning algorithms have recently been actively studied with a series of near-tight results established by~\citet{feldman2019high,bousquet2020sharper}. However, for randomized algorithms with on-average uniform stability, such as stochastic gradient descent (SGD) with time decaying learning rates, it still remains less well understood if these deviation bounds still hold with high confidence over the internal randomness of algorithm. This paper addresses this open question and makes progress towards answering it inside a classic framework of confidence-boosting. To this end, we first establish an in-expectation first moment generalization error bound for randomized learning algorithm with on-average uniform stability, based on which we then show that a properly designed subbagging process leads to near-tight high probability generalization bounds over the randomness of data and algorithm. We further substantialize these generic results to SGD to derive improved high probability generalization bounds for convex or non-convex optimization with natural time decaying learning rates, which have not been possible to prove with the existing uniform stability results. Specially for deterministic uniformly stable algorithms, our confidence-boosting results improve upon the best known generalization bounds in terms of a logarithmic factor on sample size, which moves a step forward towards resolving an open question raised by~\citet{bousquet2020sharper}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A confidence-boosting method for deriving near-tight generalization bounds with high probability for uniformly stable randomized learning algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EhwEUb2ynIa" data-number="4591">
        <h4>
          <a href="https://openreview.net/forum?id=EhwEUb2ynIa">
              How to Adapt Your Large-Scale Vision-and-Language Model
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EhwEUb2ynIa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konwoo_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Konwoo_Kim1">Konwoo Kim</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepak_Pathak1">Deepak Pathak</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EhwEUb2ynIa-details-550" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EhwEUb2ynIa-details-550"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transfer learning, fine-tuning, layernorm, CLIP, prompt-tuning, adaptation, zero-shot, pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training large-scale vision and language models (e.g. CLIP) has shown promising results in representation and transfer learning. We investigate the question of how to efficiently adapt these models to downstream tasks. For image classification, linear probes have been the standard for ease of use and efficiency, while for language, other approaches like prompt tuning have emerged. We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one. We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board. We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks. Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models. Website at https://sites.google.com/view/adapt-large-scale-models</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a thorough analysis of different methods on how to adapt large-scale pretrained vision-and-language models to several downstream classification tasks, and find that just tuning LayerNorm is an effective fine-tuning baseline.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zLb9oSWy933" data-number="4583">
        <h4>
          <a href="https://openreview.net/forum?id=zLb9oSWy933">
              Fast Finite Width Neural Tangent Kernel
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zLb9oSWy933" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Roman_Novak2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roman_Novak2">Roman Novak</a>, <a href="https://openreview.net/profile?id=~Jascha_Sohl-Dickstein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jascha_Sohl-Dickstein2">Jascha Sohl-Dickstein</a>, <a href="https://openreview.net/profile?id=~Samuel_Stern_Schoenholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samuel_Stern_Schoenholz1">Samuel Stern Schoenholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">29 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zLb9oSWy933-details-277" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zLb9oSWy933-details-277"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Tangent Kernel, NTK, Finite Width, Fast, Algorithm, JAX, Jacobian, Software</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="80" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-msup><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.577em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="normal">Θ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>∂</mi><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>∂</mi><mi>θ</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>∂</mi><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>∂</mi><mi>θ</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><mi>T</mi></msup></math></mjx-assistive-mml></mjx-container>, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite-width NTK is notoriously expensive to compute, which severely limits its practical utility. 
        
        We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. 
        Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency.
        
        We open-source (https://github.com/iclr2022anon/fast_finite_width_ntk) our two algorithms as general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc.) and introduce no new hyper-parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop and open-source a new algorithm for fast computation of the finite width Neural Tangent Kernel, the outer product of Jacobians of a neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zLb9oSWy933&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zaALYtvbRlH" data-number="4574">
        <h4>
          <a href="https://openreview.net/forum?id=zaALYtvbRlH">
              SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zaALYtvbRlH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Peng_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Qi1">Peng Qi</a>, <a href="https://openreview.net/profile?id=~Guangtao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangtao_Wang1">Guangtao Wang</a>, <a href="https://openreview.net/profile?id=~Jing_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jing_Huang3">Jing Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zaALYtvbRlH-details-272" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zaALYtvbRlH-details-272"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequential data, sample efficiency, data augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zaALYtvbRlH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PC8u74o7xc2" data-number="4571">
        <h4>
          <a href="https://openreview.net/forum?id=PC8u74o7xc2">
              Embedding models through the lens of Stable Coloring
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PC8u74o7xc2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aditya_Desai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Desai1">Aditya Desai</a>, <a href="https://openreview.net/profile?id=~Shashank_Sonkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shashank_Sonkar1">Shashank Sonkar</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PC8u74o7xc2-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PC8u74o7xc2-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Embedding-based approaches find the semantic meaning of tokens in structured data such as natural language, graphs, and even images. To a great degree, these approaches have developed independently in different domains. However, we find a common principle underlying these formulations, and it is rooted in solutions to the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For instance, we find links between stable coloring, distribution hypothesis in natural language processing, and non-local-means denoising algorithm in image signal processing. We even find that stable coloring has strong connections to a broad class of unsupervised embedding models which is surprising at first since stable coloring is generally applied for combinatorial problems. To establish this connection concretely we define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them. Grounded on this framework, we show that many algorithms ranging across different domains are, in fact, searching for continuous stable coloring solutions of an underlying graph corresponding to the domain.  We show that popular and widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and Vis-Transformer can be understood  as instantiations of our general algorithm that solves the problem of continuous stable coloring. These instantiations offer useful insights into the workings of state-of-the-art models like BERT stimulating new research directions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose unified theoretical framework underlying the state-of-the art embedding models</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XIZaWGCPl0b" data-number="4564">
        <h4>
          <a href="https://openreview.net/forum?id=XIZaWGCPl0b">
              Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XIZaWGCPl0b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Atul_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Atul_Sharma1">Atul Sharma</a>, <a href="https://openreview.net/profile?id=~Wei_Chen26" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Chen26">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Joshua_Christian_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_Christian_Zhao1">Joshua Christian Zhao</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Somali_Chaterji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Somali_Chaterji1">Somali Chaterji</a>, <a href="https://openreview.net/profile?id=~Saurabh_Bagchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Bagchi1">Saurabh Bagchi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">33 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XIZaWGCPl0b-details-574" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XIZaWGCPl0b-details-574"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, aggregation, security, untargeted model poisoning attack</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learning—multi-party, distributed learning in a decentralized environment—is vulnerable to model poisoning attacks, even more so than centralized learning approaches.  This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Trimmed mean, and FoolsGold.  However, a recently developed targeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be pushed away from the optima to increase the test error rate. In this work, we develop tesseract—a defense against this directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT is based on a simple intuition that in a federated learning setting, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that TESSERACT provides robustness against even an adaptive white-box version of the attack.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">How to defend federated learning against local model poisoning attack, the most effective attack known to date, using the pattern of progression of gradients as each client learns.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XIZaWGCPl0b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rX3rZYP8zZF" data-number="4561">
        <h4>
          <a href="https://openreview.net/forum?id=rX3rZYP8zZF">
              CareGraph: A Graph-based Recommender System for Diabetes Self-Care
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rX3rZYP8zZF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sirinart_Tangruamsub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sirinart_Tangruamsub1">Sirinart Tangruamsub</a>, <a href="https://openreview.net/profile?id=~Karthik_Kappaganthu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Kappaganthu1">Karthik Kappaganthu</a>, <a href="https://openreview.net/profile?email=jodonovan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="jodonovan@teladochealth.com">John O'Donovan</a>, <a href="https://openreview.net/profile?email=anmol.madan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="anmol.madan@teladochealth.com">Anmol Madan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rX3rZYP8zZF-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rX3rZYP8zZF-details-370"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graph, knowledge graph embedding, recommendation system</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management.  We propose a Deep Neural Network-based recommender that uses the knowledge graph embeddings to recommend health nudges for maximizing engagement by combating the cold-start and sparsity problems. We use a leave-one-out approach to evaluate the model. We compare the proposed model performance with a text similarity and Deep-and-Cross Network-based approach as the baseline. The overall improvement in Click-Through-Rate prediction AUC for the Knowledge-Graph-based model was 11%. We also observe that our model improved the average AUC by 5% in cold-start situations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nct9j3BVswZ" data-number="4558">
        <h4>
          <a href="https://openreview.net/forum?id=Nct9j3BVswZ">
              Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nct9j3BVswZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Sercan_O_Arik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sercan_O_Arik1">Sercan O Arik</a>, <a href="https://openreview.net/profile?id=~Chen-Yu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen-Yu_Lee2">Chen-Yu Lee</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tomas_Pfister1">Tomas Pfister</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nct9j3BVswZ-details-45" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nct9j3BVswZ-details-45"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Anomaly detection, Data refinement, Iterative training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classification method by 6.3 AUC and 12.5 average precision / 22.9 F1-score.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QKEkEFpKBBv" data-number="4557">
        <h4>
          <a href="https://openreview.net/forum?id=QKEkEFpKBBv">
              DNBP: Differentiable Nonparametric Belief Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QKEkEFpKBBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anthony_Opipari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anthony_Opipari1">Anthony Opipari</a>, <a href="https://openreview.net/profile?id=~Jana_Pavlasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jana_Pavlasek1">Jana Pavlasek</a>, <a href="https://openreview.net/profile?email=joecc%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="joecc@umich.edu">Chao Chen</a>, <a href="https://openreview.net/profile?email=shoutian%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shoutian@umich.edu">Shoutian Wang</a>, <a href="https://openreview.net/profile?id=~Karthik_Desingh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Desingh1">Karthik Desingh</a>, <a href="https://openreview.net/profile?id=~Odest_Jenkins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Odest_Jenkins1">Odest Jenkins</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QKEkEFpKBBv-details-265" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QKEkEFpKBBv-details-265"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Belief Propagation, Bayesian Inference, Nonparametric Inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. Existing nonparametric belief propagation methods rely on domain-specific features encoded in the probabilistic factors of a graphical model. In this work, we replace each crafted factor with a differentiable neural network enabling the factors to be learned using an efficient optimization routine from labeled data. By combining differentiable neural networks with an efficient belief propagation algorithm, our method learns to maintain a set of marginal posterior samples using end-to-end training. We evaluate our differentiable nonparametric belief propagation (DNBP) method on a set of articulated pose tracking tasks and compare performance with learned baselines. Results from these experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches. The project webpage is available at: https://sites.google.com/view/diff-nbp</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=QKEkEFpKBBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TEt7PsVZux6" data-number="4540">
        <h4>
          <a href="https://openreview.net/forum?id=TEt7PsVZux6">
              I-PGD-AT: Efficient Adversarial Training via Imitating Iterative PGD Attack 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TEt7PsVZux6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaosen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaosen_Wang1">Xiaosen Wang</a>, <a href="https://openreview.net/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>, <a href="https://openreview.net/profile?id=~Krishnaram_Kenthapadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krishnaram_Kenthapadi1">Krishnaram Kenthapadi</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TEt7PsVZux6-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TEt7PsVZux6-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Single-step Adversarial Training, Catastrophic Overfitting, Adversarial Robustness, Adversarial Example</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial training has been widely used in various machine learning paradigms to improve the robustness; while it would increase the training cost due to the perturbation optimization process. To improve the efficiency, recent studies leverage Fast Gradient Sign Method with Random Start (FGSM-RS) for adversarial training. However, such methods would lead to relatively low robustness and catastrophic overfitting, which means the robustness against iterative attacks (e.g. Projected Gradient Descent (PGD)) would suddenly drop to 0%. Different approaches have been proposed to address this problem, while later studies show that catastrophic overfitting still remains. In this paper, motivated by the fact that expensive iterative adversarial training methods achieve high robustness without catastrophic overfitting, we aim to ask: Can we perform iterative adversarial training in an efficient way? To this end, we first analyze the difference of perturbation generated by FGSM-RS and PGD and find that PGD tends to craft diverse discrete values instead of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="81" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>±</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> in FGSM-RS. Based on this observation, we propose an efficient single-step adversarial training method I-PGD-AT by adopting I-PGD attack for training, in which I-PGD imitates PGD virtually. Unlike FGSM that crafts the perturbation directly using the sign of gradient, I-PGD imitates the perturbation of PGD based on the magnitude of gradient. Extensive empirical evaluations on CIFAR-10 and Tiny ImageNet demonstrate that our I-PGD-AT can improve the robustness compared with the baselines and significantly delay catastrophic overfitting. Moreover, we explore and discuss the factors that affect catastrophic overfitting. Finally, to demonstrate the generality of I-PGD-AT, we integrate it into PGD adversarial training and show that it can even further improve the robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an efficient adversarial training approach I-PGD-AT by imitating PGD virtually to improve single-step adversarial training effectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TEt7PsVZux6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wQ7RCayXUSl" data-number="4539">
        <h4>
          <a href="https://openreview.net/forum?id=wQ7RCayXUSl">
              Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters.
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wQ7RCayXUSl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seyed_Kamyar_Seyed_Ghasemipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyed_Kamyar_Seyed_Ghasemipour1">Seyed Kamyar Seyed Ghasemipour</a>, <a href="https://openreview.net/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Nachum1">Ofir Nachum</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wQ7RCayXUSl-details-874" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQ7RCayXUSl-details-874"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline reinforcement learning, batch reinforcement learning, ensembles, uncertainty estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order to achieve strong performance in offline reinforcement learning (RL),  it is necessary to act conservatively with respect to confident lower-bounds on anticipated values of actions. Thus, a valuable approach would be to obtain high quality uncertainty estimates on action values. In current supervised learning literature, state-of-the-art approaches to uncertainty estimation and calibration rely on ensembling methods. In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL. We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble. Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods. Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains. In the more challenging domains of the D4RL offline RL benchmark, MSG significantly surpasses highly well-tuned state-of-the-art methods in batch RL. Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective. We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles. We hope our work engenders increased focus into deep network uncertainty estimation techniques directed for reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We demonstrate how significantly beneficial uncertainty estimation through ensembles can be for offline RL and demonstrate much work is still needed for efficient ensembles to be as effective as deep ensembles.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQ7RCayXUSl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="E9e18Ms5TeV" data-number="4537">
        <h4>
          <a href="https://openreview.net/forum?id=E9e18Ms5TeV">
              A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=E9e18Ms5TeV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zachary_Nado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zachary_Nado1">Zachary Nado</a>, <a href="https://openreview.net/profile?id=~Justin_Gilmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Justin_Gilmer1">Justin Gilmer</a>, <a href="https://openreview.net/profile?id=~Christopher_J_Shallue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_J_Shallue1">Christopher J Shallue</a>, <a href="https://openreview.net/profile?id=~Rohan_Anil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Anil1">Rohan Anil</a>, <a href="https://openreview.net/profile?id=~George_Edward_Dahl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Edward_Dahl1">George Edward Dahl</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#E9e18Ms5TeV-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E9e18Ms5TeV-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural networks, deep learning, neural network optimization, hyperparameter tuning, optimizer comparison</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We retune the Nesterov/Adam optimizers on pipelines where LARS/LAMB are commonly used and achieve similar or better performance, providing competitive baselines for the large batch training setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=E9e18Ms5TeV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ueTHF0yAlZ" data-number="4533">
        <h4>
          <a href="https://openreview.net/forum?id=5ueTHF0yAlZ">
              Improving greedy core-set configurations for active learning with uncertainty-scaled distances
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ueTHF0yAlZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuchen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuchen_Li1">Yuchen Li</a>, <a href="https://openreview.net/profile?id=~Frank_Rudzicz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_Rudzicz2">Frank Rudzicz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ueTHF0yAlZ-details-478" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ueTHF0yAlZ-details-478"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Active learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We scale perceived distances of the core-set algorithm by a factor of uncertainty and search for low-confidence configurations, finding significant improvements in sample efficiency across CIFAR10/100 and SVHN image classification, especially in larger acquisition sizes. We show the necessity of our modifications and explain how the improvement is due to a probabilistic quadratic speed-up in the convergence of core-set loss, under assumptions about the relationship of model uncertainty and misclassification.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Improved core-set for active learning using confidence-scaled distances.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ueTHF0yAlZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nus6fOfh1HW" data-number="4531">
        <h4>
          <a href="https://openreview.net/forum?id=Nus6fOfh1HW">
              On the Relationship between Heterophily and Robustness of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nus6fOfh1HW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiong_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiong_Zhu1">Jiong Zhu</a>, <a href="https://openreview.net/profile?id=~Junchen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junchen_Jin1">Junchen Jin</a>, <a href="https://openreview.net/profile?id=~Donald_Loveland2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Donald_Loveland2">Donald Loveland</a>, <a href="https://openreview.net/profile?id=~Michael_T_Schaub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_T_Schaub1">Michael T Schaub</a>, <a href="https://openreview.net/profile?id=~Danai_Koutra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danai_Koutra1">Danai Koutra</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nus6fOfh1HW-details-825" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nus6fOfh1HW-details-825"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph neural networks, adversarial attacks, heterophily, structural perturbation, robustness, relation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels). In this work, we formalize the relation between heterophily and robustness, bridging two topics previously investigated by separate lines of research. We theoretically and empirically show that for graphs exhibiting homophily (low heterophily), impactful structural attacks always lead to increased levels of heterophily, while for graph with heterophily the change in the homophily level depends on the node degrees. By leveraging these insights, we deduce that a design principle identified to significantly improve predictive performance under heterophily—separate aggregators for ego- and neighbor-embeddings—can also inherently offer increased robustness to GNNs. Our extensive empirical analysis shows that GNNs adopting this design alone can achieve significantly improved empirical and certifiable robustness compared to the best-performing unvaccinated model. Furthermore, models with this design can be readily combined with explicit defense mechanisms to yield improved robustness with up to 18.33% increase in performance under attacks compared to the best-performing vaccinated model.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We explore the interplay between heterophily &amp; robustness in GNNs, and show that 1) effective structural attacks on homophilous graphs increase heterophily, 2) heterophilous GNN designs can be combined with defense mechanisms for improved robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Nus6fOfh1HW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-29uFS4FiDZ" data-number="4524">
        <h4>
          <a href="https://openreview.net/forum?id=-29uFS4FiDZ">
              Word Sense Induction with Knowledge Distillation from BERT
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-29uFS4FiDZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anik_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anik_Saha1">Anik Saha</a>, <a href="https://openreview.net/profile?id=~Alex_Gittens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Gittens1">Alex Gittens</a>, <a href="https://openreview.net/profile?id=~Bulent_Yener2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bulent_Yener2">Bulent Yener</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-29uFS4FiDZ-details-338" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-29uFS4FiDZ-details-338"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embeddings, sense embeddings, word sense induction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems.  Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Effective approach to distil word meaning from contextual embeddings to word sense embeddings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PGGjnBiQ84G" data-number="4520">
        <h4>
          <a href="https://openreview.net/forum?id=PGGjnBiQ84G">
              Learning Surface Parameterization for Document Image Unwarping
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PGGjnBiQ84G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sagnik_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sagnik_Das1">Sagnik Das</a>, <a href="https://openreview.net/profile?id=~Ke_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ke_Ma3">Ke Ma</a>, <a href="https://openreview.net/profile?id=~Zhixin_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhixin_Shu1">Zhixin Shu</a>, <a href="https://openreview.net/profile?id=~Dimitris_Samaras3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dimitris_Samaras3">Dimitris Samaras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PGGjnBiQ84G-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PGGjnBiQ84G-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit functions, texture mapping, surface parameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping. We propose an efficient method to learn surface parameterization by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates. Our surface parameterization network can be conveniently plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Recent work on differentiable rendering techniques for implicit surfaces has shown high-quality 3D scene reconstruction and view synthesis results. However, these methods typically learn the appearance color as a function of the surface points and lack explicit surface parameterization. Thus they do not allow texture map extraction or texture editing. By introducing explicit surface parameterization and learning with a recent differentiable renderer for implicit surfaces, we demonstrate state-of-the-art document-unwarping via texture extraction. We show that our approach can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. We also demonstrate the usefulness of our system by applying it to document texture editing.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning surface parameterization using rendering loss and multiview images</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PGGjnBiQ84G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4JlwgTbmzXQ" data-number="4519">
        <h4>
          <a href="https://openreview.net/forum?id=4JlwgTbmzXQ">
              EqR: Equivariant Representations for Data-Efficient Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4JlwgTbmzXQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnab_Kumar_Mondal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnab_Kumar_Mondal1">Arnab Kumar Mondal</a>, <a href="https://openreview.net/profile?id=~Vineet_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vineet_Jain1">Vineet Jain</a>, <a href="https://openreview.net/profile?id=~Kaleem_Siddiqi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaleem_Siddiqi1">Kaleem Siddiqi</a>, <a href="https://openreview.net/profile?id=~Siamak_Ravanbakhsh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siamak_Ravanbakhsh1">Siamak Ravanbakhsh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4JlwgTbmzXQ-details-529" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4JlwgTbmzXQ-details-529"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Equivariance, Invariance, Representation learning, Reinforcement learning, Symmetric MDPs, MDP homomorphism, Lie parameterization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent’s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Equivariant representation learning for data-efficient reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=4JlwgTbmzXQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JHXjK94yH-y" data-number="4518">
        <h4>
          <a href="https://openreview.net/forum?id=JHXjK94yH-y">
              Explore and Control with Adversarial Surprise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JHXjK94yH-y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnaud_Fickinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnaud_Fickinger1">Arnaud Fickinger</a>, <a href="https://openreview.net/profile?id=~Natasha_Jaques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Natasha_Jaques1">Natasha Jaques</a>, <a href="https://openreview.net/profile?id=~Samyak_Parajuli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samyak_Parajuli1">Samyak Parajuli</a>, <a href="https://openreview.net/profile?id=~Michael_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Chang1">Michael Chang</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stuart_Russell1">Stuart Russell</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JHXjK94yH-y-details-658" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JHXjK94yH-y-details-658"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, intrinsic motivation, exploration, multi-agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering. However, a central challenge in unsupervised RL is to extract behaviors that meaningfully affect the world and cover the range of possible outcomes, without getting distracted by inherently unpredictable, uncontrollable, and stochastic elements in the environment. To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences. The Explore agent seeks out states that maximally surprise the Control agent, which in turn aims to minimize surprise, and thereby manipulate the environment to return to familiar and predictable states. The competition between these two policies drives them to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions. Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Two policies play a multi-agent adversarial game over the amount of surprise or observation entropy an agent experiences, leading the agent to fully explore the underlying state space and learn meaningful behaviors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ck_iw4jMC4l" data-number="4508">
        <h4>
          <a href="https://openreview.net/forum?id=Ck_iw4jMC4l">
              Logical Activation Functions: Logit-space equivalents of Boolean Operators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ck_iw4jMC4l" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Scott_C_Lowe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Scott_C_Lowe1">Scott C Lowe</a>, <a href="https://openreview.net/profile?email=robearle11%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="robearle11@gmail.com">Robert Earle</a>, <a href="https://openreview.net/profile?id=~Jason_d%26%23x27%3BEon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_d&#39;Eon1">Jason d'Eon</a>, <a href="https://openreview.net/profile?id=~Thomas_Trappenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Trappenberg1">Thomas Trappenberg</a>, <a href="https://openreview.net/profile?id=~Sageev_Oore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sageev_Oore1">Sageev Oore</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ck_iw4jMC4l-details-257" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ck_iw4jMC4l-details-257"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">activation functions, logits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence (versus absence) of features within the stimulus. Under this interpretation, we can derive the probability <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="82" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>∩</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> that a pair of independent features are both present in the stimulus from their logits. By converting the resulting probability back into a logit, we obtain a logit-space equivalent of the AND operation. However, since this function involves taking multiple exponents and logarithms, it is not well suited to be directly used within neural networks. We thus constructed an efficient approximation named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="83" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> (the AND operator Approximate for Independent Logits) utilizing only comparison and addition operations, which can be deployed as an activation function in neural networks. Like MaxOut, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="84" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> is a generalization of ReLU to two-dimensions. Additionally, we constructed efficient approximations of the logit-space equivalents to the OR and XNOR operators. We deployed these new activation functions, both in isolation and in conjunction, and demonstrated their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We motivate novel activation functions which are logit-space equivalents to boolean operations, and find they work well on a wide variety of tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ofLwshMBL_H" data-number="4507">
        <h4>
          <a href="https://openreview.net/forum?id=ofLwshMBL_H">
              Continual Learning Using Task Conditional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ofLwshMBL_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Honglin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Honglin_Li1">Honglin Li</a>, <a href="https://openreview.net/profile?id=~Frieder_Ganz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frieder_Ganz1">Frieder Ganz</a>, <a href="https://openreview.net/profile?id=~David_J._Sharp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_J._Sharp1">David J. Sharp</a>, <a href="https://openreview.net/profile?id=~Payam_M._Barnaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_M._Barnaghi1">Payam M. Barnaghi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ofLwshMBL_H-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ofLwshMBL_H-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">catastrophic forgetting, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning changes, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. Dynamic approaches, which assign new neuron resources to the upcoming tasks, are introduced to address this issue. However, most of the dynamic methods need task information about the upcoming tasks during the inference phase to activate the corresponding neurons. To address this issue, we introduce Task Conditional Neural Network which allows the model to identify the task information automatically. The proposed model can continually learn and embed new tasks into the model without losing the information about previously learned tasks. We evaluate the proposed model combined with the mixture of experts approach on the MNIST and CIFAR100 datasets and show how it significantly improves the continual learning process without requiring task information in advance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A dynamic approach for continual learning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xspalMXAB0M" data-number="4483">
        <h4>
          <a href="https://openreview.net/forum?id=xspalMXAB0M">
              A Boosting Approach to Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xspalMXAB0M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nataly_Brukhim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nataly_Brukhim1">Nataly Brukhim</a>, <a href="https://openreview.net/profile?id=~Elad_Hazan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elad_Hazan1">Elad Hazan</a>, <a href="https://openreview.net/profile?id=~Karan_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karan_Singh1">Karan Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xspalMXAB0M-details-921" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xspalMXAB0M-details-921"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form.
            Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function, or by relaxing the solution guarantee to a local optimality condition.
        
            We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an effective policy. The notion of weak learning we study is that of sampled-based approximate optimization of linear functions over policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods iteratively. We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular, our bound does not explicitly depend on the number of states.
        
            A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global optimality guarantees.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xspalMXAB0M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HUeyM2qVey2" data-number="4482">
        <h4>
          <a href="https://openreview.net/forum?id=HUeyM2qVey2">
              Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HUeyM2qVey2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michael_Anthony_Puthawala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Anthony_Puthawala1">Michael Anthony Puthawala</a>, <a href="https://openreview.net/profile?id=~Matti_Lassas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matti_Lassas1">Matti Lassas</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_Dokmanić1">Ivan Dokmanić</a>, <a href="https://openreview.net/profile?id=~Maarten_V._de_Hoop2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maarten_V._de_Hoop2">Maarten V. de Hoop</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HUeyM2qVey2-details-492" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HUeyM2qVey2-details-492"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Universality, Flow Networks, Manifold Learning, Density Estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements. We find that such networks universally approximate a large class of manifolds simultaneously with densities supported on them. Among others, our results apply to the well-known coupling and autoregressive flows. We build on the work of Teshima et al. 2020 on bijective flows and study injective architectures proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a new theoretical device called the \emph{embedding gap}, which measures how far one continuous manifold is from embedding another. We relate the embedding gap to a relaxation of universally we call the \emph{manifold embedding property}, capturing the geometric part of universality. Our proof also establishes that optimality of a network can be established ``in reverse,''  resolving a conjecture made in Brehmer et al. 2020 and opening the door for simple layer-wise training schemes. Finally, we show that the studied networks admit an exact layer-wise projection result, Bayesian uncertainty quantification, and black-box recovery of network weights.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements and find that such networks universally approximate a large class of manifolds and densities there on.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zrdUVVAvcP2" data-number="4477">
        <h4>
          <a href="https://openreview.net/forum?id=zrdUVVAvcP2">
              GrASP: Gradient-Based Affordance Selection for Planning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zrdUVVAvcP2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vivek_Veeriah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vivek_Veeriah2">Vivek Veeriah</a>, <a href="https://openreview.net/profile?id=~Zeyu_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zeyu_Zheng1">Zeyu Zheng</a>, <a href="https://openreview.net/profile?id=~Richard_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Lewis1">Richard Lewis</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satinder_Singh2">Satinder Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zrdUVVAvcP2-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zrdUVVAvcP2-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, affordances</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for \emph{selecting} affordances useful for planning---for learning which small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option  affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning to select affordances in the form of options and primitive actions for lookahead planning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="14kbUbOaZUc" data-number="4476">
        <h4>
          <a href="https://openreview.net/forum?id=14kbUbOaZUc">
              Metric Learning on Temporal Graphs via Few-Shot Examples
          </a>
        
          
            <a href="https://openreview.net/pdf?id=14kbUbOaZUc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongqi_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongqi_Fu1">Dongqi Fu</a>, <a href="https://openreview.net/profile?id=~Liri_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liri_Fang1">Liri Fang</a>, <a href="https://openreview.net/profile?id=~Ross_Maciejewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ross_Maciejewski1">Ross Maciejewski</a>, <a href="https://openreview.net/profile?id=~Vetle_I_Torvik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vetle_I_Torvik1">Vetle I Torvik</a>, <a href="https://openreview.net/profile?id=~Jingrui_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingrui_He1">Jingrui He</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#14kbUbOaZUc-details-155" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="14kbUbOaZUc-details-155"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Metric Learning, Few-Shot Learning, Temporal Graph</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs; Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales. Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The first attempt to learn temporal graph representations, on the graph-level, covering the whole lifetime, and only consuming a few labeled samples. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=14kbUbOaZUc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a61qArWbjw_" data-number="4475">
        <h4>
          <a href="https://openreview.net/forum?id=a61qArWbjw_">
              Scalable multimodal variational autoencoders with surrogate joint posterior
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a61qArWbjw_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Masahiro_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Masahiro_Suzuki1">Masahiro Suzuki</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yutaka_Matsuo1">Yutaka Matsuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a61qArWbjw_-details-465" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a61qArWbjw_-details-465"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep generative models, multimodal learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To obtain a joint representation from multimodal data in variational autoencoders (VAEs), it is important to infer the representation from arbitrary subsets of modalities after learning.  A scalable way to achieve this is to aggregate the inferences of each modality as experts. A state-of-the-art approach to learning this aggregation of experts is to encourage all modalities to be reconstructed and cross-generated from arbitrary subsets. However, this learning may be insufficient if cross-generation is difficult. Furthermore, to evaluate its objective function, exponential generation paths concerning the number of modalities are required. To alleviate these problems, we propose to explicitly minimize the divergence between inferences from arbitrary subsets and the surrogate joint posterior that approximates the true joint posterior. We also proposed using a gradient origin network, a deep generative model that learns inferences without using an inference network, thereby reducing the need for additional parameters by introducing the surrogate posterior. We demonstrate that our method performs better than existing scalable multimodal VAEs in inference and generation.    
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We proposed a scalable and high performance multimodal VAE in the framework of approximating inferences from arbitrary subsets of modalities to a surrogate joint posterior.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fwJWhOxuzV9" data-number="4471">
        <h4>
          <a href="https://openreview.net/forum?id=fwJWhOxuzV9">
              Semi-supervised Offline Reinforcement Learning with Pre-trained Decision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fwJWhOxuzV9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Catherine_Cang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Catherine_Cang1">Catherine Cang</a>, <a href="https://openreview.net/profile?id=~Kourosh_Hakhamaneshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Hakhamaneshi1">Kourosh Hakhamaneshi</a>, <a href="https://openreview.net/profile?id=~Ryan_Rudes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryan_Rudes1">Ryan Rudes</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Aravind_Rajeswaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aravind_Rajeswaran1">Aravind Rajeswaran</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fwJWhOxuzV9-details-870" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fwJWhOxuzV9-details-870"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-task RL, Decision Transformer, self-supervised RL, Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training deep neural network models using large unlabelled datasets followed by fine-tuning them on small task-specific datasets has emerged as a dominant paradigm in natural language processing (NLP) and computer vision (CV). Despite the widespread success, such a paradigm has remained atypical in reinforcement learning (RL).
        In this paper, we investigate how we can leverage large reward-free (i.e. task-agnostic) offline datasets of prior interactions to pre-train agents that can then be fine-tuned using a small reward-annotated dataset. To this end, we present Pre-trained Decision Transformer (PDT), a simple yet powerful algorithm for semi-supervised Offline RL. By masking reward tokens during pre-training, the transformer learns to autoregressivley predict actions based on previous state and action context and effectively extracts behaviors present in the dataset. During fine-tuning, rewards are un-masked and the agent learns the set of skills that should be invoked for the desired behavior as per the reward function. We demonstrate the efficacy of this simple and flexible approach on tasks from the D4RL benchmark with limited reward annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce Pre-trained Decision Transformers, a simple and flexible architecture that can be pre-trained on unlabeled environment interactions and can quickly adapt to several downstream tasks with just a small reward-annotated fine-tuning dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_Ko4kT3ckWy" data-number="4470">
        <h4>
          <a href="https://openreview.net/forum?id=_Ko4kT3ckWy">
              Increase and Conquer: Training Graph Neural Networks on Growing Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_Ko4kT3ckWy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Juan_Cervino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_Cervino1">Juan Cervino</a>, <a href="https://openreview.net/profile?id=~Luana_Ruiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luana_Ruiz1">Luana Ruiz</a>, <a href="https://openreview.net/profile?id=~Alejandro_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alejandro_Ribeiro1">Alejandro Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_Ko4kT3ckWy-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_Ko4kT3ckWy-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Graph Neural Networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) use graph convolutions to exploit network invariances and learn meaningful features from network data. However, on large-scale graphs convolutions incur in high computational cost, leading to scalability limitations. Leveraging the graphon --- the limit object of a graph --- in this paper we consider the problem of learning a graphon neural network (WNN) --- the limit object of a GNN --- by training GNNs on graphs sampled Bernoulli from the graphon. Under smoothness conditions, we show that: (i) the expected distance between the learning steps on the GNN and on the WNN decreases asymptotically with the size of the graph, and (ii) when training on a sequence of growing graphs, gradient descent follows the learning direction of the WNN. Inspired by these results, we propose a novel algorithm to learn GNNs on large-scale graphs that, starting from a moderate number of nodes, successively increases the size of the graph during training. This algorithm is benchmarked on both a recommendation system and a decentralized control problem where it is shown to retain comparable performance, to its large-scale counterpart, at a reduced computational cost.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper describes a way to train GNNs on a sequence of growing graphs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_Ko4kT3ckWy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MQuxKr2F1Xw" data-number="4468">
        <h4>
          <a href="https://openreview.net/forum?id=MQuxKr2F1Xw">
              Multi-Trigger-Key: Towards Multi-Task Privacy-Preserving In Deep Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MQuxKr2F1Xw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ren_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ren_Wang1">Ren Wang</a>, <a href="https://openreview.net/profile?id=~Zhe_Xu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhe_Xu7">Zhe Xu</a>, <a href="https://openreview.net/profile?id=~Alfred_Hero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alfred_Hero1">Alfred Hero</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MQuxKr2F1Xw-details-953" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MQuxKr2F1Xw-details-953"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning-based Multi-Task Classification (MTC) is widely used in applications like facial attribute and healthcare that warrant strong privacy guarantees. In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective. MTK associates each secured task in the multi-task dataset with a specifically designed trigger-key. The true information can be revealed by adding the trigger-key if the user is authorized. We obtain such an MTK model by training it with a newly generated training set. To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance. Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=MQuxKr2F1Xw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>‹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">›</a>
      </li>
      <li class="  right-arrow" data-page-number="31">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="desk-rejected-withdrawn-submissions">

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="XNYOJD0QdBD" data-number="4713">
        <h4>
          <a href="https://openreview.net/forum?id=XNYOJD0QdBD">
              Personalized PageRank meets Graph Attention Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XNYOJD0QdBD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Julie_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julie_Choi1">Julie Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XNYOJD0QdBD-details-676" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XNYOJD0QdBD-details-676"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GNN, Personalized PageRank, Graph Attention Network, Graph Neural Network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rising interest in graph neural networks (GNNs) for representation learning over the past few years. GNNs provide a general and efficient framework to learn from graph-structured data. However, GNNs typically only use the information of a very limited neighborhood for each node. A larger neighborhood would be desirable to provide the model with more information. However, increasing the size of the neighborhood is not trivial since neighborhood aggregation over many layers leads to over-smoothing. In this work, we incorporate the limit distribution of Personalized PageRank (PPR) into graph attention networks (GATs) to address this issue. Intuitively, message aggregation based on Personalized PageRank corresponds to infinitely many neighborhood aggregation layers. We show that our models outperform a variety ofbaseline models across all datasets used for our experiments. Our implementation is publicly available online.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Personalized PageRank meets Graph Attention Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HLTLhiBtUcW" data-number="4712">
        <h4>
          <a href="https://openreview.net/forum?id=HLTLhiBtUcW">
              Enhanced neural network regularization with macro-block dropout
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HLTLhiBtUcW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HLTLhiBtUcW-details-188" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HLTLhiBtUcW-details-188"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">macro block dropout, regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> This paper proposes a new regularization algorithm referred to as macro-block dropout. The overfitting issue has been a difficult problem in training large network models. The dropout technique has proven to be simple yet very effective for regularization by preventing complex co-adaptations on training data.  In this work, we observe that in the hidden outputs, the correlations between geometrically close elements are usually stronger than those between distant elements. Motivated by this observation, we define a macro-block that contains multiple elements of the hidden output layer in order to reduce co-adaptations more effectively. Rather than applying dropout to each element, we apply random dropout to each macro-block. In our experiments with  image classification tasks on the MNIST and the ImageNet datasets as well as a speech recognition task on the LibriSpeech set, this simple algorithm has shown a quite significant improvement over the conventional dropout approach</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a macro-block dropout for better regularization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XY1DWeh58WR" data-number="4709">
        <h4>
          <a href="https://openreview.net/forum?id=XY1DWeh58WR">
              Deep Recurrent Neural Network Layers with Layerwise Loss
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XY1DWeh58WR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XY1DWeh58WR-details-82" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XY1DWeh58WR-details-82"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning techniques have brought significant performance improvement to various areas of machine learning. Especially in the computer vision area, very deep networks such as ResNet have shown notable performance improvement. However, in speech recognition or language processing, such kinds of a very deep network have not been extensively employed. In this paper, we propose a very deep LSTM structure and their training strategy. In our training strategy, we first start training a conventional model with several LSTM layers. One notable difference is that for the top LSTM layer of the initial model, the Connectionist Temporal Classification (CTC) loss is applied both to the input and output of this top LSTM layer. Once this initial model is sufficiently layered, this top layer is copied to construct a very deep LSTM stack. For this newly constructed stack, the CTC loss is applied to every output of the LSTM layer as well as the top of the stack. Experimental results show that this deep LSTM structure shows significantly better results than the conventional model with 5 ~ 6 layers with a comparable number of parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a very deep neural network with layerwise loss</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="np5BgCFSsbm" data-number="4433">
        <h4>
          <a href="https://openreview.net/forum?id=np5BgCFSsbm">
              Neocortical cell type classification from electrophysiology recordings using deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=np5BgCFSsbm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raymond_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raymond_Wang1">Raymond Wang</a>, <a href="https://openreview.net/profile?id=~Sang_Min_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sang_Min_Han1">Sang Min Han</a>, <a href="https://openreview.net/profile?id=~Marta_Agnieszka_Gajowa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marta_Agnieszka_Gajowa1">Marta Agnieszka Gajowa</a>, <a href="https://openreview.net/profile?id=~Chunlei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chunlei_Liu1">Chunlei Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#np5BgCFSsbm-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="np5BgCFSsbm-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuron type classification, convolutional neural network, electrophysiology</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the neural code requires identifying different functional units involved in the neural circuits. One way to identify these functional units is to solve a neuron type classification problem. For decades, current clamp electrophysiology recordings have provided the means to classify the neurons based on subtle differences in action potential shapes and spiking patterns. However, significant variations in neuronal type definitions, classification pipelines, and variability in the neuronal activities make unambiguous determination of neuron type challenging. Previous solutions to this electrophysiology-based cell type classification problem consisted of dimensionality reduction juxtaposed with clustering using hand-crafted action potential features. Recent discoveries have allowed genetic-based cell-type classifications, which have fewer ambiguities, but they are less practical in vivo and have even lower throughput. Leveraging the unprecedented ground truth data published in the Allen Institute Cell Types Database, which contains anatomical, genetic, and electrophysiology characterizations of neurons in the mouse neocortex, we construct a robust and efficient convolutional neural network (CNN) that successfully classifies neurons according to their genetic label or broad type (excitatory or inhibitory) solely using current-clamp electrophysiology recordings. The CNN is configured as a multiple-input single-output network consisting of three subnetworks that take in the raw time series electrophysiology recording as well as the real and imaginary components of its Fourier coefficients. Our single pipeline method is fast and streamlined while simultaneously outperforming previous methods and achieving more classification classes using only single current-clamp trace as the input. This end-to-end convolutional neural network-based classification method removes the need for hand-crafted features, specific knowledge, or human intervention for quick identification of the cell type with high accuracy, enabling interpretation of the experimental data in a bias-free manner and a much broader scientific context.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust and efficient convolutional neural network successfully classifies neurons according to their genetic label and broad type using only current-clamp electrophysiology recordings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NMSugaVzIT" data-number="4294">
        <h4>
          <a href="https://openreview.net/forum?id=NMSugaVzIT">
              Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMSugaVzIT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Meena_Jagadeesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Meena_Jagadeesan1">Meena Jagadeesan</a>, <a href="https://openreview.net/profile?id=~Ilya_Razenshteyn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ilya_Razenshteyn1">Ilya Razenshteyn</a>, <a href="https://openreview.net/profile?id=~Suriya_Gunasekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Suriya_Gunasekar1">Suriya Gunasekar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMSugaVzIT-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMSugaVzIT-details-264"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">minimizing parameter l2 norm, representation cost, implicit bias</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We provide a function space characterization of the inductive bias resulting from minimizing the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="85" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of the weights in multi-channel linear convolutional networks. We define an \textit{induced regularizer} in the function space as the minimum <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="86" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of weights of a network required to realize a function.  For two layer linear convolutional networks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="87" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> output channels and kernel size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="88" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, we show the following: (a) If the inputs to the network have a single channel, the induced regularizer for any <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="89" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is \textit{independent} of the number of output channels <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="90" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for networks with multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias \emph{does} depend on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="91" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. However, for sufficiently large <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="92" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>, the induced regularizer is again given by an SDP that is independent of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="93" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. In particular, the induced regularizer for  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="94" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="95" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mi>D</mi></math></mjx-assistive-mml></mjx-container> are given in closed form as the nuclear norm and the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="96" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> group-sparse norm, respectively, of the Fourier coefficients.
        We investigate the applicability of our theoretical results to a broader scope of ReLU convolutional networks through experiments on MNIST and CIFAR-10 datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the function space view of minimizing l2 norm of weights in multi-channel linear convolutional networks, uncovering an invariance to the number of output channels. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMSugaVzIT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DVSN9nJB1_" data-number="3792">
        <h4>
          <a href="https://openreview.net/forum?id=DVSN9nJB1_">
              E-LANG: Energy-based Joint Inferencing of Super and Swift Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DVSN9nJB1_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mohammad_Akbari3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Akbari3">Mohammad Akbari</a>, <a href="https://openreview.net/profile?id=~Amin_Banitalebi-Dehkordi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amin_Banitalebi-Dehkordi1">Amin Banitalebi-Dehkordi</a>, <a href="https://openreview.net/profile?id=~Yong_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yong_Zhang2">Yong Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DVSN9nJB1_-details-385" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DVSN9nJB1_-details-385"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based models, dynamic inference, joint language models, super model optimization, NLP, BERT, T5</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building very large and highly capable language models has been a trend in the past several years. Despite their great performance, they incur a high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the incoming samples to one of the two models based on the energy characteristics of the representations in the latent space. The proposed approach is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. Unlike existing methods that are for the most part only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The performance of the proposed Energy-based joint inferencing of LANGuage models, E-LANG, is verified through an extensive set of experiments with T5 and BERT architectures on GLUE, SuperGLUE, and WMT benchmarks. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA (state-of-the-art) on GLUE with 3.2X less computations. Code is available in the supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we present E-LANG, an energy-based joint inference approach, which combines Super and Swift language models for achieving efficient inference without sacrificing the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=DVSN9nJB1_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="k_Zy6glYaqc" data-number="3557">
        <h4>
          <a href="https://openreview.net/forum?id=k_Zy6glYaqc">
              Quantum Alphatron
          </a>
        
          
            <a href="https://openreview.net/pdf?id=k_Zy6glYaqc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyi_Yang1">Siyi Yang</a>, <a href="https://openreview.net/profile?id=~Patrick_Rebentrost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_Rebentrost1">Patrick Rebentrost</a>, <a href="https://openreview.net/profile?id=~Miklos_Santha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miklos_Santha1">Miklos Santha</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#k_Zy6glYaqc-details-310" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="k_Zy6glYaqc-details-310"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finding provably efficient algorithms for learning neural networks is a fundamental challenge in the theory of machine learning. The Alphatron of Goel and Klivans is the first provably efficient algorithm for learning neural networks with more than one nonlinear layer. The algorithm succeeds with any distribution on the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="97" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-dimensional unit ball and without any assumption on the structure of the network. In this work, we refine the original Alphatron by a pre-computing phase for its most time-consuming part, the evaluation of the kernel function. This refined algorithm improves the run time of the original Alphatron, while retaining the same learning guarantee. Based on the refined algorithm, we quantize the pre-computing phase with provable learning guarantee in the fault-tolerant quantum computing model. In a well-defined learning model, this quantum algorithm is able to provide a quadratic speedup in the data dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="98" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>. In addition, we discuss the second type of speedup, quantizing the evaluation of the gradient in the stochastic gradient descent procedure. Our work contributes to the study of quantum learning with kernels and from samples.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QWc35QxXPzZ" data-number="3480">
        <h4>
          <a href="https://openreview.net/forum?id=QWc35QxXPzZ">
              The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QWc35QxXPzZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chi_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chi_Jin1">Chi Jin</a>, <a href="https://openreview.net/profile?id=~Qinghua_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghua_Liu1">Qinghua Liu</a>, <a href="https://openreview.net/profile?id=~Tiancheng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tiancheng_Yu1">Tiancheng Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QWc35QxXPzZ-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QWc35QxXPzZ-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">theoretical reinforcement learning, Markov games with general function approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern reinforcement learning (RL) commonly engages practical problems with large state spaces, where function approximation must be deployed to approximate either the value function or the policy. While recent progresses in RL theory address a rich set of RL problems with general function approximation, such successes are mostly restricted to the single-agent setting. It remains elusive how to extend these results to multi-agent RL, especially due to the new challenges arising from its game-theoretical nature. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algorithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension -- a new complexity measure adapted from its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter, which facilitates the learning of the main player by deliberately exploiting her weakness. Our theoretical framework is generic, which applies to a wide range of models including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper studies sample-efficient learning of Markov Games with general function approximation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6Q5RdltG3L" data-number="3243">
        <h4>
          <a href="https://openreview.net/forum?id=6Q5RdltG3L">
              Human imperceptible attacks and applications to improve fairness
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6Q5RdltG3L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xinru_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xinru_Hua1">Xinru Hua</a>, <a href="https://openreview.net/profile?id=~Huanzhong_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huanzhong_Xu1">Huanzhong Xu</a>, <a href="https://openreview.net/profile?id=~Jose_Blanchet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jose_Blanchet1">Jose Blanchet</a>, <a href="https://openreview.net/profile?id=~Viet_Anh_Nguyen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Viet_Anh_Nguyen2">Viet Anh Nguyen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6Q5RdltG3L-details-494" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Q5RdltG3L-details-494"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural networks are able to perform at least as well as humans in numerous tasks involving object classification and image generation. However, there is also evidence that perturbations which are imperceptible to humans may significantly degrade the performance of well-trained deep neural networks. We provide a Distributionally Robust Optimization (DRO) framework which integrates human-based image quality assessment methods to design optimal attacks that are imperceptible to humans but significantly damaging to deep neural networks. Our attack algorithm can generate better-quality (less perceptible to humans) attacks than other state-of-the-art human imperceptible attack methods. We provide an algorithmic implementation of independent interest which can speed up DRO training significantly. Finally, we demonstrate how the use of optimally designed human imperceptible attacks can improve group fairness in image classification while maintaining a similar accuracy.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=6Q5RdltG3L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vNDHZZa-Q92" data-number="3220">
        <h4>
          <a href="https://openreview.net/forum?id=vNDHZZa-Q92">
              Neural Extensions: Training Neural Networks with Set Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vNDHZZa-Q92" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nikolaos_Karalias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikolaos_Karalias1">Nikolaos Karalias</a>, <a href="https://openreview.net/profile?id=~Joshua_David_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_David_Robinson1">Joshua David Robinson</a>, <a href="https://openreview.net/profile?id=~Andreas_Loukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Loukas1">Andreas Loukas</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vNDHZZa-Q92-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vNDHZZa-Q92-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continuous extensions, algorithmic reasoning, set functions, machine learning, combinatorial optimization, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integrating discrete computational steps into deep learning architectures is an important consideration when learning to reason over discrete items. However, many tasks that involve discrete choices are defined via (combinatorial) set functions, and thereby pose challenges for end-to-end training. In this work, we explore a general framework to construct continuous extensions of such discrete functions that enables training via gradient methods. Our framework includes well-known extensions such as the Lovasz extension of submodular set functions and facilitates the design of novel continuous extensions based on problem-specific considerations, including constraints. We demonstrate the versatility of our framework on tasks ranging from combinatorial optimization to image classification. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A principled framework for continuous extensions of set functions in machine learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oykI6Kmq3Xi" data-number="3194">
        <h4>
          <a href="https://openreview.net/forum?id=oykI6Kmq3Xi">
              Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oykI6Kmq3Xi" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ryann_Sim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryann_Sim1">Ryann Sim</a>, <a href="https://openreview.net/profile?id=~EFSTRATIOS_PANTELEIMON_SKOULAKIS2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~EFSTRATIOS_PANTELEIMON_SKOULAKIS2">EFSTRATIOS PANTELEIMON SKOULAKIS</a>, <a href="https://openreview.net/profile?id=~Lillian_J_Ratliff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lillian_J_Ratliff1">Lillian J Ratliff</a>, <a href="https://openreview.net/profile?id=~Georgios_Piliouras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Piliouras1">Georgios Piliouras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oykI6Kmq3Xi-details-940" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oykI6Kmq3Xi-details-940"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">extensive form games, network extensive form games, online learning, optimistic gradient descent ascent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFG) and particularly in EFGs with many agents lags far behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="99" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> rate of convergence to the set of Nash equilibria. Moreover, we show that the day-to-day behavior also converges to Nash with rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="100" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mrow data-mjx-texclass="ORD"><mo>−</mo><mi>t</mi></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for some game-dependent constant <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="101" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mo>&gt;</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide a formulation of network zero-sum extensive form games and show that optimistic gradient ascent admits fast convergence to Nash, both in time average and in the day-to-day sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oykI6Kmq3Xi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JvVFSmFV8G" data-number="3163">
        <h4>
          <a href="https://openreview.net/forum?id=JvVFSmFV8G">
              Which model to trust: assessing the influence of models on the performance of reinforcement learning algorithms for continuous control tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JvVFSmFV8G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Giacomo_Arcieri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Giacomo_Arcieri1">Giacomo Arcieri</a>, <a href="https://openreview.net/profile?email=woelfle%40fzi.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="woelfle@fzi.de">David Wölfle</a>, <a href="https://openreview.net/profile?id=~Eleni_Chatzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleni_Chatzi1">Eleni Chatzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JvVFSmFV8G-details-602" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JvVFSmFV8G-details-602"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, model-based reinforcement learning, deep learning, bayesian deep learning, gaussian processes, continuous control, model uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JvVFSmFV8G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ulj0tR-k7q" data-number="3070">
        <h4>
          <a href="https://openreview.net/forum?id=Ulj0tR-k7q">
              On strong convergence of the two-tower model for recommender system
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ulj0tR-k7q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~SHIRONG_XU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SHIRONG_XU1">SHIRONG XU</a>, <a href="https://openreview.net/profile?id=~Junhui_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junhui_Wang3">Junhui Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ulj0tR-k7q-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ulj0tR-k7q-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Artificial neural networks, Collaborative filtering, Empirical process, Recommender system, Two-tower model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recommender system is capable of predicting preferred items for a user by integrating information from similar users or items. A popular model in recommender system is the so-called two-tower model, which employs two deep neural networks to embed users and items into a low-dimensional space, and predicts ratings via the geometrical relationship of the embeddings of user and item in the embedded space. Even though it is popularly used for recommendations, its theoretical properties remain largely unknown. In this paper, we establish some asymptotic results of the two-tower model in terms of its strong convergence to the optimal recommender system, showing that it achieves a fast convergence rate depending on the intrinsic dimensions of inputs features. To the best of our knowledge, this is among the first attempts to establish the statistical guarantee of the two-tower model. Through numerical experiments, we also demonstrate that the two-tower model is capable of capturing the effects of users' and items' features on ratings, leading to higher prediction accuracy over its competitors in both simulated examples and a real application data set. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Establishing theoretical guarantee for the two-tower model in recommender system</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ulj0tR-k7q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UquMPXFTpgp" data-number="2976">
        <h4>
          <a href="https://openreview.net/forum?id=UquMPXFTpgp">
              Cluster Tree for Nearest Neighbor Search
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UquMPXFTpgp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dan_Kushnir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Kushnir1">Dan Kushnir</a>, <a href="https://openreview.net/profile?id=~Sandeep_Silwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandeep_Silwal1">Sandeep Silwal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UquMPXFTpgp-details-391" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UquMPXFTpgp-details-391"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nearest neighbor search, tree algorithms, graph cuts, random projections</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Tree-based algorithms are an important and widely used class of algorithms for  Nearest Neighbor Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in spite of possessing theoretical guarantees and strong practical performance, a major drawback of the RP tree is its lack of adaptability to the input dataset.
        
        Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing ClusterTree, a new tree based algorithm. Our approach utilizes randomness as in RP trees while adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees and other tree based methods for NNS while maintaining efficient construction time. In addition, we show theoretically and empirically that ClusterTree finds partitions which are superior to those found by RP trees in preserving the cluster structure of the input dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel tree-based algorithm for nearest neighbor search which adapts to the cluster structure of the input dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xGZcxaYbJBF" data-number="2193">
        <h4>
          <a href="https://openreview.net/forum?id=xGZcxaYbJBF">
              A Multi-Task Learning Algorithm for Non-personalized Recommendations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xGZcxaYbJBF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Zhang8">Jiawei Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xGZcxaYbJBF-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xGZcxaYbJBF-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Recommendation and Ranking, Non-personalized Recommendations, Multitask Learning, collaborative filtering, Two-tower DNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce a multi-task learning (MTL) algorithm for recommending non-personalized videos to watch next on industrial video sharing platforms. Personalized recommendations have been studied for decades, while researches on non-personalized solutions are very rare to be seen, which still remain a huge portion in industry. As an indispensable part in recommender system, non-personalized video recommender system also faces several real-world challenges, including maintaining high relevance between source item and target items, as well as achieving multiple competing ranking objectives. To solve these, we largely extended model-based collaborative filtering algorithm by adding related candidate generation stage, Two-tower DNN structure and a multi-task learning mechanism. Compared with typical baseline solutions, our proposed algorithm can capture both linear and non-linear relationships from user-item interactions, and live experiments demonstrate that it can significantly advance the state of the art on recommendation quality.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A multi-task learning (MTL) algorithm is introduced for recommending non-personalized videos to watch next on industrial video sharing platforms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XTzAhbVbKgq" data-number="2123">
        <h4>
          <a href="https://openreview.net/forum?id=XTzAhbVbKgq">
              Batched Lipschitz Bandits
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XTzAhbVbKgq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yasong_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yasong_Feng1">Yasong Feng</a>, <a href="https://openreview.net/profile?id=~Zengfeng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zengfeng_Huang1">Zengfeng Huang</a>, <a href="https://openreview.net/profile?id=~Tianyu_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyu_Wang4">Tianyu Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XTzAhbVbKgq-details-375" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XTzAhbVbKgq-details-375"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-armed bandits, online learning, batched bandits, Lipschitz bandits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we study the batched Lipschitz bandit problem, where the expected reward is Lipschitz and the reward observations are collected in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback setting. In particular, we show that for a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="102" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>-step problem with Lipschitz reward of zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="103" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, our algorithm achieves theoretically optimal regret rate of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="104" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.509em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.501em; margin-left: 0.056em;"><mjx-texatom size="s" texclass="ORD"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mi>T</mi><mrow data-mjx-texclass="ORD"><mfrac><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>2</mn></mrow></mfrac></mrow></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> using only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="105" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" style="font-size: 83.3%;"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mrow><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>T</mi></mrow><msub><mi>d</mi><mi>z</mi></msub></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> batches. For the lower bound, we show that in an environment with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="106" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>-batches, for any policy <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="107" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></mjx-assistive-mml></mjx-container>, there exists a problem instance such that the expected regret is lower bounded by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="108" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.361em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c28" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 1.233em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-msup><mjx-mrow><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.763em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c29" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi mathvariant="normal">Ω</mi><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow><mi>B</mi></msup></mrow></mfrac></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="109" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="110" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="111" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the dimension of the arm space. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel landscape-aware algorithm to solve the batched Lipschitz bandit problem, and show that our algorithm matches the optimal regret upper bound using less than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="112" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> batches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XTzAhbVbKgq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CdqsSPLNx-" data-number="2094">
        <h4>
          <a href="https://openreview.net/forum?id=CdqsSPLNx-">
              Deep Dynamic Attention Model with Gate Mechanism for Solving Time-dependent Vehicle Routing Problems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CdqsSPLNx-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Feng_Guo7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Guo7">Feng Guo</a>, <a href="https://openreview.net/profile?id=~Qu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qu_Wei1">Qu Wei</a>, <a href="https://openreview.net/profile?id=~Miao_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miao_Wang2">Miao Wang</a>, <a href="https://openreview.net/profile?id=~Zhaoxia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhaoxia_Guo1">Zhaoxia Guo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CdqsSPLNx--details-819" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CdqsSPLNx--details-819"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vehicle routing problems (VRPs) are a type of classical combinatorial optimization problems widely existing in logistics and transportation operations. There has been an increasing interest to use deep reinforcement learning (DRL) techniques to tackle VRPs, and previous DRL-based studies assumed time-independent travel times between customers. However, travel times in real-world road networks are time-varying, which need to be considered in practical VRPs. We thus propose a Deep Dynamic Attention Models with Gate Mechanisms (DDAM-GM) to learn heuristics for time-dependent VRPs (TDVRPs) in real-world road networks. It extracts the information of node location, node demand, and time-varying travel times between nodes to obtain enhanced node embeddings through a dimension-reducing MHA layer and a synchronous encoder. In addition, we use a gate mechanism to obtain better context embedding. On the basis of a 110-day travel time dataset with 240 time periods per day from an urban road network with 408 nodes and 1250 directed links, we conduct a series of experiments to validate the effectiveness of the proposed model on TDVRPs without and with consideration of time windows, respectively. Experimental results show that our model outperforms significantly two state-of-the-art DRL-based models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lDvJM5XUyrx" data-number="1436">
        <h4>
          <a href="https://openreview.net/forum?id=lDvJM5XUyrx">
              Towards Understanding Catastrophic Overfitting in Fast Adversarial Training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lDvJM5XUyrx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Renjie_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Renjie_Chen2">Renjie Chen</a>, <a href="https://openreview.net/profile?id=~Yuan_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan_Luo1">Yuan Luo</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisen_Wang1">Yisen Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lDvJM5XUyrx-details-600" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lDvJM5XUyrx-details-600"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robustness, Fast Adversarial Training, Catastrophic Overfitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">After adversarial training was proposed, a series of works focus on improving the compunational efficiency of adversarial training for deep neural networks (DNNs). Recently, FGSM based single-step adversarial training has been found to be able to train a robust model with the robustness comparable to the one trained by multi-step PGD, but it is an order of magnitude faster. However, there exists a failure mode called Catastrophic Overfitting (CO) where the network loses its robustness against PGD attack suddenly and can be hardly recovered by itself during the training process. In this paper, we identify that CO is closely related to the high-order terms in Taylor expansion after rethinking and decomposing the min-max problem in adversarial training. The negative high-order terms lead to a phenomenon called Perturbation Loss Distortion, which is the underlying cause of CO. Based on the observations, we propose a simple but effective regularization method named Fast Linear Adversarial Training (FLAT) to avoid CO in the single-step adversarial training by making the loss surface flat.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Fast Linear Adversarial Training (FLAT) can help prevent the Catastrophic Overfitting in single-step adversarial training.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lDvJM5XUyrx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZVqsBl2HapR" data-number="1434">
        <h4>
          <a href="https://openreview.net/forum?id=ZVqsBl2HapR">
              Error-based or target-based? A unifying framework for learning in recurrent spiking networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZVqsBl2HapR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cristiano_Capone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cristiano_Capone1">Cristiano Capone</a>, <a href="https://openreview.net/profile?id=~Paolo_Muratore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paolo_Muratore1">Paolo Muratore</a>, <a href="https://openreview.net/profile?id=~Pier_Stanislao_Paolucci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pier_Stanislao_Paolucci1">Pier Stanislao Paolucci</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZVqsBl2HapR-details-915" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZVqsBl2HapR-details-915"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">target-based, error-based, recurrent neural network, spiking neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in biological or artificial networks means changing the laws governing the network dynamics in order to better behave in a specific situation. In the field of supervised learning, two complementary approaches stand out: error-based and target-based learning. However, there exists no consensus on which is better suited for which task, and what is the most biologically plausible. Here we propose a comprehensive theoretical framework that includes these two frameworks as special cases. This novel theoretical formulation offers major insights into the differences between the two approaches. In particular, we show how target-based naturally emerges from error-based when the number of constraints on the target dynamics, and as a consequence on the internal network dynamics, is comparable to the degrees of freedom of the network. Moreover, given the experimental evidences on the relevance that spikes have in biological networks, we investigate the role of coding with specific patterns of spikes by introducing a parameter that defines the tolerance to precise spike timing during learning. Our approach naturally lends itself to Imitation Learning (and Behavioral Cloning in particular) and we apply it to solve relevant closed-loop tasks such as the button-and-food task, and the 2D Bipedal Walker. We show that a high dimensionality feedback structure is extremely important when it is necessary to solve a task that requires retaining memory for a long time (button-and-food). On the other hand, we find that coding with specific patterns of spikes enables optimal performances in a motor task (the 2D Bipedal Walker). Finally, we show that our theoretical formulation suggests protocols to deduce the structure of learning feedback in biological networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new learning framework that includes target- and error-based approches as special cases. It allows to understand their relationship and to explore what learning rule is optimal in the different tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZVqsBl2HapR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="x8l2miKNqPb" data-number="1409">
        <h4>
          <a href="https://openreview.net/forum?id=x8l2miKNqPb">
              Generate Triggers  in Neural Relation Extraction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=x8l2miKNqPb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Yujiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Yujiang1">Liu Yujiang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#x8l2miKNqPb-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x8l2miKNqPb-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">relation triggers，evolutive mask， pointer network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the relation extraction task, the relationship between two entities is determined by some specific words in their source text. These words are called relation triggers, which are the evidence to explain the relationship; other words are called ir-relevant words. The current relationship extraction neural network model aims at identifying the relation type between two entities mentioned in source text by encoding the text and entities. However, these models cannot output the relation triggers, but only gives the result of relation classification. Although models can generate weights for every single word through the improvement of attention mechanism, the weights will be affected by irrelevant words essentially, which are not required by the relation extraction task. In order to output re-lation triggers accurately, we propose a novel training frame-work for Relation Extraction (RE) that reduces the negative effect of irrelevant words on them in the encoding stage. In specific, we leverage Evolutive Mask based Point Network (EMPN) as a decoder to generate relation triggers and encode these words again. For an ordered output in relation triggers, we utilize order loss to constrain the output order in them. Ex-tensive experiment results demonstrate that the effectiveness of our proposed model achieves state-of-the-art performance on three RE benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Predict relation type and generate trigger words to make the results reasonable with Evolutive Mask based Point Network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=x8l2miKNqPb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IPwwNwMvHFW" data-number="1384">
        <h4>
          <a href="https://openreview.net/forum?id=IPwwNwMvHFW">
              Multi-Agent Decentralized Belief Propagation on Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IPwwNwMvHFW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yitao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yitao_Chen1">Yitao Chen</a>, <a href="https://openreview.net/profile?id=~Deepanshu_Vasal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepanshu_Vasal1">Deepanshu Vasal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IPwwNwMvHFW-details-428" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IPwwNwMvHFW-details-428"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">I-pomdps, Belief propagation, Multi-agent control</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the problem of interactive partially observable Markov decision processes (I-POMDPs),where the agents are located at the nodes of a communication network.  Specifically, we assume a certain message type for all messages.  Moreover, each agent makes individual decisions based on the interactive belief states, the information observed locally and the messages received from its neighbors over the network.Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors.  We propose a decentralized belief propagation algorithm for the problem,  and prove the convergence of our algorithm.Finally we show multiple applications of our framework. Our work appears to be the first study of decentralized belief propagation algorithm for networked multi-agent I-POMDPs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a methodology to do multi agent belief propagation on grahps</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PU3VGS93gxD" data-number="1202">
        <h4>
          <a href="https://openreview.net/forum?id=PU3VGS93gxD">
              Sample Complexity of Deep Active Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PU3VGS93gxD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhao_Song6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Song6">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Baocheng_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Baocheng_Sun1">Baocheng Sun</a>, <a href="https://openreview.net/profile?id=~Danyang_Zhuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danyang_Zhuo1">Danyang Zhuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PU3VGS93gxD-details-988" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PU3VGS93gxD-details-988"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning algorithms require large numbers of labeled training data to deliver state-of-the-art results. However, in many domains of AI, there are abundant unlabeled data but it is costly to get data labeled by experts, such as medical diagnosis and fraud detection. In these domains, active learning, where an algorithm maximizes model accuracy while requiring the least number of labeled data, is appealing.
        Active learning uses both labeled and unlabeled data to train models, and the learning algorithm decides which subset of data should acquire labels.
        Due to the costly label acquisition, it is interesting to know whether it is possible from a theoretical perspective to understand how many labeled data are actually needed to train a machine learning model. This question is known as the sample complexity problem, and it has been extensively explored for training linear machine learning models (e.g., linear regression). Today, deep learning has become the de facto method for machine learning, but the sample complexity problem for deep active learning remains unsolved. This problem is challenging due to the non-linear nature of neural networks.
        In this paper, we present the first deep active learning algorithm which has a provable sample complexity. Using this algorithm, we have derived the first upper bound on the number of required labeled data for training neural networks. 
        Our upper bound shows that the minimum number of labeled data a neural net needs does not depend on the data distribution or the width of the neural network but is determined by the smoothness of non-linear activation and the dimension of the input data.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove the first upper bound on the sample complexity of active learning for training one-hidden layer neural networks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PU3VGS93gxD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xO4xryFQltO" data-number="943">
        <h4>
          <a href="https://openreview.net/forum?id=xO4xryFQltO">
              A new perspective on probabilistic image modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xO4xryFQltO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Gepperth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Gepperth1">Alexander Gepperth</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xO4xryFQltO-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xO4xryFQltO-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep mixture models, sum-product networks, probabilistic circuits, image modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new probabilistic approach for image modeling capable of density estimation, sampling and tractable inference. DCGMM instances exhibit a CNN-like layered structure, in which the principal building  blocks are convolutional Gaussian Mixture (cGMM) layers. A key innovation w.r.t. related models lile sum-produdct networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer optimizes an independent loss function and therefore has an independent probabilistic interpretation. This modular approach permits intervening transformation layers to harness the full spectrum of 
        (potentially non-invertible) mappings available to CNNs, e.g., max-pooling or (half-)convolutions. DCGMM sampling and inference are realized by a deep chain of hierarchical priors, where samples generated by each cGMM layer parameterize sampling in the next-lower cGMM layer. For sampling through non-invertible transformation layers, we introduce a new gradient-based sharpening technique that exploits redundancy (overlap) in, e.g., half-convolutions. The basic quantities forward-transported through a DCGMM instance are the posterior probabilities of cGMM layers, which ensures numerical stability and facilitates the selection of learning rates.
        DCGMMs can be trained end-to-end by SGD from random initial conditions, much like CNNs. We experimentally show that DCGMMs compare favorably to several recent PC and SPN models in terms of inference, classification and sampling, the latter particularly for challenging datasets such as SVHN. A public TF2 implementation is provided as well.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A conceptually new approach for probabilistic image modeling based on multiple linked GMMs, which can generate samples of excellent quality w.r.t. related approaches, particularly for SVHN.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UajXTGRjuKB" data-number="840">
        <h4>
          <a href="https://openreview.net/forum?id=UajXTGRjuKB">
              Sampling Before Training: Rethinking the Effect of Edges in the Process of Training Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UajXTGRjuKB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hengyuan_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hengyuan_Ma1">Hengyuan Ma</a>, <a href="https://openreview.net/profile?email=xianmu.yq%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="xianmu.yq@antgroup.com">Qi Yang</a>, <a href="https://openreview.net/profile?email=wenxi.sbw%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="wenxi.sbw@antgroup.com">Bowen Sun</a>, <a href="https://openreview.net/profile?email=shunlong.wxd%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shunlong.wxd@antgroup.com">Long Shun</a>, <a href="https://openreview.net/profile?email=kui.lijk%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="kui.lijk@antgroup.com">Junkui Li</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Feng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianfeng_Feng2">Jianfeng Feng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UajXTGRjuKB-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UajXTGRjuKB-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNN) demonstrate excellent performance on many graph-based tasks; however, they also impose a heavy computational burden when trained on a large-scale graph. Although various sampling methods have been proposed to speed up training GNN by shrinking the scale of the graph during training, they become unavailable if we need to perform sampling before training. In this paper, we quantify the importance of every edge for training in the graph with the extra information they convey in addition to the node features, as inspired by a manifold learning algorithm called diffusion map. Based on this calculation, we propose Graph Diffusion Sampling (GDS), a simple but effective sampling method for shrinking the size of the edge set before training. GDS prefers to sample edges with high importance, and edges dropped by GDS will never be used in the training procedure. We empirically show that GDS preserves the edges crucial for training in a variety of models (GCN, GraphSAGE, GAT, and JKNet). Compared to training on the full graph, GDS can guarantee the performance of the model while only samples a small fraction of the edges.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UajXTGRjuKB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xREjEGUoY4c" data-number="761">
        <h4>
          <a href="https://openreview.net/forum?id=xREjEGUoY4c">
              Robot Intent Recognition Method Based on State Grid Business Office
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xREjEGUoY4c" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lanfang_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lanfang_Dong1">Lanfang Dong</a>, <a href="https://openreview.net/profile?id=~Zhao_Pu_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Pu_Hu1">Zhao Pu Hu</a>, <a href="https://openreview.net/profile?id=~Hanchao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanchao_Liu1">Hanchao Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xREjEGUoY4c-details-785" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xREjEGUoY4c-details-785"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial intelligence is currently in an era of change, not only changing the artificial intelligence technology itself, but also changing human society. It has become more and more common to use artificial intelligence as the core human-computer interaction technology to replace manpower. Intention recognition is an important part of the human-machine dialogue system, and deep learning technology is gradually being applied to the task of intent recognition. However, intent recognition based on deep learning often has problems such as low recognition accuracy and slow recognition speed. In response to these problems, this paper designs a BERT fine-tuning to improve the network structure based on the pre-training model and proposes new continuous pre-training goals. To improve the accuracy of intent recognition, a method based on multi-teacher model compression is proposed to compress the pre-training model, which reduces the time consumption of model inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gTdmGt48ht1" data-number="691">
        <h4>
          <a href="https://openreview.net/forum?id=gTdmGt48ht1">
              On the Double Descent of Random Features Models Trained with SGD
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gTdmGt48ht1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fanghui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fanghui_Liu1">Fanghui Liu</a>, <a href="https://openreview.net/profile?id=~Johan_Suykens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johan_Suykens1">Johan Suykens</a>, <a href="https://openreview.net/profile?id=~Volkan_Cevher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Volkan_Cevher1">Volkan Cevher</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gTdmGt48ht1-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gTdmGt48ht1-details-805"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random features, over-parameterized model, double descent, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study generalization properties of random features (RF) regression in high dimensions  optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds of RF regression under both constant and adaptive step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well in the interpolation setting, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm interpolator, as a theoretical justification of using SGD in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that, random features models trained with SGD in high dimensions still generalizes well for interpolation learning, recovers double descent, and incurs no loss in the excess risk when compared to the exact closed-form solution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fG9WttDhAaa" data-number="4723">
        <h4>
          <a href="https://openreview.net/forum?id=fG9WttDhAaa">
              Rethinking Positional Encoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fG9WttDhAaa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jianqiao_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianqiao_Zheng1">Jianqiao Zheng</a>, <a href="https://openreview.net/profile?id=~Sameera_Ramasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sameera_Ramasinghe1">Sameera Ramasinghe</a>, <a href="https://openreview.net/profile?email=simon.lucey%40adelaide.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="simon.lucey@adelaide.edu.au">Simon Lucey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fG9WttDhAaa-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fG9WttDhAaa-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions.  Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=fG9WttDhAaa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LZVXOnSrD0Y" data-number="4720">
        <h4>
          <a href="https://openreview.net/forum?id=LZVXOnSrD0Y">
              Pareto Frontier Approximation Network (PA-Net) Applied to Multi-objective TSP
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LZVXOnSrD0Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ishaan_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ishaan_Mehta1">Ishaan Mehta</a>, <a href="https://openreview.net/profile?email=s.saeedi%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="s.saeedi@ryerson.ca">Sajad Saeedi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LZVXOnSrD0Y-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LZVXOnSrD0Y-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotics, planning, TSP, RL, Multi Objective Optimization, Pareto Optimality</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-objective optimization is used in various areas of robotics like control, planning etc. Their solutions are dependent on multiple objective functions, which can be conflicting in nature. In such cases, the optimality is defined in terms of Pareto optimality. A set of these Pareto Optimal solutions in the objective space form a Pareto front (or frontier). Each solution has its own trade off. For instance, the travelling salesman problem (TSP) is used in robotics for task/resource allocation. Often this allocation is influenced by multiple objective functions and is solved using Multi-objective travelling salesman problem (MOTSP). In this work, we present PA-Net, a network that generates good approximations of the Pareto front for the multi-objective optimization problems. Our training framework is applicable to other multi-objective optimization problems; however, in this work, we focus on solving MOTSP.  Firstly, MOTSP is converted into a constrained optimization problem. We then train our network to solve this constrained problem using the Lagrangian relaxation and policy gradient. With PA-Net we are able to generate better quality Pareto fronts with fast inference times as compared to other learning based and classical methods. Finally, we present the application of PA-Net to find optimal visiting order in coverage planning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">PA-Net: a network that approximates Pareto Frontier for Multi Objective TSP problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LZVXOnSrD0Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aJORhCrlYqu" data-number="4718">
        <h4>
          <a href="https://openreview.net/forum?id=aJORhCrlYqu">
              ARMCMC:  Online Bayesian Density Estimation of Model Parameters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aJORhCrlYqu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedram_Agand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedram_Agand1">Pedram Agand</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?id=~Hamid_Taghirad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hamid_Taghirad1">Hamid Taghirad</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aJORhCrlYqu-details-622" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aJORhCrlYqu-details-622"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian, Probabilistic approaches, MCMC, Hunt Crossley, parameter identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although the Bayesian paradigm provides a rigorous framework to estimate the full probability distribution over unknown parameters,  its online  implementation can be challenging due to heavy computational costs. This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches. These shortcomings include: being solely able to account for Gaussian noise, being applicable to systems with linear in the parameters (LIP) constraint, or having requirements on persistence excitation (PE). In ARMCMC, we propose a variable jump distribution, which depends on a temporal forgetting factor.  This allows one to adjust the trade-off between exploitation and exploration, depending on whether there is an abrupt change to the parameter being estimated. We prove that ARMCMC requires fewer samples to achieve the same precision and reliability compared to conventional MCMC approaches.  We demonstrate our approach on two challenging benchmarks:  the estimation of parameters in a soft bending actuator and the Hunt-Crossley dynamic model. Our method shows at-least 70\% improvement in parameter point estimation accuracy and approximately 55\% reduction in tracking error of the value of interest compared to recursive least squares and conventional MCMC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=aJORhCrlYqu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Xd6T7cT7vwj" data-number="4673">
        <h4>
          <a href="https://openreview.net/forum?id=Xd6T7cT7vwj">
              Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Xd6T7cT7vwj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marcus_L%C3%A5ng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marcus_Lång1">Marcus Lång</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Xd6T7cT7vwj-details-839" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xd6T7cT7vwj-details-839"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent studies have show that wide neural networks with orthogonal linear layers and Gaussian Poincaré normalized activation functions avoid vanishing and exploding gradients for input vectors with the correct magnitude. This paper introduces a strengthening of the condition that the activation function must be Gaussian Poincaré normalized which creates robustness to deviations from standard normal distribution in the pre-activations, thereby reducing the dependence on the requirement that the network is wide and that the input vector has the correct magnitude. In implicit representation learning this allows the training of deep networks of this type where the linear layers are no longer constrained to be orthogonal linear transformations. Networks of this type can be fitted to a reference image to 1/10th the mean square error achievable with previous methods. Herein is also given an improved positional encoding for implicit representation learning of two-dimensional images and a small-batch training procedure for fitting of neural networks to images which allows fitting in fewer epochs, leading to substantial improvement in training time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Similar to SIREN, but able to fit images to higher accuracy (PSNR=67 instead PSNR=50 for a specific reference image).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sHUFhv03qX_" data-number="4671">
        <h4>
          <a href="https://openreview.net/forum?id=sHUFhv03qX_">
              Q-Learning Scheduler for Multi-Task Learning through the use of Histogram of Task Uncertainty
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sHUFhv03qX_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kourosh_Meshgi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Meshgi2">Kourosh Meshgi</a>, <a href="https://openreview.net/profile?id=~Maryam_Sadat_Mirzaei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maryam_Sadat_Mirzaei1">Maryam Sadat Mirzaei</a>, <a href="https://openreview.net/profile?id=~Satoshi_Sekine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satoshi_Sekine1">Satoshi Sekine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sHUFhv03qX_-details-63" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sHUFhv03qX_-details-63"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Q-learning, Multi-Task Learning, MTL Scheduling, Histogram of Task Uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Simultaneous training of a multi-task learning network on different domains or tasks is not always straightforward. It could lead to inferior performance or generalization compared to the corresponding single-task networks. To maximally taking advantage of the benefits of multi-task learning, an effective training scheduling method is deemed necessary. Traditional schedulers follow a heuristic or prefixed strategy, ignoring the relation of the tasks, their sample complexities, and the state of the emergent shared features. We proposed a deep Q-Learning Scheduler (QLS) that monitors the state of the tasks and the shared features using a novel histogram of task uncertainty, and through trial-and-error, learns an optimal policy for task scheduling. Extensive experiments on multi-domain and multi-task settings with various task difficulty profiles have been conducted, the proposed method is benchmarked against other schedulers, its superior performance has been demonstrated, and results are discussed.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A deep Q-learning-based task scheduling method to improve multi-tasking learning based on a novel histogram of task uncertainty.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lTiW8Jet8t" data-number="4664">
        <h4>
          <a href="https://openreview.net/forum?id=lTiW8Jet8t">
              Efficient Ensembles of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lTiW8Jet8t" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Amrit_Nagarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amrit_Nagarajan1">Amrit Nagarajan</a>, <a href="https://openreview.net/profile?id=~Jacob_R._Stevens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_R._Stevens1">Jacob R. Stevens</a>, <a href="https://openreview.net/profile?id=~Anand_Raghunathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anand_Raghunathan1">Anand Raghunathan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lTiW8Jet8t-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lTiW8Jet8t-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) have enabled the power of deep learning to be applied to inputs beyond the Euclidean domain, with applications ranging from social networks and product recommendation engines to the life sciences. GNNs, like other classes of machine learning models, benefit from ensemble learning, wherein multiple models are combined to provide higher accuracy and robustness than single models. However, ensembles suffer from significantly higher inference processing and storage requirements, limiting their use in practical applications. In this work, we leverage the unique characteristics of GNNs to overcome these overheads, creating efficient ensemble GNNs that are faster than even single models at inference time. We observe that during message passing, nodes that are incorrectly classified (error nodes) also end up adversely affecting the representations of other nodes in their neighborhood. This error propagation also makes GNNs more difficult to approximate (e.g., through pruning) for efficient inference. We propose a technique to create ensembles of diverse models, and further propose Error Node Isolation (ENI), which prevents error nodes from sending messages to (and thereby influencing) other nodes. In addition to improving accuracy, ENI also leads to a significant reduction in the memory footprint and the number of arithmetic operations required to evaluate the computational graphs of all neighbors of error nodes. Remarkably, these savings outweigh even the overheads of using multiple models in the ensemble. A second key benefit of ENI is that it  enhances the resilience of GNNs to approximations. Consequently, we propose Edge Pruning and Network Pruning techniques that target both the input graph and the neural networks used to process the graph. Our experiments on GNNs for transductive and inductive node classification demonstrate that ensembles with ENI are simultaneously more accurate (by up to 4.6% and 3.8%) and  faster (by up to 2.8<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="113" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> and 5.7<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="114" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container>) when compared to the best-performing single models and ensembles without ENI, respectively. In addition, GNN ensembles with ENI are consistently more accurate than single models and ensembles without ENI when subject to pruning, leading to additional speedups of up to 5<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="115" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> with no loss in accuracy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ndffz5uo6H" data-number="4656">
        <h4>
          <a href="https://openreview.net/forum?id=Ndffz5uo6H">
              Updater-Extractor Architecture for Inductive World State Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ndffz5uo6H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arsenii_Kirillovich_Moskvichev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arsenii_Kirillovich_Moskvichev1">Arsenii Kirillovich Moskvichev</a>, <a href="https://openreview.net/profile?id=~James_A_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_A_Liu1">James A Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ndffz5uo6H-details-122" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ndffz5uo6H-details-122"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformers, long-term-memory, sequential processing, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Developing sequential models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure. Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Proposing a theoretically and practically justified way to introduce persistent world state representations into transformer architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ndffz5uo6H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ys-bh0Eer_" data-number="4654">
        <h4>
          <a href="https://openreview.net/forum?id=ys-bh0Eer_">
              Block Contextual MDPs for Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ys-bh0Eer_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Franziska_Meier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Franziska_Meier2">Franziska Meier</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joelle_Pineau1">Joelle Pineau</a>, <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amy_Zhang1">Amy Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ys-bh0Eer_-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ys-bh0Eer_-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, MDP, Block Contextual MDP, Continual Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In reinforcement learning (RL), when defining a Markov Decision Process (MDP), the environment dynamics is implicitly assumed to be stationary. This assumption of stationarity, while simplifying, can be unrealistic in many scenarios. In the continual reinforcement learning scenario, the sequence of tasks is another source of nonstationarity. In this work, we propose to examine this continual reinforcement learning setting through the block contextual MDP (BC-MDP) framework, which enables us to relax the assumption of stationarity. This framework challenges RL algorithms to handle both nonstationarity and rich observation settings and, by additionally leveraging smoothness properties, enables us to study generalization bounds for this setting. Finally, we take inspiration from adaptive control to propose a novel algorithm that addresses the challenges introduced by this more realistic BC-MDP setting, allows for zero-shot adaptation at evaluation time, and achieves strong performance on several nonstationary environments.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce the Lipschitz Block Contextual MDP framework for the continual RL setting and propose a representation learning algorithm that enables RL agents to generalize to non-stationary environments.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FeaitX_a5Av" data-number="4645">
        <h4>
          <a href="https://openreview.net/forum?id=FeaitX_a5Av">
              GSD: Generalized Stochastic Decoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FeaitX_a5Av" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ning_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ning_Gong1">Ning Gong</a>, <a href="https://openreview.net/profile?id=~Nianmin_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nianmin_Yao1">Nianmin Yao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FeaitX_a5Av-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FeaitX_a5Av-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Processing, Decoding Algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although substantial progress has been made in various text generation tasks, there remains a vast gap between current generations and human languages. One reason is that virtually all decoding methods currently developed are pragmatic to address the text degeneration problem, which exists in both deterministic and stochastic decoding algorithms. So, why text generated from these algorithms are divergent? What is the critical difference between these algorithms? Moreover, is it possible to design a generalized framework where existing decoding algorithms can be naturally connected, uniformly described, and mutually inspired?
        In this paper, we try to explore answers to these intriguing questions. Correctly, we propose a generalized decoding framework that can be used to describe and connect existing popular decoding algorithms. Based on the framework, we propose a novel implementation with a distinctive core from existing decoding algorithms. As far as we know, this is the first work trying to propose a generalized framework to bridge these decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms. Experiments show that text produced by our method is closest to the characteristics of human languages. Source code and the generated text can be accessed from https://github.com/ginoailab/gsd.git.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel work proposing a generalized framework to connect existing decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JgmY4TUgznC" data-number="4641">
        <h4>
          <a href="https://openreview.net/forum?id=JgmY4TUgznC">
              Few-Shot Multi-task Learning via Implicit regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JgmY4TUgznC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsung_Huh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongsung_Huh1">Dongsung Huh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JgmY4TUgznC-details-11" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JgmY4TUgznC-details-11"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Few Shot Learning, Learning Instability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern machine learning is highly data-intensive. Few-shot learning (FSL) aims to resolve this sample efficiency problem by learning from multiple tasks and quickly adapt to new tasks containing only a few samples. However,  FSL problems proves to be significantly more challenging and require more compute expensive process to optimize. In this work, we consider multi-task linear regression (MTLR) as a canonical problem for few-shot learning, and investigate the source of challenge of FSL. We find that the MTLR exhibits local minimum problems that are not present in single-task problem, and thus making the learning much more challenging. We also show that the problem can be resolved by  overparameterizing the  model by increasing both the width and depth of the linear network and initializing the weights with small values, exploiting the implicit regularization bias of gradient descent-based learning.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UzOEYQM-xTg" data-number="4635">
        <h4>
          <a href="https://openreview.net/forum?id=UzOEYQM-xTg">
              Robust Long-Tailed Learning under Label Noise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UzOEYQM-xTg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tong_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tong_Wei1">Tong Wei</a>, <a href="https://openreview.net/profile?id=~Jiang-Xin_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiang-Xin_Shi1">Jiang-Xin Shi</a>, <a href="https://openreview.net/profile?id=~Wei-Wei_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei-Wei_Tu1">Wei-Wei Tu</a>, <a href="https://openreview.net/profile?id=~Yu-Feng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Feng_Li1">Yu-Feng Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UzOEYQM-xTg-details-779" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UzOEYQM-xTg-details-779"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weakly-supervised learning, long-tailed learning, learning with noisy labels, semi-supervised learning, multi-label learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework,~\algo, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on both benchmark and real-world datasets demonstrate substantial improvement over many existing methods. For example, \algo\ outperforms baselines by more than 5\% in test accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UzOEYQM-xTg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a3NaSCJ20V" data-number="4634">
        <h4>
          <a href="https://openreview.net/forum?id=a3NaSCJ20V">
              Equivariant Grasp learning In Real Time
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a3NaSCJ20V" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xupeng_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xupeng_Zhu1">Xupeng Zhu</a>, <a href="https://openreview.net/profile?id=~Dian_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dian_Wang1">Dian Wang</a>, <a href="https://openreview.net/profile?id=~Ondrej_Biza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ondrej_Biza1">Ondrej Biza</a>, <a href="https://openreview.net/profile?id=~Robert_Platt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_Platt1">Robert Platt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a3NaSCJ20V-details-55" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a3NaSCJ20V-details-55"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotic Grasping, Equivariance, Reinforcement Leanring</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual grasp detection is a key problem in robotics where the agent must learn to model the grasp function, a mapping from an image of a scene onto a set of feasible grasp poses. In this paper, we recognize that the grasp function is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="116" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">SE</mi></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>-equivariant and that it can be modeled using an equivariant convolutional neural network. As a result, we are able to significantly improve the sample efficiency of grasp learning to the point where we can learn a good approximation of the grasp function within only 500 grasp experiences. This is fast enough that we can learn to grasp completely on a physical robot in about an hour. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="I13PP8-cdvz" data-number="4632">
        <h4>
          <a href="https://openreview.net/forum?id=I13PP8-cdvz">
              SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=I13PP8-cdvz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheng_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheng_Cheng1">Sheng Cheng</a>, <a href="https://openreview.net/profile?id=~Yi_Ren3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Ren3">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yezhou_Yang1">Yezhou Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#I13PP8-cdvz-details-859" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I13PP8-cdvz-details-859"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stroke-based representation, Spatial robustness, Robust feature learning, Novel pattern generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing end-to-end visual recognition models do not possess innate spatial invariance and are thus vulnerable to out-of-training attacks. This suggests the need of a better representation design. This paper follows existing cognitive studies to investigate a sketch representation that specify stroke information on vertices and inter-stroke information on edges. The resultant representation, combined with a graph neural network, achieves both high classification accuracy and high robustness against translation, rotation, and stroke-wise parametric and topological attacks thanks to the use of spatially invariant stroke features and GNN architecture. While prior studies demonstrated similar sketch representations for classification and generation, these attempts heavily relied on run-time statistical inference rather than more efficient bottom-up computation via GNN. The presented sketch representation poses good structured expression capability as it enables generation of sketches semantically different from the training dataset.  Lastly, we show SSR-GNNs are able to accomplish all tasks (classification, robust feature learning, and novel pattern generation), which shows that the representation is task-agnostic. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper presents a Stroke-based Sketch Representation with Graph Neural Networks which is spatially robust, with structured expression capability and is task-agnostic.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RSd79AULOu" data-number="4625">
        <h4>
          <a href="https://openreview.net/forum?id=RSd79AULOu">
              Fairness-aware Federated Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RSd79AULOu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhuozhuo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuozhuo_Tu1">Zhuozhuo Tu</a>, <a href="https://openreview.net/profile?id=~zhiqiang_xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~zhiqiang_xu1">zhiqiang xu</a>, <a href="https://openreview.net/profile?id=~Tairan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tairan_Huang1">Tairan Huang</a>, <a href="https://openreview.net/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dacheng_Tao1">Dacheng Tao</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RSd79AULOu-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RSd79AULOu-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Learning Theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized. In such a setting, naively minimizing an aggregate loss may introduce bias and disadvantage model performance on certain clients. To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set. By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable. We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees. We further prove generalization bounds for learning with this objective. Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new framework to address the fairness issues in federated learning and provide theoretical guarantees.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="edqz84cQ79T" data-number="4622">
        <h4>
          <a href="https://openreview.net/forum?id=edqz84cQ79T">
              Shaping latent representations using Self-Organizing Maps with Relevance Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=edqz84cQ79T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedro_Braga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedro_Braga1">Pedro Braga</a>, <a href="https://openreview.net/profile?email=hrm%40cin.ufpe.br" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="hrm@cin.ufpe.br">Heitor Medeiros</a>, <a href="https://openreview.net/profile?id=~Hansenclever_Bassani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hansenclever_Bassani1">Hansenclever Bassani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#edqz84cQ79T-details-367" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="edqz84cQ79T-details-367"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Clustering, Learning Prototypes, Topological Representations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work indicates that Deep Clustering (DC) methods are a viable option for unsupervised representations learning of visual features. By combining representation learning and clustering, traditional approaches have been shown to build latent representations that capture essential features of the data while preserving topological characteristics. In this sense, models based on Self-Organizing Maps models with relevance learning (SOMRL) were considered as they perform well in clustering besides being able to create a map that learns the relevance of each input dimension for each cluster, preserving the original relations and topology of the data. We hypothesize that this type of model can produce a more intuitive and disentangled representation in the latent space by promoting smoother transitions between cluster points over time. This work proposes a representation learning framework that combines a new gradient-based SOMRL model and autoencoders. The SOMRL learns the relevance weights for each input dimension of each cluster. It creates a tendency to separate the information into subspaces. To achieve this, we designed a new loss function term that weighs these learned relevances and provides an estimated unsupervised error to be used in combination with a reconstruction loss. The model is evaluated in terms of clustering performance and quality of the learned representations and then compared with start-of-the-art models, showing competitive results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work proposes a representation learning framework that combines a new Self-Organizing Maps with autoencoders to shape their latent spaces into cluster prototypes living in separate subspaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=edqz84cQ79T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eAEcdRkcMHh" data-number="4611">
        <h4>
          <a href="https://openreview.net/forum?id=eAEcdRkcMHh">
              HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N:M Sparsity and Integer Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eAEcdRkcMHh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minjia_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minjia_Zhang1">Minjia Zhang</a>, <a href="https://openreview.net/profile?id=~Connor_Holmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Connor_Holmes1">Connor Holmes</a>, <a href="https://openreview.net/profile?id=~Yuxiong_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuxiong_He1">Yuxiong He</a>, <a href="https://openreview.net/profile?id=~Bo_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Wu1">Bo Wu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eAEcdRkcMHh-details-651" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eAEcdRkcMHh-details-651"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Efficient Inference, N:M Sparsification, Quantization, Transformer networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks. However, the huge size of these models brings significant challenges to fine-tuning and online deployment due to latency and cost constraints. Recently, hardware manufacturers have released new architectures that support efficient N:M sparsity and low-precision integer computation for fast inferencing. In contrast to unstructured sparsity, N:M sparsity specifies that out of each chunk of N contiguous weight parameters, exactly M parameters are non-zero. Moreover, these architectures also support processing data with reduced precision, such as INT8. Prior work often considers inducing N:M sparsity and integer quantization in isolation or as independent pieces of a compression pipeline. However, there lacks a systematic investigation towards how N:M sparsity and integer quantization can be effectively combined to exploit the maximum degree of redundancy and enable even faster acceleration for pre-trained Transformer networks.
        
        In this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM). We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and
        solved in a unified manner. When evaluated across the GLUE suite of NLP benchmarks, our approach outperforms baselines that consider each of these problems independently, retaining 99.4\% accuracy of the dense baseline while being able to execute on newly released hardware effectively. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">HoloFormer is a unified and systematic approach to learn N:M sparsity and integer quantization for compressing pre-trained Transformer networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ad_F_z27pCx" data-number="4595">
        <h4>
          <a href="https://openreview.net/forum?id=ad_F_z27pCx">
              A Discussion On the Validity of Manifold Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ad_F_z27pCx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dai_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dai_Shi1">Dai Shi</a>, <a href="https://openreview.net/profile?id=~Andi_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andi_Han1">Andi Han</a>, <a href="https://openreview.net/profile?id=~Yi_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Guo3">Yi Guo</a>, <a href="https://openreview.net/profile?id=~Junbin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junbin_Gao1">Junbin Gao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 30 Sept 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ad_F_z27pCx-details-132" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ad_F_z27pCx-details-132"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Manifold learning, Dimensionality Reduction, Computational Geometry, Simplicial Complex</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition (orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any -simplex decomposition result on a -manifold.  However,  constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SK1nec-Ehd" data-number="4584">
        <h4>
          <a href="https://openreview.net/forum?id=SK1nec-Ehd">
              PulseImpute: A Novel Benchmark Task and Architecture for Imputation of Physiological Signals
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SK1nec-Ehd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Maxwell_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maxwell_Xu1">Maxwell Xu</a>, <a href="https://openreview.net/profile?id=~Alexander_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Moreno1">Alexander Moreno</a>, <a href="https://openreview.net/profile?id=~James_Matthew_Rehg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Matthew_Rehg1">James Matthew Rehg</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SK1nec-Ehd-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SK1nec-Ehd-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">missingness, imputation, mHealth, sensors, transformer, self-attention</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Providing care for patients with chronic diseases is one of the biggest drivers of the nation’s rising healthcare costs, but many of these diseases are linked to mutable health behaviors. Mobile health (mHealth) biophysical sensors that continuously measure our current conditions provide the framework for a personalized guidance system for the maintenance of healthy behaviors. However, this physiological sensor data is plagued with missingness due to insecure attachments, wireless dropout, battery, and adherence issues. These issues cripple their rich diagnostic utility as well as their ability to enable temporally-precise interventions. While there is a sizable amount of research focusing on imputation methods, surprisingly, no works have addressed the patterns of missingness, quasi-periodic signal structure, and the between subject heterogeneity that characterizes physiological signals in mHealth applications. We present the PulseImpute Challenge, the first challenge dataset for physiological signal imputation which includes a large set of baselines' performances on realistic missingness models and data. Next, we demonstrate the potential to address this quasi-periodic structure and heterogeneity with our Dilated Convolution Bottleneck (DCB) Transformer, a transformer architecture with a self-attention mechanism that is able to attend to corresponding waveform features in quasi-periodic signals. By utilizing stacked dilated convolutions with bottleneck layers for query and key transformations, we visually demonstrate that the kernel similarity in the attention model gives high similarity to similar temporal features across quasi-periodic periods. We hope the release of our challenge task definitions and baseline implementations will spur the community to address this challenging and important problem. 
         </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present PulseImpute, a benchmarking challenge for the imputation of biophysical signals, and propose a novel self-attention module for attending over quasi-periodic signals.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dhLChxJwgMR" data-number="4578">
        <h4>
          <a href="https://openreview.net/forum?id=dhLChxJwgMR">
              HFSP: A Hardware-friendly Soft Pruning Framework for Vision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dhLChxJwgMR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhenglun_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhenglun_Kong1">Zhenglun Kong</a>, <a href="https://openreview.net/profile?id=~Peiyan_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peiyan_Dong1">Peiyan Dong</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaolong_Ma2">Xiaolong Ma</a>, <a href="https://openreview.net/profile?id=~Xin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Meng1">Xin Meng</a>, <a href="https://openreview.net/profile?id=~Mengshu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengshu_Sun1">Mengshu Sun</a>, <a href="https://openreview.net/profile?id=~Wei_Niu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Niu3">Wei Niu</a>, <a href="https://openreview.net/profile?id=~Bin_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Ren1">Bin Ren</a>, <a href="https://openreview.net/profile?id=~Minghai_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghai_Qin1">Minghai Qin</a>, <a href="https://openreview.net/profile?id=~Hao_Tang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Tang6">Hao Tang</a>, <a href="https://openreview.net/profile?id=~Yanzhi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanzhi_Wang3">Yanzhi Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dhLChxJwgMR-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dhLChxJwgMR-details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Hardware-friendly, Soft Token Pruning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViT, and the related edge device deployment, we leverage the input token sparsity and propose a hardware-friendly soft pruning framework (HFSP), which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique to package the pruned tokens, which integrate the less informative tokens generated by the selector module into a package token, and participates in subsequent calculations rather than being discarded completely.  From a hardware standpoint, our framework is bound to the tradeoff between accuracy and specific hardware constraints through our proposed hardware-oriented progressive training, and all the operators embedded in the framework have been well-supported. Experimental results demonstrate that the proposed framework significantly reduces the computational costs of ViTs while maintaining comparable performance on image classification. For example, our method reduces the FLOPs of DeiT-S by over 42.6% while only sacrificing 0.46% top-1 accuracy. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. Code will be publicly released.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A vision transformer pruning framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dhLChxJwgMR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="znpOLJUYGcA" data-number="4568">
        <h4>
          <a href="https://openreview.net/forum?id=znpOLJUYGcA">
              Automatic Integration for Neural Temporal Point Process
          </a>
        
          
            <a href="https://openreview.net/pdf?id=znpOLJUYGcA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zihao_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zihao_Zhou1">Zihao Zhou</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rose_Yu1">Rose Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#znpOLJUYGcA-details-34" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="znpOLJUYGcA-details-34"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point process</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integration lies at the heart of the temporal point process. Due to the intrinsic mathematical difficulty of symbolic integration, neural temporal point process models either constrain the intensity function to an integrable functional form or apply certain numerical methods. However, the former type of model has limited expressive power, and the latter type of model suffers additional numerical errors and high computational costs. In this paper, we introduce automatic integration with neural point process models, a new paradigm for efficient, closed-form nonparametric inference of temporal point process characterized by any intensity function. We test the model against a variety of synthetic temporal point process datasets and show that the model can better capture inter-event intensity changes than state-of-the-art methods. We also identify certain model settings that would lead the MLE estimator for the temporal point process to be inconsistent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Leveraging automatic integration for more efficient and accurate recovery of temporal point process's intensity</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZV7MoEj44Et" data-number="4556">
        <h4>
          <a href="https://openreview.net/forum?id=ZV7MoEj44Et">
              Measuring the Effectiveness of Self-Supervised Learning using Calibrated Learning Curves
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZV7MoEj44Et" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrei_Atanov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrei_Atanov1">Andrei Atanov</a>, <a href="https://openreview.net/profile?id=~Shijian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shijian_Xu1">Shijian Xu</a>, <a href="https://openreview.net/profile?email=onur.beker%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="onur.beker@epfl.ch">Onur Beker</a>, <a href="https://openreview.net/profile?id=~Andrey_Filatov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrey_Filatov1">Andrey Filatov</a>, <a href="https://openreview.net/profile?id=~Amir_Zamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Zamir1">Amir Zamir</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZV7MoEj44Et-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZV7MoEj44Et-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Self-Supervised Learning, Transfer Learning, Metric, Evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning has witnessed remarkable progress in recent years, in particular with the introduction of augmentation-based contrastive methods. While a number of large-scale empirical studies on the performance of self-supervised pre-training have been conducted, there isn't yet an agreed upon set of control baselines, evaluation practices, and metrics to report. We identify this as an important angle of investigation and propose an evaluation standard that aims to quantify and communicate transfer learning performance in an informative yet accessible setup. This is done by baking in a number of key control baselines in the evaluation method, particularly the blind guess (quantifying the dataset bias), the scratch model (quantifying the architectural contribution), and the gold standard (quantifying the upper-bound). We further provide a number of experiments to demonstrate how the proposed evaluation can be employed in empirical studies of basic questions -- for example, whether the effectiveness of existing self-supervised learning methods is skewed towards image classification versus other tasks, such as dense pixel-wise predictions. 
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an evaluation standard for measuring the effectiveness of self-supervised learning based on incorporating important control baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxTR9Z2DW8R" data-number="4552">
        <h4>
          <a href="https://openreview.net/forum?id=UxTR9Z2DW8R">
              Reinforcement Learning State Estimation for High-Dimensional Nonlinear Systems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxTR9Z2DW8R" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Saviz_Mowlavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saviz_Mowlavi1">Saviz Mowlavi</a>, <a href="https://openreview.net/profile?id=~Mouhacine_Benosman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mouhacine_Benosman1">Mouhacine Benosman</a>, <a href="https://openreview.net/profile?id=~Saleh_Nabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saleh_Nabi1">Saleh Nabi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxTR9Z2DW8R-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxTR9Z2DW8R-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, partial differential equation, reduced order modeling, closure models, state prediction, state estimation, dynamic mode decomposition.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RL-ROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="V2WidtMGSRG" data-number="4516">
        <h4>
          <a href="https://openreview.net/forum?id=V2WidtMGSRG">
              Provable Identifiability of ReLU Neural Networks via Lasso Regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=V2WidtMGSRG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gen_Li2">Gen Li</a>, <a href="https://openreview.net/profile?id=~Ganghua_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ganghua_Wang1">Ganghua Wang</a>, <a href="https://openreview.net/profile?id=~Yuantao_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuantao_Gu1">Yuantao Gu</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jie_Ding2">Jie Ding</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#V2WidtMGSRG-details-268" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V2WidtMGSRG-details-268"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Lasso, nonlinear regression, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">LASSO regularization is a popular regression tool to enhance the prediction accuracy of statistical models by performing variable selection through the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="117" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> penalty, initially formulated for the linear model and its variants. In this paper, the territory of LASSO is extended to the neural network model, a fashionable and powerful nonlinear regression model. Specifically, given a neural network whose output <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="118" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container> depends only on a small subset of input <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="119" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-b mjx-i"><mjx-c class="mjx-c1D499 TEX-BI"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math></mjx-assistive-mml></mjx-container>, denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="120" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>⋆</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, we prove that the LASSO estimator can stably reconstruct the neural network and identify <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="121" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>⋆</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> when the number of samples scales logarithmically with the input dimension. This challenging regime has been well understood for linear models while barely studied for neural networks. Our theory lies in an extended Restricted Isometry Property (RIP)-based analysis framework for two-layer ReLU neural networks, which may be of independent interest to other LASSO or neural network settings. Based on the result, we further propose a neural network-based variable selection method. Experiments on simulated and real-world datasets show the promising performance of our variable selection approach compared with classical techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We theoretically show that the Lasso estimator can stably identify ReLU neural networks and then propose to use neural networks as vehicles to perform variable selection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=V2WidtMGSRG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Jh9VxCkrEZn" data-number="4515">
        <h4>
          <a href="https://openreview.net/forum?id=Jh9VxCkrEZn">
              Spatiotemporal Representation Learning on Time Series with Dynamic Graph ODEs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Jh9VxCkrEZn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_6_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ming_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming_Jin3">Ming Jin</a>, <a href="https://openreview.net/profile?id=~Yuan-Fang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan-Fang_Li1">Yuan-Fang Li</a>, <a href="https://openreview.net/profile?id=~Yu_Zheng5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zheng5">Yu Zheng</a>, <a href="https://openreview.net/profile?id=~Bin_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Yang4">Bin Yang</a>, <a href="https://openreview.net/profile?id=~Shirui_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shirui_Pan1">Shirui Pan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Jh9VxCkrEZn-details-327" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jh9VxCkrEZn-details-327"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Spatiotemporal representation learning on multivariate time series has received tremendous attention in forecasting traffic and energy data. Recent works either rely on complicated discrete neural architectures or graph priors, hindering their effectiveness and applications in the real world. In this paper, inspired by neural ordinary differential equations and graph structure learning, we propose a fully continuous model named Dynamic Graph ODE (DyG-ODE) to capture both long-range spatial and temporal dependencies to learn expressive representations on arbitrary multivariate time series data without being restricted by rigid preconditions (e.g., graph priors). For modeling the continuous dynamics of spatiotemporal clues, we design a simple yet powerful dynamic graph ODE by coupling the proposed spatial and temporal ODEs, which not only allows the model to obtain infinite spatial and temporal receptive fields but also reduces numerical errors and model complexity significantly. Our empirical evaluations demonstrate the superior effectiveness and efficiency of DyG-ODE on a number of benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a fully continuous model named DyG-ODE to learn expressive spatiotemporal representations on arbitrary multivariate time series data</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>‹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">›</a>
      </li>
      <li class="  right-arrow" data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="recent-activity">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="https://openreview.net/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review, with legal nonprofit status through<!-- --> <a href="https://codeforscience.org/" target="_blank" rel="noopener noreferrer">Code for Science &amp; Society</a>. We gratefully acknowledge the support of the<!-- --> <a href="https://openreview.net/sponsors" target="_blank">OpenReview Sponsors</a>.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible. To submit a bug report or feature request, you can use the official OpenReview GitHub repository:<br><a href="https://github.com/openreview/openreview/issues/new/choose" target="_blank" rel="noreferrer">Report an issue</a></p><form><div class="form-group"><input type="email" id="feedback-from" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" id="feedback-subject" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea id="feedback-message" name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2022/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2022/Conference\n$(function() {\nvar args = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar group = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar document = null;\nvar window = null;\n\n// TODO: remove these vars when all old webfields have been archived\nvar model = {\n  tokenPayload: function() {\n    return { user: user }\n  }\n};\nvar controller = {\n  get: Webfield.get,\n  addHandler: function(name, funcMap) {\n    Object.values(funcMap).forEach(function(func) {\n      func();\n    });\n  },\n};\n\n$('#group-container').empty();\n// START GROUP CODE\n// ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar CONFERENCE_ID = 'ICLR.cc/2022/Conference';\nvar PARENT_GROUP_ID = 'ICLR.cc/2022';\nvar SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';\nvar REVIEWERS_NAME = 'Reviewers';\nvar AREA_CHAIRS_NAME = 'Area_Chairs';\nvar AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';\nvar REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';\nvar PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';\nvar AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';\nvar HEADER = {\"title\": \"The Tenth International Conference on Learning Representations \", \"subtitle\": \"ICLR 2022\", \"location\": \"Virtual\", \"date\": \"Apr 25 2022\", \"website\": \"https://iclr.cc/Conferences/2022\", \"instructions\": \"\", \"deadline\": \"Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0\", \"contact\": \"iclr2022pc@gmail.com\"};\nvar PUBLIC = true;\n\nvar WILDCARD_INVITATION = CONFERENCE_ID + '/.*';\nvar BUFFER = 0;  // deprecated\nvar PAGE_SIZE = 50;\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true,\n  showTags: false\n};\nvar commentDisplayOptions = {\n  pdfLink: false,\n  replyCount: true,\n  showContents: false,\n  showParent: true\n};\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderSubmissionButton();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\n// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/\nfunction load() {\n\n  var spotlightNotesP = $.Deferred().resolve([]);\n  var oralNotesP = $.Deferred().resolve([]);\n  var posterNotesP = $.Deferred().resolve([]);\n  \n  \n  var activityNotesP = $.Deferred().resolve([]);\n  var authorNotesP = $.Deferred().resolve([]);\n  var userGroupsP = $.Deferred().resolve([]);\n  var withdrawnNotesP = $.Deferred().resolve([]);\n  var deskRejectedNotesP = $.Deferred().resolve([]);\n  var submittedNotesP = $.Deferred().resolve([]);\n\n  if (PUBLIC) {\n      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      //   pageSize: PAGE_SIZE,\n      //   details: 'replyCount',\n      //   includeCount: true\n      // });\n\n    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Spotlight',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Oral',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Poster',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Submitted',\n      details: 'replyCount',\n      includeCount: true\n    });\n\n    if (WITHDRAWN_SUBMISSION_ID) {\n      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount',\n        includeCount: true\n      });\n    }\n\n    if (DESK_REJECTED_SUBMISSION_ID) {\n      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount,invitation,original',\n        includeCount: true\n      });\n    }\n  }\n\n  if (user \u0026\u0026 !_.startsWith(user.id, 'guest_')) {\n    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {\n      pageSize: PAGE_SIZE,\n      details: 'forumContent,invitation,writable'\n    });\n\n    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });\n\n    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.authorids': user.profile.id\n    });\n  }\n\n  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);\n}\n\n// Render functions\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction renderSubmissionButton() {\n  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})\n    .then(function(invitation) {\n      Webfield.ui.submissionButton(invitation, user, {\n        onNoteCreated: function() {\n          // Callback funtion to be run when a paper has successfully been submitted (required)\n          if (PUBLIC) {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'A list of all submissions will be available after the deadline.');\n          } else {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'The author console page for managing your submissions will be available soon.');\n          }\n\n          load().then(renderContent).then(function() {\n            $('.tabs-container a[href=\"#your-consoles\"]').click();\n          });\n        }\n      });\n    });\n}\n\nfunction renderConferenceTabs() {\n  var sections = [\n    {\n      heading: 'Your Consoles',\n      id: 'your-consoles',\n    }\n  ];\n\n  if (PUBLIC) {\n    // sections.push({\n    //   heading: 'All Submissions',\n    //   id: 'all-submissions',\n    // });\n    sections.push({\n      heading: 'Oral Presentations',\n      id: 'oral-submissions',\n    });\n    sections.push({\n      heading: 'Spotlight Presentations',\n      id: 'spotlight-submissions',\n    });\n    sections.push({\n      heading: 'Poster Presentations',\n      id: 'poster-submissions',\n    });\n    sections.push({\n      heading: 'Rejected Submissions',\n      id: 'submitted-submissions',\n    });\n    // if (WITHDRAWN_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Withdrawn Submissions',\n    //     id: 'withdrawn-submissions',\n    //   })\n    // }\n    // if (DESK_REJECTED_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Desk Rejected Submissions',\n    //     id: 'desk-rejected-submissions',\n    //   })\n    // }\n    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {\n      sections.push({\n        heading: 'Desk Rejected/Withdrawn Submissions',\n        id: 'desk-rejected-withdrawn-submissions',\n      })\n    }\n  }\n\n  sections.push({\n    heading: 'Recent Activity',\n    id: 'recent-activity',\n  }\n)\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortBy(_.uniq(allGroups));\n  var links = [];\n  uniqueGroups.forEach(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n    links.push(\n      [\n        '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n        '\u003c/li\u003e'\n      ].join('')\n    );\n  });\n\n  $('#your-consoles .submissions-list').append(links);\n}\n\nfunction renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {\n\n  // Your Consoles tab\n  if (userGroups.length || authorNotes.length) {\n\n    var $container = $('#your-consoles').empty();\n    $container.append('\u003cul class=\"list-unstyled submissions-list\"\u003e');\n\n    var allConsoles = [];\n    if (authorNotes.length) {\n      allConsoles.push(AUTHORS_ID);\n    }\n    userGroups.forEach(function(group) {\n      allConsoles.push(group.id);\n    });\n\n    // Render all console links for the user\n    createConsoleLinks(allConsoles);\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n\n  // Oral Papers tab\n  var oralNotesCount = oralNotes.count || 0;\n  oralNotes = oralNotes.notes || [];\n\n  $('#oral-submissions').empty();\n\n  if (oralNotesCount) {\n    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#oral-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(oralNotes, {\n      heading: null,\n      container: '#oral-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Oral',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);\n          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: oralNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Oral',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#oral-submissions\"]').parent().hide();\n  }\n\n  // Spotlight Papers tab\n  var spotlightNotesCount = spotlightNotes.count || 0;\n  spotlightNotes = spotlightNotes.notes || [];\n\n  $('#spotlight-submissions').empty();\n\n  if (spotlightNotesCount) {\n    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#spotlight-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(spotlightNotes, {\n      heading: null,\n      container: '#spotlight-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Spotlight',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);\n          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: spotlightNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Spotlight',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#spotlight-submissions\"]').parent().hide();\n  }\n\n  // Poster Papers tab\n  var posterNotesCount = posterNotes.count || 0;\n  posterNotes = posterNotes.notes || [];\n\n  $('#poster-submissions').empty();\n\n  if (posterNotesCount) {\n    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#poster-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(posterNotes, {\n      heading: null,\n      container: '#poster-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Poster',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);\n          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: posterNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Poster',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#poster-submissions\"]').parent().hide();\n  }\n\n  // Rejected Papers tab\n  var submittedNotesCount = submittedNotes.count || 0;\n  submittedNotes = submittedNotes.notes || [];\n\n  $('#submitted-submissions').empty();\n\n  if (submittedNotesCount) {\n    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#submitted-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(submittedNotes, {\n      heading: null,\n      container: '#submitted-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Submitted',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);\n          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: submittedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Submitted',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#submitted-submissions\"]').parent().hide();\n  }\n\n  // Activity Tab\n  if (activityNotes.length) {\n    var displayOptions = {\n      container: '#recent-activity',\n      user: user \u0026\u0026 user.profile,\n      showActionButtons: true\n    };\n\n    $(displayOptions.container).empty();\n\n    Webfield.ui.activityList(activityNotes, displayOptions);\n\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().hide();\n  }\n  \n  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))\n  \n  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);\n  if (removedNotesCount) {\n    $('#desk-rejected-withdrawn-submissions').empty();\n\n    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);\n    Webfield.ui.submissionList(removedNotesArray, {\n      heading: null,\n      container: '#desk-rejected-withdrawn-submissions',\n      search: {\n        enabled: false\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: removedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        if (offset \u003c deskRejectedNotes.count) {\n          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset\n          });\n        } else {\n          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset - roundedDeskRejectedNotes\n          });\n        }\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#desk-rejected-withdrawn-submissions\"]').parent().hide();\n  }\n\n  // var withdrawnNotesCount = withdrawnNotes.count || 0;\n  // if (withdrawnNotesCount) {\n  //   $('#withdrawn-submissions').empty();\n\n  //   var withdrawnNotesArray = withdrawnNotes.notes || [];\n  //   Webfield.ui.submissionList(withdrawnNotesArray, {\n  //     heading: null,\n  //     container: '#withdrawn-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: withdrawnNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#withdrawn-submissions\"]').parent().hide();\n  // }\n\n  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;\n  // if (deskRejectedNotesCount) {\n  //   $('#desk-rejected-submissions').empty();\n\n  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];\n  //   Webfield.ui.submissionList(deskRejectedNotesArray, {\n  //     heading: null,\n  //     container: '#desk-rejected-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: deskRejectedNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#desk-rejected-submissions\"]').parent().hide();\n  // }\n\n  $('#notes .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n// END GROUP CODE\n});\n//# sourceURL=webfieldCode.js","writable":false,"query":{"id":"ICLR.cc/2022/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2022/Conference"},"buildId":"v1.5.0-1-g2b07894","isFallback":false,"gip":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;">ICLR 2022 Conference | OpenReview</p></next-route-announcer><script>// Webfield Code for ICLR.cc/2022/Conference
$(function() {
var args = {"id":"ICLR.cc/2022/Conference"};
var group = {"id":"ICLR.cc/2022/Conference"};
var document = null;
var window = null;

// TODO: remove these vars when all old webfields have been archived
var model = {
  tokenPayload: function() {
    return { user: user }
  }
};
var controller = {
  get: Webfield.get,
  addHandler: function(name, funcMap) {
    Object.values(funcMap).forEach(function(func) {
      func();
    });
  },
};

$('#group-container').empty();
// START GROUP CODE
// ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var CONFERENCE_ID = 'ICLR.cc/2022/Conference';
var PARENT_GROUP_ID = 'ICLR.cc/2022';
var SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';
var REVIEWERS_NAME = 'Reviewers';
var AREA_CHAIRS_NAME = 'Area_Chairs';
var AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';
var REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';
var PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';
var AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';
var HEADER = {"title": "The Tenth International Conference on Learning Representations ", "subtitle": "ICLR 2022", "location": "Virtual", "date": "Apr 25 2022", "website": "https://iclr.cc/Conferences/2022", "instructions": "", "deadline": "Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0", "contact": "iclr2022pc@gmail.com"};
var PUBLIC = true;

var WILDCARD_INVITATION = CONFERENCE_ID + '/.*';
var BUFFER = 0;  // deprecated
var PAGE_SIZE = 50;

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true,
  showTags: false
};
var commentDisplayOptions = {
  pdfLink: false,
  replyCount: true,
  showContents: false,
  showParent: true
};

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }

  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderSubmissionButton();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/
function load() {

  var spotlightNotesP = $.Deferred().resolve([]);
  var oralNotesP = $.Deferred().resolve([]);
  var posterNotesP = $.Deferred().resolve([]);
  
  
  var activityNotesP = $.Deferred().resolve([]);
  var authorNotesP = $.Deferred().resolve([]);
  var userGroupsP = $.Deferred().resolve([]);
  var withdrawnNotesP = $.Deferred().resolve([]);
  var deskRejectedNotesP = $.Deferred().resolve([]);
  var submittedNotesP = $.Deferred().resolve([]);

  if (PUBLIC) {
      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      //   pageSize: PAGE_SIZE,
      //   details: 'replyCount',
      //   includeCount: true
      // });

    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Spotlight',
      details: 'replyCount',
      includeCount: true
    });
  
    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Oral',
      details: 'replyCount',
      includeCount: true
    });
  
    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Poster',
      details: 'replyCount',
      includeCount: true
    });
  
    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Submitted',
      details: 'replyCount',
      includeCount: true
    });

    if (WITHDRAWN_SUBMISSION_ID) {
      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount',
        includeCount: true
      });
    }

    if (DESK_REJECTED_SUBMISSION_ID) {
      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount,invitation,original',
        includeCount: true
      });
    }
  }

  if (user && !_.startsWith(user.id, 'guest_')) {
    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {
      pageSize: PAGE_SIZE,
      details: 'forumContent,invitation,writable'
    });

    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });

    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.authorids': user.profile.id
    });
  }

  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);
}

// Render functions
function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);

  Webfield.ui.spinner('#notes', { inline: true });
}

function renderSubmissionButton() {
  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})
    .then(function(invitation) {
      Webfield.ui.submissionButton(invitation, user, {
        onNoteCreated: function() {
          // Callback funtion to be run when a paper has successfully been submitted (required)
          if (PUBLIC) {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'A list of all submissions will be available after the deadline.');
          } else {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'The author console page for managing your submissions will be available soon.');
          }

          load().then(renderContent).then(function() {
            $('.tabs-container a[href="#your-consoles"]').click();
          });
        }
      });
    });
}

function renderConferenceTabs() {
  var sections = [
    {
      heading: 'Your Consoles',
      id: 'your-consoles',
    }
  ];

  if (PUBLIC) {
    // sections.push({
    //   heading: 'All Submissions',
    //   id: 'all-submissions',
    // });
    sections.push({
      heading: 'Oral Presentations',
      id: 'oral-submissions',
    });
    sections.push({
      heading: 'Spotlight Presentations',
      id: 'spotlight-submissions',
    });
    sections.push({
      heading: 'Poster Presentations',
      id: 'poster-submissions',
    });
    sections.push({
      heading: 'Rejected Submissions',
      id: 'submitted-submissions',
    });
    // if (WITHDRAWN_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Withdrawn Submissions',
    //     id: 'withdrawn-submissions',
    //   })
    // }
    // if (DESK_REJECTED_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Desk Rejected Submissions',
    //     id: 'desk-rejected-submissions',
    //   })
    // }
    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {
      sections.push({
        heading: 'Desk Rejected/Withdrawn Submissions',
        id: 'desk-rejected-withdrawn-submissions',
      })
    }
  }

  sections.push({
    heading: 'Recent Activity',
    id: 'recent-activity',
  }
)

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortBy(_.uniq(allGroups));
  var links = [];
  uniqueGroups.forEach(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }
    links.push(
      [
        '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
        '</li>'
      ].join('')
    );
  });

  $('#your-consoles .submissions-list').append(links);
}

function renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {

  // Your Consoles tab
  if (userGroups.length || authorNotes.length) {

    var $container = $('#your-consoles').empty();
    $container.append('<ul class="list-unstyled submissions-list">');

    var allConsoles = [];
    if (authorNotes.length) {
      allConsoles.push(AUTHORS_ID);
    }
    userGroups.forEach(function(group) {
      allConsoles.push(group.id);
    });

    // Render all console links for the user
    createConsoleLinks(allConsoles);

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }


  // Oral Papers tab
  var oralNotesCount = oralNotes.count || 0;
  oralNotes = oralNotes.notes || [];

  $('#oral-submissions').empty();

  if (oralNotesCount) {
    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#oral-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(oralNotes, {
      heading: null,
      container: '#oral-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Oral',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);
          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: oralNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Oral',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#oral-submissions"]').parent().hide();
  }

  // Spotlight Papers tab
  var spotlightNotesCount = spotlightNotes.count || 0;
  spotlightNotes = spotlightNotes.notes || [];

  $('#spotlight-submissions').empty();

  if (spotlightNotesCount) {
    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#spotlight-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(spotlightNotes, {
      heading: null,
      container: '#spotlight-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Spotlight',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);
          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: spotlightNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Spotlight',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#spotlight-submissions"]').parent().hide();
  }

  // Poster Papers tab
  var posterNotesCount = posterNotes.count || 0;
  posterNotes = posterNotes.notes || [];

  $('#poster-submissions').empty();

  if (posterNotesCount) {
    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#poster-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(posterNotes, {
      heading: null,
      container: '#poster-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Poster',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);
          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: posterNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Poster',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#poster-submissions"]').parent().hide();
  }

  // Rejected Papers tab
  var submittedNotesCount = submittedNotes.count || 0;
  submittedNotes = submittedNotes.notes || [];

  $('#submitted-submissions').empty();

  if (submittedNotesCount) {
    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#submitted-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(submittedNotes, {
      heading: null,
      container: '#submitted-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Submitted',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);
          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: submittedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Submitted',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#submitted-submissions"]').parent().hide();
  }

  // Activity Tab
  if (activityNotes.length) {
    var displayOptions = {
      container: '#recent-activity',
      user: user && user.profile,
      showActionButtons: true
    };

    $(displayOptions.container).empty();

    Webfield.ui.activityList(activityNotes, displayOptions);

    $('.tabs-container a[href="#recent-activity"]').parent().show();
  } else {
    $('.tabs-container a[href="#recent-activity"]').parent().hide();
  }
  
  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))
  
  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);
  if (removedNotesCount) {
    $('#desk-rejected-withdrawn-submissions').empty();

    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);
    Webfield.ui.submissionList(removedNotesArray, {
      heading: null,
      container: '#desk-rejected-withdrawn-submissions',
      search: {
        enabled: false
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: removedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        if (offset < deskRejectedNotes.count) {
          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset
          });
        } else {
          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset - roundedDeskRejectedNotes
          });
        }
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#desk-rejected-withdrawn-submissions"]').parent().hide();
  }

  // var withdrawnNotesCount = withdrawnNotes.count || 0;
  // if (withdrawnNotesCount) {
  //   $('#withdrawn-submissions').empty();

  //   var withdrawnNotesArray = withdrawnNotes.notes || [];
  //   Webfield.ui.submissionList(withdrawnNotesArray, {
  //     heading: null,
  //     container: '#withdrawn-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: withdrawnNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#withdrawn-submissions"]').parent().hide();
  // }

  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;
  // if (deskRejectedNotesCount) {
  //   $('#desk-rejected-submissions').empty();

  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];
  //   Webfield.ui.submissionList(deskRejectedNotesArray, {
  //     heading: null,
  //     container: '#desk-rejected-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: deskRejectedNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#desk-rejected-submissions"]').parent().hide();
  // }

  $('#notes .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();
// END GROUP CODE
});
//# sourceURL=webfieldCode.js</script><script src="./ICLR2022_poster_6_files/index-55b7d6149a41ddec.js.download"></script><script src="./ICLR2022_poster_6_files/about-77a61c48f23a1315.js.download"></script><script src="./ICLR2022_poster_6_files/venues-458c67c21955226a.js.download"></script><script src="./ICLR2022_poster_6_files/contact-2c775b351d0a5421.js.download"></script><script src="./ICLR2022_poster_6_files/sponsors-987fb223230996d5.js.download"></script><script src="./ICLR2022_poster_6_files/terms-e2f16e0dce69665f.js.download"></script><script src="./ICLR2022_poster_6_files/privacy-276f217f8a2a3840.js.download"></script><script src="./ICLR2022_poster_6_files/login-6d19e702f9bd1dcc.js.download"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>