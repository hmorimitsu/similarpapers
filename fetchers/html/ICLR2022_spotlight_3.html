<!DOCTYPE html>
<!-- saved from url=(0077)https://openreview.net/group?id=ICLR.cc/2022/Conference#spotlight-submissions -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://openreview.net/favicon.ico"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><script async="" src="./ICLR2022_spotlight_3_files/js"></script><script>window.dataLayer = window.dataLayer || [];
function gtag() { dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-108703919-1', {
  page_path: window.location.pathname + window.location.search,
  transport_type: 'beacon'
});</script><title>ICLR 2022 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta property="og:title" content="ICLR 2022 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta name="next-head-count" content="14"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preload" href="./ICLR2022_spotlight_3_files/ed513f7ce02040ba.css" as="style"><link rel="stylesheet" href="./ICLR2022_spotlight_3_files/ed513f7ce02040ba.css" data-n-g=""><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./ICLR2022_spotlight_3_files/polyfills-5cd94c89d3acac5f.js.download"></script><script src="./ICLR2022_spotlight_3_files/webpack-363f7b452897525e.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/framework-79bce4a3a540b080.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/main-aaf30a81beffaaa5.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/_app-dd140e4fb7437f25.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/6253-86ab2843b0544962.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/group-18de1916b352abe5.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/_buildManifest.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/_ssgManifest.js.download" defer=""></script><script src="./ICLR2022_spotlight_3_files/_middlewareManifest.js.download" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap">@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4DRG.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZtyH.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNb4Q.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFlYA.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARPQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARGQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARDQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4AROQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARBQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARNQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARMQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARCQ_mu72Bi.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyOzW1IPriezag.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyHzW1IPriezag.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyCzW1IPriezag.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyPzW1IPriezag.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyAzW1IPriezag.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyMzW1IPriezag.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyNzW1IPriezag.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyDzW1IPrie.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr4TRASf6M7VBj.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6TRASf6M7VBj.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6zRASf6M7VBj.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5TRASf6M7Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVadyBx2pqPIif.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVYNyBx2pqPIif.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZdyBx2pqPIif.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVaNyBx2pqPIif.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZ9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVa9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVatyBx2pqPIif.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZNyBx2pqPA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><script src="./ICLR2022_spotlight_3_files/tex-chtml-full.js.download" async="" crossorigin="anonymous"></script><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/index-55b7d6149a41ddec.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/about-77a61c48f23a1315.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/venues-458c67c21955226a.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/contact-2c775b351d0a5421.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/sponsors-987fb223230996d5.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/terms-e2f16e0dce69665f.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/privacy-276f217f8a2a3840.js.download"><link as="script" rel="prefetch" href="./ICLR2022_spotlight_3_files/login-6d19e702f9bd1dcc.js.download"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-stretchy-v.mjx-c28 mjx-beg mjx-c::before {
  content: "\239B";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c28 mjx-ext mjx-c::before {
  content: "\239C";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c28 mjx-end mjx-c::before {
  content: "\239D";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c28 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c28 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-stretchy-v.mjx-c29 mjx-beg mjx-c::before {
  content: "\239E";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c29 mjx-ext mjx-c::before {
  content: "\239F";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c29 mjx-end mjx-c::before {
  content: "\23A0";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c29 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c29 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c1D44C.TEX-I::before {
  padding: 0.683em 0.763em 0 0;
  content: "Y";
}

mjx-c.mjx-c1D44B.TEX-I::before {
  padding: 0.683em 0.852em 0 0;
  content: "X";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c2218::before {
  padding: 0.444em 0.5em 0 0;
  content: "\2218";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D435.TEX-I::before {
  padding: 0.683em 0.759em 0 0;
  content: "B";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c1D70C.TEX-I::before {
  padding: 0.442em 0.517em 0.216em 0;
  content: "\3C1";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c5E::before {
  padding: 0.694em 0.5em 0 0;
  content: "^";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c2225::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D41B.TEX-B::before {
  padding: 0.694em 0.639em 0.006em 0;
  content: "b";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c1D714.TEX-I::before {
  padding: 0.443em 0.622em 0.011em 0;
  content: "\3C9";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c7E::before {
  padding: 0.318em 0.5em 0 0;
  content: "~";
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c1D45E.TEX-I::before {
  padding: 0.442em 0.46em 0.194em 0;
  content: "q";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c3F.TEX-MI::before {
  padding: 0.716em 0.551em 0 0;
  content: "?";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c398::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "\398";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c1D715::before {
  padding: 0.715em 0.566em 0.022em 0;
  content: "\2202";
}

mjx-c.mjx-c2F.TEX-S1::before {
  padding: 0.85em 0.578em 0.349em 0;
  content: "/";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-c2229::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2229";
}

mjx-c.mjx-c41::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c4E::before {
  padding: 0.683em 0.75em 0 0;
  content: "N";
}

mjx-c.mjx-c44::before {
  padding: 0.683em 0.764em 0 0;
  content: "D";
}

mjx-c.mjx-c49::before {
  padding: 0.683em 0.361em 0 0;
  content: "I";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c2DC.TEX-S1::before {
  padding: 0.722em 0.556em 0 0;
  content: "\2DC";
}

mjx-c.mjx-c28.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: "(";
}

mjx-c.mjx-c29.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: ")";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c53::before {
  padding: 0.705em 0.556em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c45::before {
  padding: 0.68em 0.681em 0 0;
  content: "E";
}

mjx-c.mjx-c1D499.TEX-BI::before {
  padding: 0.452em 0.659em 0.008em 0;
  content: "x";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c22C6::before {
  padding: 0.486em 0.5em 0 0;
  content: "\22C6";
}
</style></head><body data-new-gr-c-s-check-loaded="14.1068.0" data-gr-ext-installed=""><div id="__next" data-reactroot=""><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off" autocorrect="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login?redirect=%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%23spotlight-submissions&amp;noprompt=true">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2022"><img class="icon" src="./ICLR2022_spotlight_3_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2022</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>The Tenth International Conference on Learning Representations </h1>
<h3>ICLR 2022</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Virtual
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> Apr 25 2022
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/Conferences/2022" title="The Tenth International Conference on Learning Representations  Homepage" target="_blank">https://iclr.cc/Conferences/2022</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2022pc@gmail.com" target="_blank">iclr2022pc@gmail.com</a>
      </span>
  </h4>

<div class="description">
    <p class="no-margin">Please see the venue website for more information.</p>
  <p>Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0</p>
</div>
</div><div id="invitation" style="display: block;"></div><div id="notes">
<div class="tabs-container " style=""><div class="mobile-full-width">
  <ul class="nav nav-tabs" role="tablist">
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
          Your Consoles
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oral-submissions" aria-controls="oral-submissions" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true" aria-expanded="false">
          Oral Presentations
        </a>
      </li>
      <li role="presentation" class="active">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#spotlight-submissions" aria-controls="spotlight-submissions" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true" aria-expanded="true">
          Spotlight Presentations
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions" aria-controls="poster-submissions" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true" aria-expanded="false">
          Poster Presentations
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#submitted-submissions" aria-controls="submitted-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
          Rejected Submissions
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#desk-rejected-withdrawn-submissions" aria-controls="desk-rejected-withdrawn-submissions" role="tab" data-toggle="tab" data-tab-index="5" data-modify-history="true">
          Desk Rejected/Withdrawn Submissions
        </a>
      </li>
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#recent-activity" aria-controls="recent-activity" role="tab" data-toggle="tab" data-tab-index="6" data-modify-history="true">
          Recent Activity
        </a>
      </li>
  </ul>
</div>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade" id="oral-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="NMEceG4v69Y" data-number="224">
        <h4>
          <a href="https://openreview.net/forum?id=NMEceG4v69Y">
              CycleMLP: A MLP-like Architecture for Dense Prediction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMEceG4v69Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shoufa_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoufa_Chen1">Shoufa Chen</a>, <a href="https://openreview.net/profile?id=~Enze_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enze_Xie1">Enze Xie</a>, <a href="https://openreview.net/profile?id=~Chongjian_GE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjian_GE1">Chongjian GE</a>, <a href="https://openreview.net/profile?id=~Runjian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runjian_Chen1">Runjian Chen</a>, <a href="https://openreview.net/profile?id=~Ding_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ding_Liang1">Ding Liang</a>, <a href="https://openreview.net/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMEceG4v69Y-details-500" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMEceG4v69Y-details-500"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">MLP, Dense Prediction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope
        with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like modelsâ€™ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A versatile MLP-like architecture for both recognition and dense prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMEceG4v69Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0xiJLKH-ufZ" data-number="222">
        <h4>
          <a href="https://openreview.net/forum?id=0xiJLKH-ufZ">
              Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0xiJLKH-ufZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fan_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fan_Bao1">Fan Bao</a>, <a href="https://openreview.net/profile?id=~Chongxuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongxuan_Li1">Chongxuan Li</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>, <a href="https://openreview.net/profile?id=~Bo_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhang2">Bo Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0xiJLKH-ufZ-details-491" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0xiJLKH-ufZ-details-491"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">diffusion probabilistic models, generative models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an analytic framework of estimating the optimal reverse variance in DPMs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0xiJLKH-ufZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uSE03demja" data-number="208">
        <h4>
          <a href="https://openreview.net/forum?id=uSE03demja">
              RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uSE03demja" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pingchuan_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pingchuan_Ma3">Pingchuan Ma</a>, <a href="https://openreview.net/profile?id=~Tao_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Du1">Tao Du</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Wojciech_Matusik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Matusik2">Wojciech Matusik</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uSE03demja-details-17" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uSE03demja-details-17"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">differentiable rendering, differentiable simulation, system identification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel approach to address the problem of identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3wU2UX0voE" data-number="154">
        <h4>
          <a href="https://openreview.net/forum?id=3wU2UX0voE">
              The Information Geometry of Unsupervised Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3wU2UX0voE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3wU2UX0voE-details-575" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3wU2UX0voE-details-575"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised skill learning, reward-free RL, mutual information, DIAYN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that mutual information skill learning is optimal in one sense but not optimal in another sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=3wU2UX0voE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="pMQwKL1yctf" data-number="86">
        <h4>
          <a href="https://openreview.net/forum?id=pMQwKL1yctf">
              Language modeling via stochastic processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=pMQwKL1yctf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rose_E_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rose_E_Wang1">Rose E Wang</a>, <a href="https://openreview.net/profile?id=~Esin_Durmus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Esin_Durmus1">Esin Durmus</a>, <a href="https://openreview.net/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#pMQwKL1yctf-details-408" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pMQwKL1yctf-details-408"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">contrastive learning, language modelling, stochastic processes</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a language model that implicitly plans via a latent stochastic process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=pMQwKL1yctf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>

<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â«</a>
      </li>
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€¹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class=" active " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="3">
          <span>â€º</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="2">
          <span>Â»</span>
      </li>
  </ul>
</nav>

</div>
    <div role="tabpanel" class="tab-pane fade active in" id="spotlight-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="FRxhHdnxt1" data-number="1737">
        <h4>
          <a href="https://openreview.net/forum?id=FRxhHdnxt1">
              Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FRxhHdnxt1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wenhao_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhao_Gao1">Wenhao Gao</a>, <a href="https://openreview.net/profile?id=~Roc%C3%ADo_Mercado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~RocÃ­o_Mercado1">RocÃ­o Mercado</a>, <a href="https://openreview.net/profile?id=~Connor_W._Coley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Connor_W._Coley1">Connor W. Coley</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FRxhHdnxt1-details-24" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FRxhHdnxt1-details-24"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">molecular design, synthesis planning, tree generation, graph generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a model that address synthesis planning and synthesizable molecular design simultaneously.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=FRxhHdnxt1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LI2bhrE_2A" data-number="1727">
        <h4>
          <a href="https://openreview.net/forum?id=LI2bhrE_2A">
              Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LI2bhrE_2A" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wengong_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wengong_Jin1">Wengong Jin</a>, <a href="https://openreview.net/profile?email=jwohlwend%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jwohlwend@csail.mit.edu">Jeremy Wohlwend</a>, <a href="https://openreview.net/profile?id=~Regina_Barzilay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Regina_Barzilay1">Regina Barzilay</a>, <a href="https://openreview.net/profile?id=~Tommi_S._Jaakkola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tommi_S._Jaakkola1">Tommi S. Jaakkola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LI2bhrE_2A-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LI2bhrE_2A-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Drug Discovery, Antibody Design, Generative Models, Graph Generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous generative approaches formulate protein design as a structure-conditioned sequence generation task, assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively refining its predicted global structure. The inferred structure in turn guides subsequent residue choices. For efficiency, we model the conditional dependence between residues inside and outside of a CDR in a coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new graph-based generative model for antibody design</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HbtFCX2PLq0" data-number="1708">
        <h4>
          <a href="https://openreview.net/forum?id=HbtFCX2PLq0">
              Churn Reduction via Distillation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HbtFCX2PLq0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Heinrich_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heinrich_Jiang1">Heinrich Jiang</a>, <a href="https://openreview.net/profile?id=~Harikrishna_Narasimhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harikrishna_Narasimhan1">Harikrishna Narasimhan</a>, <a href="https://openreview.net/profile?id=~Dara_Bahri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dara_Bahri1">Dara Bahri</a>, <a href="https://openreview.net/profile?id=~Andrew_Cotter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Cotter1">Andrew Cotter</a>, <a href="https://openreview.net/profile?id=~Afshin_Rostamizadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Afshin_Rostamizadeh1">Afshin Rostamizadeh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HbtFCX2PLq0-details-682" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HbtFCX2PLq0-details-682"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">distillation, churn, constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In real-world systems, models are frequently updated as more data becomes available, and in addition to achieving high accuracy, the goal is to also maintain a low difference in predictions compared to the base model (i.e. predictive churn). If model retraining results in vastly different behavior, then it could cause negative effects in downstream systems, especially if this churn can be avoided with limited impact on model accuracy. In this paper, we show an equivalence between training with distillation using the base model as the teacher and training with an explicit constraint on the predictive churn. We then show that distillation performs strongly for low churn training against a number of recent baselines on a wide range of datasets and model architectures, including fully-connected networks, convolutional networks, and transformers.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show distillation is a principled and practical solution to churn reduction.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7twQI5VnC8" data-number="1707">
        <h4>
          <a href="https://openreview.net/forum?id=7twQI5VnC8">
              Learning Causal Models from Conditional Moment Restrictions by Importance Weighting
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7twQI5VnC8" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Masahiro_Kato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masahiro_Kato1">Masahiro Kato</a>, <a href="https://openreview.net/profile?id=~Masaaki_Imaizumi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masaaki_Imaizumi1">Masaaki Imaizumi</a>, <a href="https://openreview.net/profile?id=~Kenichiro_McAlinn2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenichiro_McAlinn2">Kenichiro McAlinn</a>, <a href="https://openreview.net/profile?id=~Shota_Yasui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shota_Yasui1">Shota Yasui</a>, <a href="https://openreview.net/profile?id=~Haruo_Kakehi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haruo_Kakehi2">Haruo Kakehi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7twQI5VnC8-details-781" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7twQI5VnC8-details-781"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Causal inference, Conditional moment restrictions</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider learning causal relationships under conditional moment restrictions. Unlike causal inference under unconditional moment restrictions, conditional moment restrictions pose serious challenges for causal inference. To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance weighting using a conditional density ratio estimator. Then, using this transformation, we propose a method that successfully estimate a parametric or nonparametric functions defined under the conditional moment restrictions. We analyze the estimation error and provide a bound on the structural function, providing theoretical support for our proposed method. In experiments, we confirm the soundness of our proposed method.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning causal relationships under conditional moment restrictions by importance weighting using the conditional density ratio function.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hfU7Ka5cfrC" data-number="1635">
        <h4>
          <a href="https://openreview.net/forum?id=hfU7Ka5cfrC">
              Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hfU7Ka5cfrC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ross_M_Clarke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ross_M_Clarke1">Ross M Clarke</a>, <a href="https://openreview.net/profile?id=~Elre_Talea_Oldewage1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elre_Talea_Oldewage1">Elre Talea Oldewage</a>, <a href="https://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~JosÃ©_Miguel_HernÃ¡ndez-Lobato1">JosÃ© Miguel HernÃ¡ndez-Lobato</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hfU7Ka5cfrC-details-368" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hfU7Ka5cfrC-details-368"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hyperparameter Optimisation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a gradient-based hyperparameter optimisation algorithm, applicable to a wide range of continuous hyperparameters, and scaling to large numbers of hyperparameters, without dramatically increasing training time from the non-HPO baseline.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=hfU7Ka5cfrC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vrW3tvDfOJQ" data-number="1632">
        <h4>
          <a href="https://openreview.net/forum?id=vrW3tvDfOJQ">
              Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vrW3tvDfOJQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vincent_Mai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vincent_Mai1">Vincent Mai</a>, <a href="https://openreview.net/profile?id=~Kaustubh_Mani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaustubh_Mani1">Kaustubh Mani</a>, <a href="https://openreview.net/profile?id=~Liam_Paull1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liam_Paull1">Liam Paull</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vrW3tvDfOJQ-details-434" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vrW3tvDfOJQ-details-434"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep reinforcement learning, uncertainty estimation, inverse-variance, heteroscedastic</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The sample efficiency and performance of model-free DRL is improved by estimating the predictive uncertainty of the targets using probabilistic ensembles and down-weighting the uncertain samples using batch inverse-variance weighting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vrW3tvDfOJQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="O1DEtITim__" data-number="1627">
        <h4>
          <a href="https://openreview.net/forum?id=O1DEtITim__">
              Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining
          </a>
        
          
            <a href="https://openreview.net/pdf?id=O1DEtITim__" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lu_Miao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Miao1">Lu Miao</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Luo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Luo3">Xiaolong Luo</a>, <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="https://openreview.net/profile?id=~Wuyang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wuyang_Chen1">Wuyang Chen</a>, <a href="https://openreview.net/profile?id=~Dong_Liu6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong_Liu6">Dong Liu</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#O1DEtITim__-details-42" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O1DEtITim__-details-42"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pruning, Frank-Wolfe</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a novel framework to train a large deep neural network (DNN) for only $\textit{once}$, which can then be pruned to $\textit{any sparsity ratio}$ to preserve competitive accuracy $\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit $\textit{pruning-aware}$ process: that is formulated with an auxiliary $K$-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by $K$-sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named $\textbf{SFW-pruning}$ consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only needs to train once on the same model and dataset, for obtaining arbitrary ratios, while requiring neither iterative pruning nor retraining. All codes will be released to the public. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel and state-of-the-art one-shot pruning method, which can generate sparse networks at any pruning ratio in one pruning and without any retraining.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qTHBE7E9iej" data-number="1618">
        <h4>
          <a href="https://openreview.net/forum?id=qTHBE7E9iej">
              Learning transferable motor skills with hierarchical latent mixture policies
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qTHBE7E9iej" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dushyant_Rao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dushyant_Rao1">Dushyant Rao</a>, <a href="https://openreview.net/profile?id=~Fereshteh_Sadeghi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fereshteh_Sadeghi3">Fereshteh Sadeghi</a>, <a href="https://openreview.net/profile?id=~Leonard_Hasenclever1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonard_Hasenclever1">Leonard Hasenclever</a>, <a href="https://openreview.net/profile?id=~Markus_Wulfmeier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Wulfmeier1">Markus Wulfmeier</a>, <a href="https://openreview.net/profile?id=~Martina_Zambelli2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martina_Zambelli2">Martina Zambelli</a>, <a href="https://openreview.net/profile?id=~Giulia_Vezzani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giulia_Vezzani1">Giulia Vezzani</a>, <a href="https://openreview.net/profile?id=~Dhruva_Tirumala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dhruva_Tirumala1">Dhruva Tirumala</a>, <a href="https://openreview.net/profile?id=~Yusuf_Aytar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusuf_Aytar1">Yusuf Aytar</a>, <a href="https://openreview.net/profile?id=~Josh_Merel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Josh_Merel1">Josh Merel</a>, <a href="https://openreview.net/profile?id=~Nicolas_Heess1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Heess1">Nicolas Heess</a>, <a href="https://openreview.net/profile?id=~raia_hadsell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~raia_hadsell1">raia hadsell</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qTHBE7E9iej-details-282" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qTHBE7E9iej-details-282"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotics, Reinforcement Learning, Hierarchical, Latent Variable Models, Skills, Transfer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios.
        We propose an approach to learn skills from data using a hierarchical mixture latent variable model.
        Our method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a discrete set of abstract high-level behaviours while allowing for variance in how they are executed.
        We demonstrate in manipulation domains that the method can effectively cluster offline data into distinct, executable behaviours, while retaining the flexibility of a continuous latent variable model.
        The resulting skills can be transferred to new tasks, unseen objects, and from state to vision-based policies, yielding significantly better sample efficiency and asymptotic performance compared to existing skill- and imitation-based methods.
        We also perform further analysis showing how and when the skills are most beneficial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An approach to learn reusable and transferable skills from data via a hierarchical latent mixture policy, which can significantly improve sample efficiency and asymptotic performance on downstream RL tasks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gPvB4pdu_Z" data-number="1606">
        <h4>
          <a href="https://openreview.net/forum?id=gPvB4pdu_Z">
              Compositional Training for End-to-End Deep AUC Maximization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gPvB4pdu_Z" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhuoning_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuoning_Yuan1">Zhuoning Yuan</a>, <a href="https://openreview.net/profile?id=~Zhishuai_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhishuai_Guo1">Zhishuai Guo</a>, <a href="https://openreview.net/profile?id=~Nitesh_Chawla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nitesh_Chawla1">Nitesh Chawla</a>, <a href="https://openreview.net/profile?id=~Tianbao_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianbao_Yang1">Tianbao Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gPvB4pdu_Z-details-957" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gPvB4pdu_Z-details-957"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Compositional Training, Imbalanced Losses, AUC optimization, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, deep AUC maximization (DAM) has achieved great success in different domains (e.g., medical image classification). However, the end-to-end training for deep AUC maximization still remains a challenging problem. Previous studies employ an ad-hoc  two-stage approach that first trains the network by optimizing a traditional  loss (e.g., cross-entropy loss) and then finetunes the network by optimizing an AUC loss. This is because that training a deep neural network from scratch by maximizing an AUC loss usually does not yield a satisfactory performance. This phenomenon can be attributed to the degraded feature representations learned by maximizing the AUC loss from scratch. To address this issue, we propose a novel compositional training framework for end-to-end DAM, namely compositional DAM. The key idea of compositional training is to minimize a compositional objective function, where the outer function corresponds to an AUC loss and the inner function represents  a gradient descent step for minimizing a traditional loss, e.g., the cross-entropy (CE) loss. To optimize the non-standard compositional objective, we propose an efficient and provable stochastic optimization algorithm. The proposed algorithm enhances the capabilities  of  both robust feature learning and robust classifier learning  by alternatively taking a gradient descent step for the CE loss and for the AUC loss in a systematic way.  We conduct extensive empirical studies on imbalanced benchmark and medical image datasets, which unanimously verify the effectiveness of the proposed method.  Our results show that the compositional training approach dramatically improves both the feature representations and the testing AUC score compared with traditional deep learning approaches, and yields better performance than the two-stage approaches for DAM as well. The proposed method is implemented in our open-sourced library LibAUC (https://www.libauc.org) and code is available at https://github.com/Optimization-AI/LibAUC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel end-to-end training framework with a provable stochastic algorithm for deep AUC maximization. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="45Mr7LeKR9" data-number="1604">
        <h4>
          <a href="https://openreview.net/forum?id=45Mr7LeKR9">
              Explanations of Black-Box Models based on Directional Feature Interactions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=45Mr7LeKR9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aria_Masoomi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aria_Masoomi1">Aria Masoomi</a>, <a href="https://openreview.net/profile?id=~Davin_Hill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Davin_Hill1">Davin Hill</a>, <a href="https://openreview.net/profile?id=~Zhonghui_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhonghui_Xu1">Zhonghui Xu</a>, <a href="https://openreview.net/profile?id=~Craig_P_Hersh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Craig_P_Hersh1">Craig P Hersh</a>, <a href="https://openreview.net/profile?id=~Edwin_K._Silverman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edwin_K._Silverman1">Edwin K. Silverman</a>, <a href="https://openreview.net/profile?id=~Peter_J._Castaldi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_J._Castaldi1">Peter J. Castaldi</a>, <a href="https://openreview.net/profile?id=~Stratis_Ioannidis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stratis_Ioannidis1">Stratis Ioannidis</a>, <a href="https://openreview.net/profile?id=~Jennifer_Dy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_Dy1">Jennifer Dy</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#45Mr7LeKR9-details-225" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="45Mr7LeKR9-details-225"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Explainability, Shapley values, Interpretability, Directional interaction, feature interaction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.  </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a bivariate explainer to explain directional feature interactions in black box models. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=45Mr7LeKR9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Fl3Mg_MZR-" data-number="1589">
        <h4>
          <a href="https://openreview.net/forum?id=Fl3Mg_MZR-">
              On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Fl3Mg_MZR-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marc_Vischer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Vischer1">Marc Vischer</a>, <a href="https://openreview.net/profile?id=~Robert_Tjarko_Lange1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Tjarko_Lange1">Robert Tjarko Lange</a>, <a href="https://openreview.net/profile?email=h.sprekeler%40tu-berlin.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="h.sprekeler@tu-berlin.de">Henning Sprekeler</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Fl3Mg_MZR--details-531" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Fl3Mg_MZR--details-531"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Sparsity, Pruning, Lottery Ticket Hypothesis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate the mechanisms underlying the lottery ticket effect in Deep RL and show that the derived mask extracts minimal task representations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4AZz9osqrar" data-number="1578">
        <h4>
          <a href="https://openreview.net/forum?id=4AZz9osqrar">
              Self-supervised Learning is More Robust to Dataset Imbalance
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4AZz9osqrar" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hong_Liu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hong_Liu5">Hong Liu</a>, <a href="https://openreview.net/profile?id=~Jeff_Z._HaoChen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeff_Z._HaoChen1">Jeff Z. HaoChen</a>, <a href="https://openreview.net/profile?id=~Adrien_Gaidon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrien_Gaidon1">Adrien Gaidon</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Ma1">Tengyu Ma</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4AZz9osqrar-details-846" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4AZz9osqrar-details-846"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, dataset imbalance, representation learning, long-tailed recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments as well as rigorous mathematical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that  consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that self-supervised pre-training yields representations more robust to dataset imbalance, because it captures more diverse features from the frequent classes, and can be improved further by re-weighting regularization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="apv504XsysP" data-number="1541">
        <h4>
          <a href="https://openreview.net/forum?id=apv504XsysP">
              Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=apv504XsysP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nicholas_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Gao1">Nicholas Gao</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_GÃ¼nnemann1">Stephan GÃ¼nnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#apv504XsysP-details-374" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="apv504XsysP-details-374"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Computational Physics, Self-Generative Learning, Machine Learning for Science</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Solving the SchrÃ¶dinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modelling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the SchrÃ¶dinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network (PESNet) speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a PESNet, a new network architecture that solves the SchrÃ¶dinger equation for multiple geometries simultaneously.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SidzxAb9k30" data-number="1485">
        <h4>
          <a href="https://openreview.net/forum?id=SidzxAb9k30">
              Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SidzxAb9k30" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaoyu_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyu_Chen2">Xiaoyu Chen</a>, <a href="https://openreview.net/profile?id=~Jiachen_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiachen_Hu1">Jiachen Hu</a>, <a href="https://openreview.net/profile?id=~Lin_Yang12" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lin_Yang12">Lin Yang</a>, <a href="https://openreview.net/profile?id=~Liwei_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liwei_Wang1">Liwei Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SidzxAb9k30-details-278" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SidzxAb9k30-details-278"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reward-free exploration, model-based reinforcement learning, learning theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that \emph{any planning algorithm} on the learned model can give a near-optimal policy. Specicially, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking $\tilde{O}(d^2H^3/\epsilon^2)$ interactions with the environment and \emph{any} $\epsilon$-optimal planner on the model gives an $O(\epsilon)$-optimal policy on the original model. This sample complexity matches lower bounds for non-plug-in approaches and is \emph{statistically optimal}. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDP.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose near-optimal exploration algorithms for reward-free exploration with plug-in solver.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MEpKGLsY8f" data-number="1474">
        <h4>
          <a href="https://openreview.net/forum?id=MEpKGLsY8f">
              Meta Discovery: Learning to Discover Novel Classes given Very Limited Data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MEpKGLsY8f" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Haoang_Chi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoang_Chi1">Haoang Chi</a>, <a href="https://openreview.net/profile?id=~Feng_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feng_Liu2">Feng Liu</a>, <a href="https://openreview.net/profile?id=~Wenjing_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjing_Yang1">Wenjing Yang</a>, <a href="https://openreview.net/profile?id=~Long_Lan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Lan2">Long Lan</a>, <a href="https://openreview.net/profile?id=~Tongliang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tongliang_Liu1">Tongliang Liu</a>, <a href="https://openreview.net/profile?id=~Bo_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Han1">Bo Han</a>, <a href="https://openreview.net/profile?id=~Gang_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gang_Niu1">Gang Niu</a>, <a href="https://openreview.net/profile?id=~Mingyuan_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyuan_Zhou1">Mingyuan Zhou</a>, <a href="https://openreview.net/profile?id=~Masashi_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masashi_Sugiyama1">Masashi Sugiyama</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MEpKGLsY8f-details-267" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MEpKGLsY8f-details-267"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PRZoSmCinhf" data-number="1470">
        <h4>
          <a href="https://openreview.net/forum?id=PRZoSmCinhf">
              Constrained Policy Optimization via Bayesian World Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PRZoSmCinhf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yarden_As1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yarden_As1">Yarden As</a>, <a href="https://openreview.net/profile?id=~Ilnura_Usmanova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ilnura_Usmanova1">Ilnura Usmanova</a>, <a href="https://openreview.net/profile?id=~Sebastian_Curi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Curi1">Sebastian Curi</a>, <a href="https://openreview.net/profile?id=~Andreas_Krause1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Krause1">Andreas Krause</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PRZoSmCinhf-details-35" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PRZoSmCinhf-details-35"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, Constrained Markov decision processes, Constrained policy optimization, Bayesian model-based RL</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Solving constrained Markov decision processes with Bayesian model-based reinforcement learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="OIs3SxU5Ynl" data-number="1451">
        <h4>
          <a href="https://openreview.net/forum?id=OIs3SxU5Ynl">
              VAE Approximation Error: ELBO and Exponential Families
          </a>
        
          
            <a href="https://openreview.net/pdf?id=OIs3SxU5Ynl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Shekhovtsov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Shekhovtsov1">Alexander Shekhovtsov</a>, <a href="https://openreview.net/profile?id=~Dmitrij_Schlesinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitrij_Schlesinger1">Dmitrij Schlesinger</a>, <a href="https://openreview.net/profile?id=~Boris_Flach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Flach1">Boris Flach</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#OIs3SxU5Ynl-details-989" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OIs3SxU5Ynl-details-989"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The importance of Variational Autoencoders reaches far beyond standalone generative models -- the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors caused by the combination of the ELBO objective and encoder models from conditional exponential families, including, but not limited to, commonly used conditionally independent discrete and continuous models.
        We characterize subclasses of generative models consistent with these encoder families. We show that the ELBO optimizer is pulled away from the likelihood optimizer towards the consistent subset and study this effect experimentally. Importantly, this subset can not be enlarged, and the respective error cannot be decreased, by considering deeper encoder/decoder networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">VAEs have an inductive bias towards RBMs and generalized linear models</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CAjxVodl_v" data-number="1408">
        <h4>
          <a href="https://openreview.net/forum?id=CAjxVodl_v">
              Generalized Decision Transformer for Offline Hindsight Information Matching
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CAjxVodl_v" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hiroki_Furuta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hiroki_Furuta1">Hiroki Furuta</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutaka_Matsuo1">Yutaka Matsuo</a>, <a href="https://openreview.net/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 26 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CAjxVodl_v-details-139" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CAjxVodl_v-details-139"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hindsight Information Matching, Decision Transformer, State-Marginal Matching, Hindsight Experience Replay, Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay (HER) or returns-to-go in Decision Transformer (DT) -- enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. Categorical DT, which simply replaces anti-causal summation with anti-causal binning in DT, enables arguably the first effective offline multi-task SMM algorithm that generalizes well to unseen (and even synthetic) multi-modal reward or state-feature distributions. Bi-directional DT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We generalize hindsight algorithms in RL, and propose Distributional Decision Transformer for information matching.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=CAjxVodl_v&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="1HxTO6CTkz" data-number="1370">
        <h4>
          <a href="https://openreview.net/forum?id=1HxTO6CTkz">
              Unifying Likelihood-free Inference with Black-box Optimization and Beyond
          </a>
        
          
            <a href="https://openreview.net/pdf?id=1HxTO6CTkz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dinghuai_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinghuai_Zhang1">Dinghuai Zhang</a>, <a href="https://openreview.net/profile?id=~Jie_Fu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Fu2">Jie Fu</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#1HxTO6CTkz-details-189" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1HxTO6CTkz-details-189"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">biological sequence design, black-box optimization, likelihood-free inference, Bayesian inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be "reinvented" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a framework to unify likelihood-free inference and black-box sequence design and further propose novel sequence design algorithms based on the framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=1HxTO6CTkz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AJAR-JgNw__" data-number="1367">
        <h4>
          <a href="https://openreview.net/forum?id=AJAR-JgNw__">
              DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AJAR-JgNw__" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wei_Fan6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Fan6">Wei Fan</a>, <a href="https://openreview.net/profile?id=~Shun_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shun_Zheng1">Shun Zheng</a>, <a href="https://openreview.net/profile?email=xiaohan.yi%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiaohan.yi@microsoft.com">Xiaohan Yi</a>, <a href="https://openreview.net/profile?id=~Wei_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Cao1">Wei Cao</a>, <a href="https://openreview.net/profile?id=~Yanjie_Fu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanjie_Fu2">Yanjie Fu</a>, <a href="https://openreview.net/profile?id=~Jiang_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiang_Bian1">Jiang Bian</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AJAR-JgNw__-details-299" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AJAR-JgNw__-details-299"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this paper, we introduce a deep expansion learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decoupled formulation by introducing the periodic state as a hidden variable, which stimulates us to make two dedicated modules to tackle the aforementioned two challenges. First, we develop an expansion module on top of residual learning to perform a layer-by-layer expansion of those complicated dependencies. Second, we introduce a periodicity module with a parameterized periodic function that holds sufficient capacity to capture diversified periods. Moreover, our two customized modules also have certain interpretable capabilities, such as attributing the forecasts to either local momenta or global periodicity and characterizing certain core periodic properties, e.g., amplitudes and frequencies. Extensive experiments on both synthetic data and real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves significant improvements over the best baseline. Specifically, the error reduction can even reach up to 20% for a few cases. All codes for this paper are publicly available.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="tBtoZYKd9n" data-number="1335">
        <h4>
          <a href="https://openreview.net/forum?id=tBtoZYKd9n">
              Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=tBtoZYKd9n" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Leslie_O%26%23x27%3BBray1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leslie_O&#39;Bray1">Leslie O'Bray</a>, <a href="https://openreview.net/profile?id=~Max_Horn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Horn1">Max Horn</a>, <a href="https://openreview.net/profile?id=~Bastian_Rieck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bastian_Rieck1">Bastian Rieck</a>, <a href="https://openreview.net/profile?id=~Karsten_Borgwardt2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karsten_Borgwardt2">Karsten Borgwardt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#tBtoZYKd9n-details-730" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tBtoZYKd9n-details-730"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph generative models, model evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on the maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate the potential pitfalls of using MMD to evaluate graph generative models and propose recommendations for the practitioner on how to mitigate those challenges.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="tV3N0DWMxCg" data-number="1328">
        <h4>
          <a href="https://openreview.net/forum?id=tV3N0DWMxCg">
              Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=tV3N0DWMxCg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Bertrand_Charpentier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bertrand_Charpentier2">Bertrand Charpentier</a>, <a href="https://openreview.net/profile?id=~Oliver_Borchert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oliver_Borchert1">Oliver Borchert</a>, <a href="https://openreview.net/profile?id=~Daniel_Z%C3%BCgner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_ZÃ¼gner1">Daniel ZÃ¼gner</a>, <a href="https://openreview.net/profile?id=~Simon_Geisler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Geisler1">Simon Geisler</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_GÃ¼nnemann1">Stephan GÃ¼nnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#tV3N0DWMxCg-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tV3N0DWMxCg-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uncertainty, Exponential Family, Bayesian Update, Conjugate Prior</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KxbhdyiPHE" data-number="1287">
        <h4>
          <a href="https://openreview.net/forum?id=KxbhdyiPHE">
              Learning Altruistic Behaviours in Reinforcement Learning without External Rewards
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KxbhdyiPHE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tim_Franzmeyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Franzmeyer1">Tim Franzmeyer</a>, <a href="https://openreview.net/profile?id=~Mateusz_Malinowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mateusz_Malinowski1">Mateusz Malinowski</a>, <a href="https://openreview.net/profile?id=~Joao_F._Henriques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joao_F._Henriques1">Joao F. Henriques</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KxbhdyiPHE-details-410" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KxbhdyiPHE-details-410"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, altruistic behavior in AI, multi-agent systems</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose and investigate unsupervised training of agents to behave altruistically towards others by actively maximizing others' choice.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=KxbhdyiPHE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wQfgfb8VKTn" data-number="1272">
        <h4>
          <a href="https://openreview.net/forum?id=wQfgfb8VKTn">
              Context-Aware Sparse Deep Coordination Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wQfgfb8VKTn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tonghan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tonghan_Wang1">Tonghan Wang</a>, <a href="https://openreview.net/profile?id=~Liang_Zeng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Zeng1">Liang Zeng</a>, <a href="https://openreview.net/profile?id=~Weijun_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weijun_Dong1">Weijun Dong</a>, <a href="https://openreview.net/profile?id=~Qianlan_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qianlan_Yang1">Qianlan Yang</a>, <a href="https://openreview.net/profile?id=~Yang_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Yu5">Yang Yu</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 07 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wQfgfb8VKTn-details-674" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQfgfb8VKTn-details-674"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent reinforcement learning, Sparse coordination graphs, Deep coordination graphs</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel method for learning sparse coordination graphs that can be theoretically justified and can significantly reduce communication overhead and improve learning performance of deep coordination graphs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQfgfb8VKTn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xDIvIqQ3DXD" data-number="1267">
        <h4>
          <a href="https://openreview.net/forum?id=xDIvIqQ3DXD">
              On the approximation properties of recurrent encoder-decoder architectures
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xDIvIqQ3DXD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhong_Li2">Zhong Li</a>, <a href="https://openreview.net/profile?id=~Haotian_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haotian_Jiang1">Haotian Jiang</a>, <a href="https://openreview.net/profile?id=~Qianxiao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qianxiao_Li1">Qianxiao Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xDIvIqQ3DXD-details-396" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xDIvIqQ3DXD-details-396"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">encoder-decoder, recurrent neural networks, approximation, temporal product</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular â€œtemporal product structureâ€ which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be efficiently learned.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Approximation properties of recurrent encoder-decoder architectures are given, where the formed temporal product structure further characterises temporal relationships able to be efficiently learned.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nfl-iXa-y7R" data-number="1260">
        <h4>
          <a href="https://openreview.net/forum?id=Nfl-iXa-y7R">
              Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nfl-iXa-y7R" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Beidi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beidi_Chen1">Beidi Chen</a>, <a href="https://openreview.net/profile?id=~Tri_Dao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tri_Dao1">Tri Dao</a>, <a href="https://openreview.net/profile?id=~Kaizhao_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaizhao_Liang1">Kaizhao Liang</a>, <a href="https://openreview.net/profile?id=~Jiaming_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaming_Yang1">Jiaming Yang</a>, <a href="https://openreview.net/profile?id=~Zhao_Song6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhao_Song6">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Atri_Rudra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atri_Rudra1">Atri Rudra</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nfl-iXa-y7R-details-319" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nfl-iXa-y7R-details-319"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Sparse training, butterfly, low-rank, Lottery Tickets, Block sparsity, Hashing, Transformer, ViT, MLP-Mixer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a simple sparse training method, which can speed up model training in wall-clock time with no drop in accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Nfl-iXa-y7R&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="shpkpVXzo3h" data-number="1231">
        <h4>
          <a href="https://openreview.net/forum?id=shpkpVXzo3h">
              8-bit Optimizers via Block-wise Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=shpkpVXzo3h" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tim_Dettmers2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Dettmers2">Tim Dettmers</a>, <a href="https://openreview.net/profile?id=~Mike_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Lewis1">Mike Lewis</a>, <a href="https://openreview.net/profile?id=~Sam_Shleifer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sam_Shleifer1">Sam Shleifer</a>, <a href="https://openreview.net/profile?id=~Luke_Zettlemoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Zettlemoyer1">Luke Zettlemoyer</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#shpkpVXzo3h-details-328" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="shpkpVXzo3h-details-328"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">language models, pretraining, finetuning, GPU memory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop 8-bit optimizers reduce the memory footprint of training and maintain 32-bit optimizer performance across NLP/CV benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=shpkpVXzo3h&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="yeP_zx9vqNm" data-number="1197">
        <h4>
          <a href="https://openreview.net/forum?id=yeP_zx9vqNm">
              Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=yeP_zx9vqNm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?email=annekh%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="annekh@mit.edu">Anne Harrington</a>, <a href="https://openreview.net/profile?id=~Arturo_Deza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arturo_Deza1">Arturo Deza</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#yeP_zx9vqNm-details-267" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yeP_zx9vqNm-details-267"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Peripheral Computation, Adversarial Robustness, Perceptual Invariance, Metamerism, Texture, Psychophysics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et al., 2019). Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding (Balas et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012). To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman \&amp; Simoncelli, 2011, Wallis et al., 2016 and Deza et al., 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al., 2018).  We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery.  Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field.  These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We suggest that the representations learned by an Adversarially Trained Network are aligned with Human Peripheral Computation</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=yeP_zx9vqNm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DmpCfq6Mg39" data-number="1181">
        <h4>
          <a href="https://openreview.net/forum?id=DmpCfq6Mg39">
              Omni-Dimensional Dynamic Convolution
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DmpCfq6Mg39" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chao_Li16" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Li16">Chao Li</a>, <a href="https://openreview.net/profile?id=~Aojun_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aojun_Zhou2">Aojun Zhou</a>, <a href="https://openreview.net/profile?id=~Anbang_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anbang_Yao1">Anbang Yao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DmpCfq6Mg39-details-684" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DmpCfq6Mg39-details-684"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Convolutional Neural Networks, Dynamic Convolution, Attention, Image Classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77%~5.71%|1.86%~3.72% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights. Code and models will be available at https://github.com/OSVAI/ODConv.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper presents Omni-dimensional Dynamic Convolution (ODConv) to advance the research in dynamic convolution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="BjyvwnXXVn_" data-number="1167">
        <h4>
          <a href="https://openreview.net/forum?id=BjyvwnXXVn_">
              EViT: Expediting Vision Transformers via Token Reorganizations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=BjyvwnXXVn_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Youwei_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youwei_Liang1">Youwei Liang</a>, <a href="https://openreview.net/profile?id=~Chongjian_GE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjian_GE1">Chongjian GE</a>, <a href="https://openreview.net/profile?id=~Zhan_Tong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhan_Tong1">Zhan Tong</a>, <a href="https://openreview.net/profile?id=~Yibing_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yibing_Song1">Yibing Song</a>, <a href="https://openreview.net/profile?id=~Jue_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jue_Wang2">Jue Wang</a>, <a href="https://openreview.net/profile?id=~Pengtao_Xie3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengtao_Xie3">Pengtao Xie</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#BjyvwnXXVn_-details-91" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BjyvwnXXVn_-details-91"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, multi-head self-attention, efficient inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose to reorganize attentive tokens in Vision Transformers to expedite inference speed.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wENMvIsxNN" data-number="1109">
        <h4>
          <a href="https://openreview.net/forum?id=wENMvIsxNN">
              D-CODE: Discovering Closed-form ODEs from Observed Trajectories
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wENMvIsxNN" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhaozhi_Qian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaozhi_Qian1">Zhaozhi Qian</a>, <a href="https://openreview.net/profile?id=~Krzysztof_Kacprzyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krzysztof_Kacprzyk1">Krzysztof Kacprzyk</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wENMvIsxNN-details-105" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wENMvIsxNN-details-105"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Symbolic Regression, Ordinary Differential Equation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For centuries, scientists have manually designed closed-form ordinary differential equations (ODEs) to model dynamical systems. An automated tool to distill closed-form ODEs from observed trajectories would accelerate the modeling process. Traditionally, symbolic regression is used to uncover a closed-form prediction function $a=f(b)$ with label-feature pairs $(a_i, b_i)$ as training examples. However, an ODE models the time derivative $\dot{x}(t)$ of a dynamical system, e.g. $\dot{x}(t) = f(x(t),t)$, and the "label" $\dot{x}(t)$ is usually *not* observed. The existing ways to bridge this gap only perform well for a narrow range of settings with low measurement noise, frequent sampling, and non-chaotic dynamics. In this work, we propose the Discovery of Closed-form ODE framework (D-CODE), which advances symbolic regression beyond the paradigm of supervised learning. D-CODE leverages a novel objective function based on the variational formulation of ODEs to bypass the unobserved time derivative. For formal justification, we prove that this objective is a valid proxy for the estimation error of the true (but unknown) ODE. In the experiments, D-CODE successfully discovered the governing equations of a diverse range of dynamical systems under challenging measurement settings with high noise and infrequent sampling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="w60btE_8T2m" data-number="1092">
        <h4>
          <a href="https://openreview.net/forum?id=w60btE_8T2m">
              Spanning Tree-based Graph Generation for Molecules
          </a>
        
          
            <a href="https://openreview.net/pdf?id=w60btE_8T2m" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sungsoo_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungsoo_Ahn1">Sungsoo Ahn</a>, <a href="https://openreview.net/profile?id=~Binghong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binghong_Chen1">Binghong Chen</a>, <a href="https://openreview.net/profile?id=~Tianzhe_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianzhe_Wang1">Tianzhe Wang</a>, <a href="https://openreview.net/profile?id=~Le_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Le_Song1">Le Song</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#w60btE_8T2m-details-728" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w60btE_8T2m-details-728"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">molecule generation, tree generation, graph generation, deep generative model, de novo drug design</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the sparsity of molecular graphs and allows using compact tree-constructive operations to define the molecular graph connectivity. Based on the intermediate graph structure of the construction process, our framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. We also newly design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Frechet ChemNet distance, and fragment similarity. We also demonstrate the usefulness of STGG in maximizing penalized LogP value of molecules.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new molecular graph generative model based on compact tree constructive operators.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=w60btE_8T2m&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bERaNdoegnO" data-number="1088">
        <h4>
          <a href="https://openreview.net/forum?id=bERaNdoegnO">
              Policy improvement by planning with Gumbel
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bERaNdoegnO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ivo_Danihelka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivo_Danihelka1">Ivo Danihelka</a>, <a href="https://openreview.net/profile?id=~Arthur_Guez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Guez1">Arthur Guez</a>, <a href="https://openreview.net/profile?id=~Julian_Schrittwieser1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julian_Schrittwieser1">Julian Schrittwieser</a>, <a href="https://openreview.net/profile?id=~David_Silver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Silver1">David Silver</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bERaNdoegnO-details-889" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bERaNdoegnO-details-889"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AlphaZero, MuZero, reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of policy improvement to replace the more heuristic mechanisms by which AlphaZero selects and uses actions, both at root nodes and at non-root nodes. Our new algorithms, Gumbel AlphaZero and Gumbel MuZero, respectively without and with model-learning, match the state of the art on Go, chess, and Atari, and significantly improve prior performance when planning with few simulations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We redesign AlphaZero to keep improving even when training with a small number of simulations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="t8O-4LKFVx" data-number="1078">
        <h4>
          <a href="https://openreview.net/forum?id=t8O-4LKFVx">
              Learning Optimal Conformal Classifiers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=t8O-4LKFVx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~David_Stutz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Stutz1">David Stutz</a>, <a href="https://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krishnamurthy_Dj_Dvijotham1">Krishnamurthy Dj Dvijotham</a>, <a href="https://openreview.net/profile?id=~Ali_Taylan_Cemgil2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Taylan_Cemgil2">Ali Taylan Cemgil</a>, <a href="https://openreview.net/profile?id=~Arnaud_Doucet2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnaud_Doucet2">Arnaud Doucet</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#t8O-4LKFVx-details-98" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="t8O-4LKFVx-details-98"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">conformal prediction, conformal classification, uncertainty estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the classifier's predictions, e.g., its probability estimates, to predict confidence sets containing the true class with a user-specified probability. However, using CP as a separate processing step after training prevents the underlying model from adapting to the prediction of confidence sets. Thus, this paper explores strategies to differentiate through CP during training with the goal of training model with the conformal wrapper end-to-end. In our approach, conformal training (ConfTr), we specifically "simulate" conformalization on mini-batches during training. Compared to standard training, ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art CP methods applied after training. Moreover, it allows to "shape" the confidence sets predicted at test time, which is difficult for standard CP. On experiments with several datasets, we show ConfTr can influence how inefficiency is distributed across classes, or guide the composition of confidence sets in terms of the included classes, while retaining the guarantees offered by CP.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Conformal training allows to train classifier and conformal predictor end-to-end, optimizing average confidence set size (inefficiency) or other application-specific losses defined on confidence sets.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="9Vrb9D0WI4" data-number="1066">
        <h4>
          <a href="https://openreview.net/forum?id=9Vrb9D0WI4">
              Multitask Prompted Training Enables Zero-Shot Task Generalization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=9Vrb9D0WI4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Victor_Sanh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Victor_Sanh1">Victor Sanh</a>, <a href="https://openreview.net/profile?id=~Albert_Webson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Webson1">Albert Webson</a>, <a href="https://openreview.net/profile?id=~Colin_Raffel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colin_Raffel1">Colin Raffel</a>, <a href="https://openreview.net/profile?id=~Stephen_Bach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Bach1">Stephen Bach</a>, <a href="https://openreview.net/profile?id=~Lintang_Sutawika1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lintang_Sutawika1">Lintang Sutawika</a>, <a href="https://openreview.net/profile?id=~Zaid_Alyafeai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zaid_Alyafeai1">Zaid Alyafeai</a>, <a href="https://openreview.net/profile?id=~Antoine_Chaffin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antoine_Chaffin1">Antoine Chaffin</a>, <a href="https://openreview.net/profile?id=~Arnaud_Stiegler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnaud_Stiegler1">Arnaud Stiegler</a>, <a href="https://openreview.net/profile?id=~Arun_Raja1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arun_Raja1">Arun Raja</a>, <a href="https://openreview.net/profile?id=~Manan_Dey3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manan_Dey3">Manan Dey</a>, <a href="https://openreview.net/profile?id=~M_Saiful_Bari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~M_Saiful_Bari2">M Saiful Bari</a>, <a href="https://openreview.net/profile?id=~Canwen_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Canwen_Xu1">Canwen Xu</a>, <a href="https://openreview.net/profile?id=~Urmish_Thakker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Urmish_Thakker1">Urmish Thakker</a>, <a href="https://openreview.net/profile?id=~Shanya_Sharma_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shanya_Sharma_Sharma1">Shanya Sharma Sharma</a>, <a href="https://openreview.net/profile?id=~Eliza_Szczechla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eliza_Szczechla1">Eliza Szczechla</a>, <a href="https://openreview.net/profile?id=~Taewoon_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taewoon_Kim1">Taewoon Kim</a>, <a href="https://openreview.net/profile?id=~Gunjan_Chhablani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunjan_Chhablani1">Gunjan Chhablani</a>, <a href="https://openreview.net/profile?id=~Nihal_Nayak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nihal_Nayak1">Nihal Nayak</a>, <a href="https://openreview.net/profile?id=~Debajyoti_Datta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Debajyoti_Datta1">Debajyoti Datta</a>, <a href="https://openreview.net/profile?id=~Jonathan_Chang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Chang2">Jonathan Chang</a>, <a href="https://openreview.net/profile?id=~Mike_Tian-Jian_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Tian-Jian_Jiang1">Mike Tian-Jian Jiang</a>, <a href="https://openreview.net/profile?id=~Han_Wang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Wang9">Han Wang</a>, <a href="https://openreview.net/profile?id=~Matteo_Manica1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matteo_Manica1">Matteo Manica</a>, <a href="https://openreview.net/profile?id=~Sheng_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheng_Shen2">Sheng Shen</a>, <a href="https://openreview.net/profile?id=~Zheng_Xin_Yong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zheng_Xin_Yong1">Zheng Xin Yong</a>, <a href="https://openreview.net/profile?id=~Harshit_Pandey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harshit_Pandey1">Harshit Pandey</a>, <a href="https://openreview.net/profile?id=~Rachel_Bawden1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rachel_Bawden1">Rachel Bawden</a>, <a href="https://openreview.net/profile?id=~Thomas_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Wang1">Thomas Wang</a>, <a href="https://openreview.net/profile?id=~Trishala_Neeraj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trishala_Neeraj1">Trishala Neeraj</a>, <a href="https://openreview.net/profile?id=~Jos_Rozen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jos_Rozen1">Jos Rozen</a>, <a href="https://openreview.net/profile?id=~Abheesht_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abheesht_Sharma1">Abheesht Sharma</a>, <a href="https://openreview.net/profile?id=~Andrea_Santilli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrea_Santilli1">Andrea Santilli</a>, <a href="https://openreview.net/profile?id=~Thibault_Fevry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thibault_Fevry1">Thibault Fevry</a>, <a href="https://openreview.net/profile?id=~Jason_Alan_Fries1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_Alan_Fries1">Jason Alan Fries</a>, <a href="https://openreview.net/profile?id=~Ryan_Teehan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryan_Teehan1">Ryan Teehan</a>, <a href="https://openreview.net/profile?id=~Teven_Le_Scao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Teven_Le_Scao1">Teven Le Scao</a>, <a href="https://openreview.net/profile?id=~Stella_Biderman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stella_Biderman1">Stella Biderman</a>, <a href="https://openreview.net/profile?id=~Leo_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leo_Gao1">Leo Gao</a>, <a href="https://openreview.net/profile?id=~Thomas_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Wolf1">Thomas Wolf</a>, <a href="https://openreview.net/profile?id=~Alexander_M_Rush1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_M_Rush1">Alexander M Rush</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#9Vrb9D0WI4-details-542" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9Vrb9D0WI4-details-542"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language modelsâ€™ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16Ã— its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6Ã— its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="57PipS27Km" data-number="1057">
        <h4>
          <a href="https://openreview.net/forum?id=57PipS27Km">
              Continuous-Time Meta-Learning with Forward Mode Differentiation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=57PipS27Km" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tristan_Deleu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tristan_Deleu1">Tristan Deleu</a>, <a href="https://openreview.net/profile?id=~David_Kanaa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Kanaa1">David Kanaa</a>, <a href="https://openreview.net/profile?id=~Leo_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leo_Feng1">Leo Feng</a>, <a href="https://openreview.net/profile?id=~Giancarlo_Kerg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giancarlo_Kerg1">Giancarlo Kerg</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Guillaume_Lajoie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guillaume_Lajoie1">Guillaume Lajoie</a>, <a href="https://openreview.net/profile?id=~Pierre-Luc_Bacon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre-Luc_Bacon1">Pierre-Luc Bacon</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#57PipS27Km-details-166" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="57PipS27Km-details-166"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-learning, few-shot learning, dynamical systems</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we  devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">COMLN is a new meta-learning algorithm, where adaptation follows a gradient flow. It enables learning the amount of adaptation using SGD. We devise a novel efficient algorithm to compute the meta-gradients of COMLN, based on forward-mode diff.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=57PipS27Km&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zXM0b4hi5_B" data-number="1035">
        <h4>
          <a href="https://openreview.net/forum?id=zXM0b4hi5_B">
              On the relation between statistical learning and perceptual distances
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zXM0b4hi5_B" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Hepburn2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Hepburn2">Alexander Hepburn</a>, <a href="https://openreview.net/profile?id=~Valero_Laparra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valero_Laparra1">Valero Laparra</a>, <a href="https://openreview.net/profile?id=~Raul_Santos-Rodriguez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raul_Santos-Rodriguez1">Raul Santos-Rodriguez</a>, <a href="https://openreview.net/profile?id=~Johannes_Ball%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_BallÃ©1">Johannes BallÃ©</a>, <a href="https://openreview.net/profile?id=~Jesus_Malo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesus_Malo1">Jesus Malo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zXM0b4hi5_B-details-407" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zXM0b4hi5_B-details-407"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vA7doMdgi75" data-number="1016">
        <h4>
          <a href="https://openreview.net/forum?id=vA7doMdgi75">
              Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vA7doMdgi75" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Paris_Giampouras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paris_Giampouras1">Paris Giampouras</a>, <a href="https://openreview.net/profile?id=~Benjamin_David_Haeffele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_David_Haeffele1">Benjamin David Haeffele</a>, <a href="https://openreview.net/profile?id=~Rene_Vidal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rene_Vidal1">Rene Vidal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vA7doMdgi75-details-728" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vA7doMdgi75-details-728"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation learning, robust subspace recovery, dual principals component pursuit, outliers, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints --adopted in previous DPCP formulations-- are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \emph{unknown dimension}. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the robust subspace recovery problem when subspace codimension is unknown.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MkTPtnjeYTV" data-number="1003">
        <h4>
          <a href="https://openreview.net/forum?id=MkTPtnjeYTV">
              On the Optimal Memorization Power of ReLU Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MkTPtnjeYTV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gal_Vardi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Vardi1">Gal Vardi</a>, <a href="https://openreview.net/profile?id=~Gilad_Yehudai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gilad_Yehudai2">Gilad Yehudai</a>, <a href="https://openreview.net/profile?id=~Ohad_Shamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ohad_Shamir1">Ohad Shamir</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MkTPtnjeYTV-details-525" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MkTPtnjeYTV-details-525"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Expressivness, Memorization, Theory, VC-dimension, Deep learning theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\tilde{O}\left(\sqrt{N}\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\Omega(\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \leq L \leq \sqrt{N}$, for memorizing $N$ samples using $\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that ReLU neural networks can memorize N samples using \sqrt{N} parameters, and prove that up to logarithmic terms this is the optimal solution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6Tk2noBdvxt" data-number="904">
        <h4>
          <a href="https://openreview.net/forum?id=6Tk2noBdvxt">
              Programmatic Reinforcement Learning without Oracles
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6Tk2noBdvxt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wenjie_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjie_Qiu1">Wenjie Qiu</a>, <a href="https://openreview.net/profile?id=~He_Zhu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~He_Zhu4">He Zhu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6Tk2noBdvxt-details-424" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Tk2noBdvxt-details-424"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Programmatic Reinforcement Learning, Compositional Reinforcement Learning, Program Synthesis, Differentiable Architecture Search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable. The code of this work is available at https://github.com/RU-Automated-Reasoning-Group/pi-PRL.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a differentiable program architecture search framework to synthesize interpretable, generalizable, and compositional programs for controlling reinforcement learning applications.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LtKcMgGOeLt" data-number="869">
        <h4>
          <a href="https://openreview.net/forum?id=LtKcMgGOeLt">
              When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LtKcMgGOeLt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiangning_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangning_Chen1">Xiangning Chen</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>, <a href="https://openreview.net/profile?id=~Boqing_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boqing_Gong1">Boqing Gong</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LtKcMgGOeLt-details-377" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LtKcMgGOeLt-details-377"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and +11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \url{https://github.com/google-research/vision_transformer}.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="01AMRlen9wJ" data-number="846">
        <h4>
          <a href="https://openreview.net/forum?id=01AMRlen9wJ">
              Online Hyperparameter Meta-Learning with Hypergradient Distillation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=01AMRlen9wJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hae_Beom_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hae_Beom_Lee1">Hae Beom Lee</a>, <a href="https://openreview.net/profile?id=~Hayeon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hayeon_Lee1">Hayeon Lee</a>, <a href="https://openreview.net/profile?id=~JaeWoong_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~JaeWoong_Shin1">JaeWoong Shin</a>, <a href="https://openreview.net/profile?id=~Eunho_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunho_Yang1">Eunho Yang</a>, <a href="https://openreview.net/profile?id=~Timothy_Hospedales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Hospedales1">Timothy Hospedales</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#01AMRlen9wJ-details-918" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="01AMRlen9wJ-details-918"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hyperparameter Optimization, Meta-learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length, Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on three different meta-learning methods and two benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a gradient-based hyperparameter optimization method based on the idea of knowledge distillation, which is fully online and applicable to high-dimensional hyperparameters.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=01AMRlen9wJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LBvk4QWIUpm" data-number="833">
        <h4>
          <a href="https://openreview.net/forum?id=LBvk4QWIUpm">
              Tighter Sparse Approximation Bounds for ReLU Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LBvk4QWIUpm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Carles_Domingo-Enrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carles_Domingo-Enrich1">Carles Domingo-Enrich</a>, <a href="https://openreview.net/profile?id=~Youssef_Mroueh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youssef_Mroueh1">Youssef Mroueh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Nov 2021)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LBvk4QWIUpm-details-168" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LBvk4QWIUpm-details-168"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural network, two-layer, infinite-width, approximation, sparse, Radon transform, Fourier transform, ReLU</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski &amp; Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\mathcal{B}_R(\mathbb{R}^d)$ up to error $\epsilon$, when the Fourier based quantity $C_f = \int_{\mathbb{R}^d} \|\xi\|^2 |\hat{f}(\xi)| \ d\xi$ is finite. More recently Ongie et al. (2019) used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based $\mathcal{R}$-norms and show that a function defined on $\mathbb{R}^d$ can be represented as an infinite-width two-layer neural network if and only if its $\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et al. (2019) and define similar Radon-based semi-norms ($\mathcal{R}, \mathcal{U}$-norms) such that a function admits an infinite-width neural network representation on a bounded open set $\mathcal{U} \subseteq \mathbb{R}^d$ when its $\mathcal{R}, \mathcal{U}$-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman (1993); Klusowski &amp; Barron (2018). Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show conditions under which a function can be represented by an infinite-width neural network on a bounded set, and refine sparse neural network approximation bounds.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LBvk4QWIUpm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vwj6aUeocyf" data-number="831">
        <h4>
          <a href="https://openreview.net/forum?id=vwj6aUeocyf">
              Long Expressive Memory for Sequence Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vwj6aUeocyf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~T._Konstantin_Rusch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~T._Konstantin_Rusch1">T. Konstantin Rusch</a>, <a href="https://openreview.net/profile?id=~Siddhartha_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddhartha_Mishra1">Siddhartha Mishra</a>, <a href="https://openreview.net/profile?id=~N._Benjamin_Erichson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~N._Benjamin_Erichson1">N. Benjamin Erichson</a>, <a href="https://openreview.net/profile?id=~Michael_W._Mahoney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W._Mahoney1">Michael W. Mahoney</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 26 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vwj6aUeocyf-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vwj6aUeocyf-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequence modeling, long-term dependencies, multiscale ordinary differential equations, dynamical systems</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel method for sequence modeling based on multiscale ODEs that is provably able to learn very long-term dependencies while being sufficiently expressive to outperform state-of-the-art recurrent sequence models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vwj6aUeocyf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LzQQ89U1qm_" data-number="803">
        <h4>
          <a href="https://openreview.net/forum?id=LzQQ89U1qm_">
              Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LzQQ89U1qm_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiehui_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiehui_Xu1">Jiehui Xu</a>, <a href="https://openreview.net/profile?id=~Haixu_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haixu_Wu1">Haixu Wu</a>, <a href="https://openreview.net/profile?id=~Jianmin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianmin_Wang1">Jianmin Wang</a>, <a href="https://openreview.net/profile?id=~Mingsheng_Long5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingsheng_Long5">Mingsheng Long</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LzQQ89U1qm_-details-358" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LzQQ89U1qm_-details-358"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Time series anomaly detection, Transformers, Anomaly attention, Association discrepancy</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space &amp; earth exploration, and water treatment.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper detects time series anomalies from a new association-based dimension. We find an inherently normal-abnormal distinguishable evidence as Association Discrepancy. Co-designed with this evidence, our model achieves the SOTA on six benchmarks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TIdIXIpzhoI" data-number="769">
        <h4>
          <a href="https://openreview.net/forum?id=TIdIXIpzhoI">
              Progressive Distillation for Fast Sampling of Diffusion Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TIdIXIpzhoI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tim_Salimans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Salimans1">Tim Salimans</a>, <a href="https://openreview.net/profile?id=~Jonathan_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Ho1">Jonathan Ho</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TIdIXIpzhoI-details-452" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TIdIXIpzhoI-details-452"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Diffusion Models, Generative Models, fast sampling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Diffusion models now need just 4 sampling steps to produce high quality samples.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TIdIXIpzhoI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7gWSJrP3opB" data-number="736">
        <h4>
          <a href="https://openreview.net/forum?id=7gWSJrP3opB">
              A General Analysis of Example-Selection for Stochastic Gradient Descent
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7gWSJrP3opB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yucheng_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yucheng_Lu1">Yucheng Lu</a>, <a href="https://openreview.net/profile?id=~Si_Yi_Meng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Si_Yi_Meng2">Si Yi Meng</a>, <a href="https://openreview.net/profile?id=~Christopher_De_Sa2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_De_Sa2">Christopher De Sa</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7gWSJrP3opB-details-640" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7gWSJrP3opB-details-640"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Training example order in SGD has long been known to affect convergence rate. Recent results show that accelerated rates are possible in a variety of cases for permutation-based sample orders, in which each example from the training set is used once before any example is reused. In this paper, we develop a broad condition on the sequence of examples used by SGD that is sufficient to prove tight convergence rates in both strongly convex and non-convex settings. We show that our approach suffices to recover, and in some cases improve upon, previous state-of-the-art analyses for four known example-selection schemes: (1) shuffle once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4) Markov Chain Gradient Descent. Motivated by our theory, we propose two new example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve unprecedented accelerated convergence rates for learning with data augmentation. Second, we greedily choose a fixed scan-order to minimize the metric used in our condition and show that we can obtain more accurate solutions from the same number of epochs of SGD. We conclude by empirically demonstrating the utility of our approach for both convex linear-model and deep learning tasks. Our code is available at: https://github.com/EugeneLYC/qmc-ordering.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WvOGCEAQhxl" data-number="715">
        <h4>
          <a href="https://openreview.net/forum?id=WvOGCEAQhxl">
              Assessing Generalization of SGD via Disagreement
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WvOGCEAQhxl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yiding_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiding_Jiang2">Yiding Jiang</a>, <a href="https://openreview.net/profile?id=~Vaishnavh_Nagarajan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vaishnavh_Nagarajan3">Vaishnavh Nagarajan</a>, <a href="https://openreview.net/profile?id=~Christina_Baek2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christina_Baek2">Christina Baek</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 08 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WvOGCEAQhxl-details-645" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WvOGCEAQhxl-details-645"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Generalization, Deep Learning, Empirical Phenomenon, Accuracy Estimation, Stochastic Gradient Descent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in Nakkiran&amp;Bansal 20, which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide a surprisingly simple technique to accurately estimate the test error of deep neural networks using unlabeled data and we prove that this works because SGD ensembles are naturally well-calibrated.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YZHES8wIdE" data-number="710">
        <h4>
          <a href="https://openreview.net/forum?id=YZHES8wIdE">
              Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YZHES8wIdE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Haichao_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haichao_Zhang4">Haichao Zhang</a>, <a href="https://openreview.net/profile?id=~Wei_Xu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Xu13">Wei Xu</a>, <a href="https://openreview.net/profile?id=~Haonan_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haonan_Yu5">Haonan Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YZHES8wIdE-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YZHES8wIdE-details-997"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly,  since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Temporally coordinated exploration in reinforcement learning using Generative Planning Method.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Y4cs1Z3HnqL" data-number="694">
        <h4>
          <a href="https://openreview.net/forum?id=Y4cs1Z3HnqL">
              Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Y4cs1Z3HnqL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chenjia_Bai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenjia_Bai2">Chenjia Bai</a>, <a href="https://openreview.net/profile?id=~Lingxiao_Wang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lingxiao_Wang6">Lingxiao Wang</a>, <a href="https://openreview.net/profile?id=~Zhuoran_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuoran_Yang1">Zhuoran Yang</a>, <a href="https://openreview.net/profile?id=~Zhi-Hong_Deng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhi-Hong_Deng2">Zhi-Hong Deng</a>, <a href="https://openreview.net/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>, <a href="https://openreview.net/profile?id=~Peng_Liu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peng_Liu5">Peng Liu</a>, <a href="https://openreview.net/profile?id=~Zhaoran_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaoran_Wang1">Zhaoran Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">31 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Y4cs1Z3HnqL-details-459" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y4cs1Z3HnqL-details-459"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pessimistic Bootstrapping, Bootstrapped Q-functions, Uncertainty Estimation, Offline Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose pessimistic bootstrapping as a purely uncertainty-driven algorithm for offline Reinforcement Learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>


<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â«</a>
      </li>
      <li class="  left-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€¹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class=" active " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  right-arrow" data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>


</div>
    <div role="tabpanel" class="tab-pane fade" id="poster-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="HndgQudNb91" data-number="4722">
        <h4>
          <a href="https://openreview.net/forum?id=HndgQudNb91">
              Learning to Downsample for Segmentation of Ultra-High Resolution Images
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HndgQudNb91" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chen_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen_Jin3">Chen Jin</a>, <a href="https://openreview.net/profile?id=~Ryutaro_Tanno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryutaro_Tanno1">Ryutaro Tanno</a>, <a href="https://openreview.net/profile?id=~Thomy_Mertzanidou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomy_Mertzanidou1">Thomy Mertzanidou</a>, <a href="https://openreview.net/profile?id=~Eleftheria_Panagiotaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleftheria_Panagiotaki1">Eleftheria Panagiotaki</a>, <a href="https://openreview.net/profile?id=~Daniel_C._Alexander1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_C._Alexander1">Daniel C. Alexander</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HndgQudNb91-details-879" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HndgQudNb91-details-879"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ultra-high resolution image segmentation, non-uniform dowmsampling, efficient segmentation, large volume image segmentation, medical image segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance
        because the segmentation difficulty varies spatially (see Figure 1 â€œUniformâ€). We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling
        module learns to sample more densely at difficult locations, thereby improving the segmentation performance (see Figure 1 "Ours"). Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method for learning to downsample ultra high-resolution images that reflects the importance of each location.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7fFO4cMBx_9" data-number="4721">
        <h4>
          <a href="https://openreview.net/forum?id=7fFO4cMBx_9">
              Variational Neural Cellular Automata
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7fFO4cMBx_9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rasmus_Berg_Palm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rasmus_Berg_Palm1">Rasmus Berg Palm</a>, <a href="https://openreview.net/profile?id=~Miguel_Gonz%C3%A1lez_Duque1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miguel_GonzÃ¡lez_Duque1">Miguel GonzÃ¡lez Duque</a>, <a href="https://openreview.net/profile?id=~Shyam_Sudhakaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shyam_Sudhakaran1">Shyam Sudhakaran</a>, <a href="https://openreview.net/profile?id=~Sebastian_Risi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sebastian_Risi1">Sebastian Risi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7fFO4cMBx_9-details-6" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7fFO4cMBx_9-details-6"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Cellular Automata, Cellular Automata, Self-Organization, Generative Models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process.
        Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, the self-organizing nature bestows the VNCA with some inherent robustness against perturbations in the early stages of growth.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose and evaluate the Variational Neural Cellular Automata, a self-organising generative model based on neural cellular automata</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FKp8-pIRo3y" data-number="4719">
        <h4>
          <a href="https://openreview.net/forum?id=FKp8-pIRo3y">
              Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FKp8-pIRo3y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Todor_Davchev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Todor_Davchev1">Todor Davchev</a>, <a href="https://openreview.net/profile?id=~Oleg_Olegovich_Sushkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oleg_Olegovich_Sushkov1">Oleg Olegovich Sushkov</a>, <a href="https://openreview.net/profile?id=~Jean-Baptiste_Regli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jean-Baptiste_Regli1">Jean-Baptiste Regli</a>, <a href="https://openreview.net/profile?id=~Stefan_Schaal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefan_Schaal1">Stefan Schaal</a>, <a href="https://openreview.net/profile?id=~Yusuf_Aytar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yusuf_Aytar1">Yusuf Aytar</a>, <a href="https://openreview.net/profile?id=~Markus_Wulfmeier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Markus_Wulfmeier1">Markus Wulfmeier</a>, <a href="https://openreview.net/profile?id=~Jon_Scholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jon_Scholz1">Jon Scholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 08 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FKp8-pIRo3y-details-935" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FKp8-pIRo3y-details-935"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">goal-conditioned reinforcement learning, learning from demonstrations, long-horizon dexterous manipulation, bi-manual manipulation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. 
        Various tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="KntaNRo6R48" data-number="4717">
        <h4>
          <a href="https://openreview.net/forum?id=KntaNRo6R48">
              L0-Sparse Canonical Correlation Analysis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=KntaNRo6R48" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ofir_Lindenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Lindenbaum1">Ofir Lindenbaum</a>, <a href="https://openreview.net/profile?id=~Moshe_Salhov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Moshe_Salhov1">Moshe Salhov</a>, <a href="https://openreview.net/profile?id=~Amir_Averbuch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Averbuch1">Amir Averbuch</a>, <a href="https://openreview.net/profile?id=~Yuval_Kluger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuval_Kluger1">Yuval Kluger</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 07 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#KntaNRo6R48-details-899" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KntaNRo6R48-details-899"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \textit{canonically correlated variates}. Here, we propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="48" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities.
        Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="49" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-regularized correlation loss. 
        We further propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="50" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="51" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-CCA method for learning correlated representations based on sparse subsets of variables from two observed modalities.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=KntaNRo6R48&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="B7ZbqNLDn-_" data-number="4715">
        <h4>
          <a href="https://openreview.net/forum?id=B7ZbqNLDn-_">
              Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=B7ZbqNLDn-_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheikh_Shams_Azam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheikh_Shams_Azam1">Sheikh Shams Azam</a>, <a href="https://openreview.net/profile?id=~Seyyedali_Hosseinalipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyyedali_Hosseinalipour1">Seyyedali Hosseinalipour</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Christopher_Brinton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_Brinton1">Christopher Brinton</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#B7ZbqNLDn-_-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B7ZbqNLDn-_-details-997"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Distributed Machine Learning, Federated Learning, Gradient Subspace, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance. Motivated by this, we propose the "Look-back Gradient Multiplier" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning. Operationally, LBGM recycles the gradients between model update rounds to significantly reduce the number of parameters to be propagated through the system. We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We observe that "gradient-space is low rank" and propose the LBGM algorithm that utilitizes this low-rank property to recycle gradients between model update rounds in federated learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ucASPPD9GKN" data-number="4711">
        <h4>
          <a href="https://openreview.net/forum?id=ucASPPD9GKN">
              Is Homophily a Necessity for Graph Neural Networks?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ucASPPD9GKN" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yao_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yao_Ma3">Yao Ma</a>, <a href="https://openreview.net/profile?id=~Xiaorui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaorui_Liu1">Xiaorui Liu</a>, <a href="https://openreview.net/profile?id=~Neil_Shah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Neil_Shah2">Neil Shah</a>, <a href="https://openreview.net/profile?id=~Jiliang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiliang_Tang1">Jiliang Tang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ucASPPD9GKN-details-667" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ucASPPD9GKN-details-667"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification,  GNNs are widely believed to work well due to the homophily assumption (``like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance.  We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations.  Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ucASPPD9GKN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ve0Wth3ptT_" data-number="4703">
        <h4>
          <a href="https://openreview.net/forum?id=Ve0Wth3ptT_">
              DEGREE: Decomposition Based Explanation for Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ve0Wth3ptT_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qizhang_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qizhang_Feng1">Qizhang Feng</a>, <a href="https://openreview.net/profile?id=~Ninghao_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ninghao_Liu2">Ninghao Liu</a>, <a href="https://openreview.net/profile?id=~Fan_Yang27" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fan_Yang27">Fan Yang</a>, <a href="https://openreview.net/profile?id=~Ruixiang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruixiang_Tang1">Ruixiang Tang</a>, <a href="https://openreview.net/profile?id=~Mengnan_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengnan_Du1">Mengnan Du</a>, <a href="https://openreview.net/profile?id=~Xia_Hu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xia_Hu4">Xia Hu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ve0Wth3ptT_-details-461" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ve0Wth3ptT_-details-461"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">XAI, GNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new decomposition based explanation for Graph Neural Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="T0B9AoM_bFg" data-number="4668">
        <h4>
          <a href="https://openreview.net/forum?id=T0B9AoM_bFg">
              Improving Mutual Information Estimation with Annealed and Energy-Based Bounds
          </a>
        
          
            <a href="https://openreview.net/pdf?id=T0B9AoM_bFg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rob_Brekelmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rob_Brekelmans1">Rob Brekelmans</a>, <a href="https://openreview.net/profile?id=~Sicong_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sicong_Huang1">Sicong Huang</a>, <a href="https://openreview.net/profile?id=~Marzyeh_Ghassemi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marzyeh_Ghassemi2">Marzyeh Ghassemi</a>, <a href="https://openreview.net/profile?id=~Greg_Ver_Steeg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Greg_Ver_Steeg1">Greg Ver Steeg</a>, <a href="https://openreview.net/profile?id=~Roger_Baker_Grosse1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roger_Baker_Grosse1">Roger Baker Grosse</a>, <a href="https://openreview.net/profile?id=~Alireza_Makhzani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Makhzani1">Alireza Makhzani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#T0B9AoM_bFg-details-435" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T0B9AoM_bFg-details-435"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">mutual information estimation, annealed importance sampling, energy-based models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We derive new annealed importance sampling and energy-based bounds, resulting in vastly more accurate estimates of mutual information.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bp-LJ4y_XC" data-number="4662">
        <h4>
          <a href="https://openreview.net/forum?id=bp-LJ4y_XC">
              Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bp-LJ4y_XC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xueyuan_She1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xueyuan_She1">Xueyuan She</a>, <a href="https://openreview.net/profile?id=~Saurabh_Dash1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Dash1">Saurabh Dash</a>, <a href="https://openreview.net/profile?id=~Saibal_Mukhopadhyay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saibal_Mukhopadhyay1">Saibal Mukhopadhyay</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bp-LJ4y_XC-details-982" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bp-LJ4y_XC-details-982"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spiking neural network, spatiotemporal processing, feedforward network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A theoretical approache to study the approximation capability of feedforward spiking neural network and optimization methods for such network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=bp-LJ4y_XC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="nwKXyFvaUm" data-number="4660">
        <h4>
          <a href="https://openreview.net/forum?id=nwKXyFvaUm">
              Diverse Client Selection for Federated Learning via Submodular Maximization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=nwKXyFvaUm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ravikumar_Balakrishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ravikumar_Balakrishnan1">Ravikumar Balakrishnan</a>, <a href="https://openreview.net/profile?id=~Tian_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tian_Li1">Tian Li</a>, <a href="https://openreview.net/profile?id=~Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyi_Zhou1">Tianyi Zhou</a>, <a href="https://openreview.net/profile?email=nageen.himayat%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="nageen.himayat@intel.com">Nageen Himayat</a>, <a href="https://openreview.net/profile?id=~Virginia_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Virginia_Smith1">Virginia Smith</a>, <a href="https://openreview.net/profile?id=~Jeff_Bilmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeff_Bilmes1">Jeff Bilmes</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#nwKXyFvaUm-details-211" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nwKXyFvaUm-details-211"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, submodularity, diversity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In every communication round of federated learning, a random subset of clients communicate  their  model  updates  back  to  the  server  which  then  aggregates them all.  The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server.  Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce â€œfederated averaging with diverse client selection (DivFL)â€. We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper addresses a key challenge of selecting the most representative clients iteratively for federated learning through formulating it as a submodular optimization problem and developing efficient algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="jT1EwXu-4hj" data-number="4651">
        <h4>
          <a href="https://openreview.net/forum?id=jT1EwXu-4hj">
              From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=jT1EwXu-4hj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Da_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Da_Xu2">Da Xu</a>, <a href="https://openreview.net/profile?id=~Yuting_Ye3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuting_Ye3">Yuting Ye</a>, <a href="https://openreview.net/profile?id=~Chuanwei_Ruan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chuanwei_Ruan1">Chuanwei Ruan</a>, <a href="https://openreview.net/profile?id=~Evren_Korpeoglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Evren_Korpeoglu1">Evren Korpeoglu</a>, <a href="https://openreview.net/profile?id=~Sushant_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sushant_Kumar1">Sushant Kumar</a>, <a href="https://openreview.net/profile?id=~Kannan_Achan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kannan_Achan1">Kannan Achan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#jT1EwXu-4hj-details-611" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jT1EwXu-4hj-details-611"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Information retrieval, Learning theory, Causal inference, Missing data, Overlapping, Reweighting, Optimal transport</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game.
        We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose and study a novel domain-transportation view for optimizing recommendation for information retrieval systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=jT1EwXu-4hj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JxFgJbZ-wft" data-number="4647">
        <h4>
          <a href="https://openreview.net/forum?id=JxFgJbZ-wft">
              Variational Predictive Routing with Nested Subjective Timescales
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JxFgJbZ-wft" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexey_Zakharov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_Zakharov1">Alexey Zakharov</a>, <a href="https://openreview.net/profile?id=~Qinghai_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghai_Guo1">Qinghai Guo</a>, <a href="https://openreview.net/profile?id=~Zafeirios_Fountas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zafeirios_Fountas1">Zafeirios Fountas</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JxFgJbZ-wft-details-459" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JxFgJbZ-wft-details-459"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hierarchical temporal abstraction, event discovery, hierarchical generative models, variational inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) â€“ a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the systemâ€™s latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the modelâ€™s latent hierarchy.  Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Variational inference hierarchical model that relies on a change detection mechanism to impose a nested temporal hierarchy on its latent structure.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RhB1AdoFfGE" data-number="4630">
        <h4>
          <a href="https://openreview.net/forum?id=RhB1AdoFfGE">
              Sample and Computation Redistribution for Efficient Face Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RhB1AdoFfGE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jia_Guo1">Jia Guo</a>, <a href="https://openreview.net/profile?id=~Jiankang_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiankang_Deng1">Jiankang Deng</a>, <a href="https://openreview.net/profile?id=~Alexandros_Lattas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexandros_Lattas1">Alexandros Lattas</a>, <a href="https://openreview.net/profile?id=~Stefanos_Zafeiriou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanos_Zafeiriou1">Stefanos Zafeiriou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RhB1AdoFfGE-details-521" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RhB1AdoFfGE-details-521"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">efficient face detection, computation redistribution, sample redistribution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages. The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="52" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4.78</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> (AP at hard set) while being more than 3<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="53" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> faster on GPUs with VGA-resolution images. Code is available at: https://github.com/deepinsight/insightface/tree/master/detection/scrfd.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We search for optimised computation distribution and training sample distribution for the task of face detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=RhB1AdoFfGE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NkZq4OEYN-" data-number="4629">
        <h4>
          <a href="https://openreview.net/forum?id=NkZq4OEYN-">
              Sound Adversarial Audio-Visual Navigation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NkZq4OEYN-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yinfeng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yinfeng_Yu1">Yinfeng Yu</a>, <a href="https://openreview.net/profile?id=~Wenbing_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wenbing_Huang1">Wenbing Huang</a>, <a href="https://openreview.net/profile?id=~Fuchun_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuchun_Sun2">Fuchun Sun</a>, <a href="https://openreview.net/profile?id=~Changan_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Changan_Chen2">Changan Chen</a>, <a href="https://openreview.net/profile?id=~Yikai_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yikai_Wang2">Yikai Wang</a>, <a href="https://openreview.net/profile?id=~Xiaohong_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaohong_Liu3">Xiaohong Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NkZq4OEYN--details-908" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NkZq4OEYN--details-908"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN .</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work aims to do an adversarial sound intervention for robust audio-visual navigation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NkZq4OEYN-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="12RoR2o32T" data-number="4618">
        <h4>
          <a href="https://openreview.net/forum?id=12RoR2o32T">
              Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=12RoR2o32T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aahlad_Manas_Puli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aahlad_Manas_Puli1">Aahlad Manas Puli</a>, <a href="https://openreview.net/profile?id=~Lily_H_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lily_H_Zhang1">Lily H Zhang</a>, <a href="https://openreview.net/profile?id=~Eric_Karl_Oermann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eric_Karl_Oermann1">Eric Karl Oermann</a>, <a href="https://openreview.net/profile?id=~Rajesh_Ranganath2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rajesh_Ranganath2">Rajesh Ranganath</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#12RoR2o32T-details-842" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="12RoR2o32T-details-842"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spurious correlations, out of distribution generalization, ml for health, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper build models robust to nuisance-induced spurious correlations by constructing a representation that distills out the influence of the nuisance variables, while also maximizing its information with the label.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="OM_lYiHXiCL" data-number="4615">
        <h4>
          <a href="https://openreview.net/forum?id=OM_lYiHXiCL">
              AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=OM_lYiHXiCL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Junfeng_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junfeng_Guo2">Junfeng Guo</a>, <a href="https://openreview.net/profile?id=~Ang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ang_Li1">Ang Li</a>, <a href="https://openreview.net/profile?id=~Cong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cong_Liu2">Cong Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 25 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#OM_lYiHXiCL-details-573" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OM_lYiHXiCL-details-573"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples,  which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution;  a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ECQL05ub0J" data-number="4609">
        <h4>
          <a href="https://openreview.net/forum?id=5ECQL05ub0J">
              Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ECQL05ub0J" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kirby_Banman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kirby_Banman1">Kirby Banman</a>, <a href="https://openreview.net/profile?id=~Garnet_Liam_Peet-Pare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Garnet_Liam_Peet-Pare1">Garnet Liam Peet-Pare</a>, <a href="https://openreview.net/profile?id=~Nidhi_Hegde1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nidhi_Hegde1">Nidhi Hegde</a>, <a href="https://openreview.net/profile?id=~Alona_Fyshe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alona_Fyshe1">Alona Fyshe</a>, <a href="https://openreview.net/profile?id=~Martha_White1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martha_White1">Martha White</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ECQL05ub0J-details-614" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ECQL05ub0J-details-614"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">optimization, momentum, stochastic gradient descent, non-iid sampling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid  sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that SGDm under covariate shift with fixed step-size can be unstable and diverge due to a phenomenon known as parametric resonance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ECQL05ub0J&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WqoBaaPHS-" data-number="4592">
        <h4>
          <a href="https://openreview.net/forum?id=WqoBaaPHS-">
              Top-label calibration and multiclass-to-binary reductions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WqoBaaPHS-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chirag_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chirag_Gupta1">Chirag Gupta</a>, <a href="https://openreview.net/profile?id=~Aaditya_Ramdas2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aaditya_Ramdas2">Aaditya Ramdas</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WqoBaaPHS--details-916" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WqoBaaPHS--details-916"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">calibration, multiclass, uncertainty quantification, distribution-free, histogram binning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported and we must condition on what is reported. However, the popular notion of confidence calibration erroneously skips this conditioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction framework that unifies confidence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to different binary calibration problems; various types of multiclass calibration can then be achieved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose top-label calibration, a new and arguably natural notion for multiclass calibration, along with 'wrapper' calibration algorithms that reduce multiclass calibration to binary calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WqoBaaPHS-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JfaWawZ8BmX" data-number="4589">
        <h4>
          <a href="https://openreview.net/forum?id=JfaWawZ8BmX">
              Anisotropic Random Feature Regression in High Dimensions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JfaWawZ8BmX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gabriel_Mel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gabriel_Mel1">Gabriel Mel</a>, <a href="https://openreview.net/profile?id=~Jeffrey_Pennington1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeffrey_Pennington1">Jeffrey Pennington</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JfaWawZ8BmX-details-600" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JfaWawZ8BmX-details-600"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random feature models, high dimensional asymptotics, generalization, learning curves, double descent, multiple descent, alignment</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but typically these analyses have utilized isotropic distributional assumptions on the underlying data generation process, thereby failing to provide a realistic characterization of real-world models that are designed to identify and harness the structure in natural data. In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function. We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment. We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs. Finally, these signatures are related to phase transitions in the spectrum of the feature kernel matrix, and unlike the double descent peak, persist even under optimal regularization.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We derive exact asymptotic formulas for the total error, bias, and variance of random feature regression with anisotropic inputs and target weights, and identify a new type of singularity in sample-wise learning curves. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JfaWawZ8BmX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="L01Nn_VJ9i" data-number="4586">
        <h4>
          <a href="https://openreview.net/forum?id=L01Nn_VJ9i">
              Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future
          </a>
        
          
            <a href="https://openreview.net/pdf?id=L01Nn_VJ9i" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Harshavardhan_Kamarthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Harshavardhan_Kamarthi1">Harshavardhan Kamarthi</a>, <a href="https://openreview.net/profile?id=~Alexander_Rodr%C3%ADguez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_RodrÃ­guez1">Alexander RodrÃ­guez</a>, <a href="https://openreview.net/profile?id=~B._Aditya_Prakash2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~B._Aditya_Prakash2">B. Aditya Prakash</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#L01Nn_VJ9i-details-287" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="L01Nn_VJ9i-details-287"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Epidemic Forecasting, Data revisions, Graph Representation learning, Time Series Forecasting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called â€˜backfillâ€™ phenomenon and its effect on model performance have been barely addressed in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. 
        We construct a detailed dataset composed of relevant signals over the past year of the pandemic. 
        We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting. Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines. In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the problem of multi-variate backfill for both features and targets and show how to leverage our insights for more general neural framework to improve both model predictions and evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=L01Nn_VJ9i&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lrocYB-0ST2" data-number="4570">
        <h4>
          <a href="https://openreview.net/forum?id=lrocYB-0ST2">
              Approximation and Learning with Deep Convolutional Models: a Kernel Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lrocYB-0ST2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alberto_Bietti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alberto_Bietti1">Alberto Bietti</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lrocYB-0ST2-details-389" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lrocYB-0ST2-details-389"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">kernel methods, deep learning theory, convolution, approximation, generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the inductive bias of multi-layer convolutional models through a kernel lens, showing generalization benefits of various architectural choices such as locality, depth, and pooling layers.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lrocYB-0ST2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vgqS1vkkCbE" data-number="4569">
        <h4>
          <a href="https://openreview.net/forum?id=vgqS1vkkCbE">
              Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vgqS1vkkCbE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dhruv_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dhruv_Shah1">Dhruv Shah</a>, <a href="https://openreview.net/profile?id=~Peng_Xu9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Xu9">Peng Xu</a>, <a href="https://openreview.net/profile?id=~Yao_Lu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yao_Lu13">Yao Lu</a>, <a href="https://openreview.net/profile?id=~Ted_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ted_Xiao1">Ted Xiao</a>, <a href="https://openreview.net/profile?id=~Alexander_T_Toshev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_T_Toshev1">Alexander T Toshev</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~brian_ichter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~brian_ichter1">brian ichter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vgqS1vkkCbE-details-977" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vgqS1vkkCbE-details-977"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">hierarchical reinforcement learning, planning, representation learning, robotics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a  representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce value function spaces, a learned representation of state through the values of low-level skills, which capture affordances and ignores distractors to enable long-horizon reasoning and zero-shot generalization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gNp54NxHUPJ" data-number="4544">
        <h4>
          <a href="https://openreview.net/forum?id=gNp54NxHUPJ">
              Fast Regression for Structured Inputs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gNp54NxHUPJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raphael_A_Meyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raphael_A_Meyer1">Raphael A Meyer</a>, <a href="https://openreview.net/profile?id=~Cameron_N_Musco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cameron_N_Musco1">Cameron N Musco</a>, <a href="https://openreview.net/profile?id=~Christopher_P_Musco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_P_Musco1">Christopher P Musco</a>, <a href="https://openreview.net/profile?id=~David_Woodruff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Woodruff1">David Woodruff</a>, <a href="https://openreview.net/profile?id=~Samson_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samson_Zhou1">Samson Zhou</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gNp54NxHUPJ-details-480" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gNp54NxHUPJ-details-480"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">regression, sublinear time algorithm, structured input</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="54" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression problem, which requires finding <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="55" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> that minimizes <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="56" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="ORD">âˆ¥</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">A</mi></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>âˆ’</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><msub><mo data-mjx-texclass="ORD">âˆ¥</mo><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> for a matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="57" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">A</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>Ã—</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> and response vector <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="58" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><mo>âˆˆ</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="59" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is very large. However, all known subsampling approaches have run time that depends exponentially on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="60" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>, typically, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="61" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, which can be prohibitively expensive. 
        
        We improve on this work by showing that for a large class of common \emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="62" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression that depend polynomially on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="63" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>. For example, we give an algorithm for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="64" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression on Vandermonde matrices that runs in time <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="65" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-msup space="2"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.421em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D714 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mtext><mjx-mstyle><mjx-mspace style="width: 0.167em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><msup><mi>log</mi><mn>3</mn></msup><mo data-mjx-texclass="NONE">â¡</mo><mi>n</mi><mo>+</mo><mo stretchy="false">(</mo><mi>d</mi><msup><mi>p</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mn>0.5</mn><mo>+</mo><mi>Ï‰</mi></mrow></msup><mo>â‹…</mo><mtext>polylog</mtext><mstyle scriptlevel="0"><mspace width="0.167em"></mspace></mstyle><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="66" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D714 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï‰</mi></math></mjx-assistive-mml></mjx-container> is the exponent of matrix multiplication. The polynomial dependence on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="67" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> crucially allows our algorithms to extend naturally to efficient algorithms for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="68" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi mathvariant="normal">âˆž</mi></msub></math></mjx-assistive-mml></mjx-container> regression, via approximation of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="69" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi mathvariant="normal">âˆž</mi></msub></math></mjx-assistive-mml></mjx-container> by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="70" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub></math></mjx-assistive-mml></mjx-container>. Of practical interest, we also develop a new subsampling algorithm for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="71" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container> regression for arbitrary matrices, which is simpler than previous approaches for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="72" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>â‰¥</mo><mn>4</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=gNp54NxHUPJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qhC8mr2LEKq" data-number="4543">
        <h4>
          <a href="https://openreview.net/forum?id=qhC8mr2LEKq">
              CrossBeam: Learning to Search in Bottom-Up Program Synthesis
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qhC8mr2LEKq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kensen_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kensen_Shi1">Kensen Shi</a>, <a href="https://openreview.net/profile?id=~Hanjun_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanjun_Dai1">Hanjun Dai</a>, <a href="https://openreview.net/profile?id=~Kevin_Ellis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kevin_Ellis1">Kevin Ellis</a>, <a href="https://openreview.net/profile?id=~Charles_Sutton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Charles_Sutton1">Charles Sutton</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qhC8mr2LEKq-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qhC8mr2LEKq-details-997"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Program Synthesis, Bottom-Up Search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose training a neural model to learn a hands-on search policy for bottom-up program synthesis, in an effort to tame the search space blowup.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="M6M8BEmd6dq" data-number="4542">
        <h4>
          <a href="https://openreview.net/forum?id=M6M8BEmd6dq">
              PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=M6M8BEmd6dq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seng_Pei_Liew1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seng_Pei_Liew1">Seng Pei Liew</a>, <a href="https://openreview.net/profile?id=~Tsubasa_Takahashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tsubasa_Takahashi1">Tsubasa Takahashi</a>, <a href="https://openreview.net/profile?id=~Michihiko_Ueno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michihiko_Ueno1">Michihiko Ueno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">8 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#M6M8BEmd6dq-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="M6M8BEmd6dq-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Differential Privacy, Generative Model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new framework of synthesizing data using deep generative models in a differentially private manner.
        Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data.
        Hence, no extra privacy costs or model constraints are incurred, in contrast to popular gradient sanitization approaches, which, among other issues, cause degradation in privacy guarantees as the training iteration increases.
        We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well.
        Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aOX3a9q3RVV" data-number="4523">
        <h4>
          <a href="https://openreview.net/forum?id=aOX3a9q3RVV">
              Divisive Feature Normalization Improves Image Recognition Performance in AlexNet
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aOX3a9q3RVV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michelle_Miller3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michelle_Miller3">Michelle Miller</a>, <a href="https://openreview.net/profile?id=~SueYeon_Chung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SueYeon_Chung1">SueYeon Chung</a>, <a href="https://openreview.net/profile?id=~Kenneth_D._Miller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kenneth_D._Miller2">Kenneth D. Miller</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aOX3a9q3RVV-details-208" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aOX3a9q3RVV-details-208"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">divisive normalization, AlexNet, ImageNet, CIFAR-100, manifold capacity, sparsity, receptive fields, Batch Normalization, Group Normalization, Layer Normalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">DIVISIVE FEATURE NORMALIZATION IMPROVES IMAGE RECOGNITION PERFORMANCE AND IN- CREASES MANIFOLD CAPACITY, SPARSITY, AND LOW-FREQUENCY REPRESENTATION IN DEEP NETS</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="bTteFbU99ye" data-number="4509">
        <h4>
          <a href="https://openreview.net/forum?id=bTteFbU99ye">
              Evaluating Distributional Distortion in Neural Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=bTteFbU99ye" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Benjamin_LeBrun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Benjamin_LeBrun1">Benjamin LeBrun</a>, <a href="https://openreview.net/profile?id=~Alessandro_Sordoni2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alessandro_Sordoni2">Alessandro Sordoni</a>, <a href="https://openreview.net/profile?id=~Timothy_J._O%26%23x27%3BDonnell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Timothy_J._O&#39;Donnell1">Timothy J. O'Donnell</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#bTteFbU99ye-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bTteFbU99ye-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate.  As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="r5qumLiYwf9" data-number="4501">
        <h4>
          <a href="https://openreview.net/forum?id=r5qumLiYwf9">
              MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining
          </a>
        
          
            <a href="https://openreview.net/pdf?id=r5qumLiYwf9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ahmed_Imtiaz_Humayun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ahmed_Imtiaz_Humayun1">Ahmed Imtiaz Humayun</a>, <a href="https://openreview.net/profile?id=~Randall_Balestriero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Randall_Balestriero1">Randall Balestriero</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#r5qumLiYwf9-details-583" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r5qumLiYwf9-details-583"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Generative Networks, Uniform Sampling, Fairness, Data Augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \&amp; recall by 4.12\% \&amp; 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a differential-geometry-based technique to provably sample uniformly from the data manifold of a trained Deep Generative Network without the need for retraining.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xnYACQquaGV" data-number="4495">
        <h4>
          <a href="https://openreview.net/forum?id=xnYACQquaGV">
              Neural Contextual Bandits with Deep Representation and Shallow Exploration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xnYACQquaGV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pan_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pan_Xu1">Pan Xu</a>, <a href="https://openreview.net/profile?id=~Zheng_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zheng_Wen1">Zheng Wen</a>, <a href="https://openreview.net/profile?id=~Handong_Zhao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Handong_Zhao3">Handong Zhao</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanquan_Gu1">Quanquan Gu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xnYACQquaGV-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xnYACQquaGV-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural network, deep representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="73" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.465em; margin-bottom: -0.215em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c7E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.169em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> finite-time regret, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="74" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A new neural network based algorithm for contextual bandit problems with theoretical guarantees and empirical advantages.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xnYACQquaGV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NoB8YgRuoFU" data-number="4494">
        <h4>
          <a href="https://openreview.net/forum?id=NoB8YgRuoFU">
              PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NoB8YgRuoFU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyan_Liu1">Siyan Liu</a>, <a href="https://openreview.net/profile?id=~Pei_Zhang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pei_Zhang6">Pei Zhang</a>, <a href="https://openreview.net/profile?id=~Dan_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Lu1">Dan Lu</a>, <a href="https://openreview.net/profile?id=~Guannan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guannan_Zhang1">Guannan Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NoB8YgRuoFU-details-949" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NoB8YgRuoFU-details-949"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NoB8YgRuoFU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kj0_45Y4r9i" data-number="4489">
        <h4>
          <a href="https://openreview.net/forum?id=kj0_45Y4r9i">
              Discriminative Similarity for Data Clustering
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kj0_45Y4r9i" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yingzhen_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yingzhen_Yang1">Yingzhen Yang</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kj0_45Y4r9i-details-131" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kj0_45Y4r9i-details-131"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Discriminative Similarity, Rademacher Complexity, Generalization Bound, Data Clustering</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\em Clustering by  Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel discriminative similarity for data clustering, and the discriminative similarity is induced by generalization error bound for unsupervised classifier </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="q4tZR1Y-UIs" data-number="4485">
        <h4>
          <a href="https://openreview.net/forum?id=q4tZR1Y-UIs">
              It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=q4tZR1Y-UIs" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuqing_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuqing_Du1">Yuqing Du</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Aditya_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Grover1">Aditya Grover</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#q4tZR1Y-UIs-details-265" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q4tZR1Y-UIs-details-265"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">curriculum generation, unsupervised reinforcement learning, goal conditioned reinforcement learning, multi agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=q4tZR1Y-UIs&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HOjLHrlZhmx" data-number="4469">
        <h4>
          <a href="https://openreview.net/forum?id=HOjLHrlZhmx">
              CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HOjLHrlZhmx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fan_Wu6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fan_Wu6">Fan Wu</a>, <a href="https://openreview.net/profile?id=~Linyi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Linyi_Li1">Linyi Li</a>, <a href="https://openreview.net/profile?id=~Zijian_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zijian_Huang2">Zijian Huang</a>, <a href="https://openreview.net/profile?id=~Yevgeniy_Vorobeychik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yevgeniy_Vorobeychik1">Yevgeniy Vorobeychik</a>, <a href="https://openreview.net/profile?id=~Ding_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ding_Zhao1">Ding Zhao</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HOjLHrlZhmx-details-332" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HOjLHrlZhmx-details-332"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the ï¬rst uniï¬ed framework CROP (Certifying Robust Policies for RL) to provide robustness certiï¬cation on both action and reward levels. In particular, we propose two robustness certiï¬cation criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a ï¬nite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certiï¬cation. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certiï¬cations are often tight. All experiment results are available at website https://crop-leaderboard.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=HOjLHrlZhmx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CCu6RcUMwK0" data-number="4448">
        <h4>
          <a href="https://openreview.net/forum?id=CCu6RcUMwK0">
              Neural Link Prediction with Walk Pooling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CCu6RcUMwK0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liming_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liming_Pan1">Liming Pan</a>, <a href="https://openreview.net/profile?id=~Cheng_Shi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng_Shi2">Cheng Shi</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_DokmaniÄ‡1">Ivan DokmaniÄ‡</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">19 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CCu6RcUMwK0-details-793" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CCu6RcUMwK0-details-793"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph neural network, Link prediction, Random walk, Graph topology.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a ``predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme.   </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=CCu6RcUMwK0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YeShU5mLfLt" data-number="4436">
        <h4>
          <a href="https://openreview.net/forum?id=YeShU5mLfLt">
              On the Convergence of Certified Robust Training with Interval Bound Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YeShU5mLfLt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yihan_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yihan_Wang2">Yihan Wang</a>, <a href="https://openreview.net/profile?id=~Zhouxing_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhouxing_Shi1">Zhouxing Shi</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanquan_Gu1">Quanquan Gu</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YeShU5mLfLt-details-818" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YeShU5mLfLt-details-818"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Certified robustness, Adversarial robustness, Convergence</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using  IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if  we have sufficiently small perturbation radius and large network width.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present the first theoretical analysis on the convergence of certified robust training with interval bound propagation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sX3XaHwotOg" data-number="4429">
        <h4>
          <a href="https://openreview.net/forum?id=sX3XaHwotOg">
              Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sX3XaHwotOg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yu_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Meng1">Yu Meng</a>, <a href="https://openreview.net/profile?id=~Chenyan_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenyan_Xiong1">Chenyan Xiong</a>, <a href="https://openreview.net/profile?id=~Payal_Bajaj2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payal_Bajaj2">Payal Bajaj</a>, <a href="https://openreview.net/profile?id=~saurabh_tiwary1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~saurabh_tiwary1">saurabh tiwary</a>, <a href="https://openreview.net/profile?id=~Paul_N._Bennett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paul_N._Bennett1">Paul N. Bennett</a>, <a href="https://openreview.net/profile?id=~Jiawei_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Han1">Jiawei Han</a>, <a href="https://openreview.net/profile?id=~Xia_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xia_Song1">Xia Song</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sX3XaHwotOg-details-295" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sX3XaHwotOg-details-295"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Language Model Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present AMOS, a new method that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0jP2n0YFmKG" data-number="4408">
        <h4>
          <a href="https://openreview.net/forum?id=0jP2n0YFmKG">
              Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0jP2n0YFmKG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anuroop_Sriram1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anuroop_Sriram1">Anuroop Sriram</a>, <a href="https://openreview.net/profile?id=~Abhishek_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Abhishek_Das1">Abhishek Das</a>, <a href="https://openreview.net/profile?id=~Brandon_M_Wood1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Brandon_M_Wood1">Brandon M Wood</a>, <a href="https://openreview.net/profile?id=~Siddharth_Goyal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siddharth_Goyal2">Siddharth Goyal</a>, <a href="https://openreview.net/profile?id=~C._Lawrence_Zitnick2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~C._Lawrence_Zitnick2">C. Lawrence Zitnick</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0jP2n0YFmKG-details-796" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0jP2n0YFmKG-details-796"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Atomic Simulations, Computational Chemistry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the recently proposed DimeNet++ and GemNet models by over an order of magnitude in the number of parameters. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric on the S2EF task and 2) 21% on the AFbT metric on the IS2RS task, establishing new state-of-the-art results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We scale GNNs used for modeling atomic simulations by an order of magnitude and obtain large performance improvements on the Open Catalyst 2020 dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="shbAgEsk3qM" data-number="4376">
        <h4>
          <a href="https://openreview.net/forum?id=shbAgEsk3qM">
              Understanding and Leveraging Overparameterization in Recursive Value Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=shbAgEsk3qM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chenjun_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenjun_Xiao1">Chenjun Xiao</a>, <a href="https://openreview.net/profile?id=~Bo_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Dai1">Bo Dai</a>, <a href="https://openreview.net/profile?id=~Jincheng_Mei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jincheng_Mei1">Jincheng Mei</a>, <a href="https://openreview.net/profile?id=~Oscar_A_Ramirez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oscar_A_Ramirez1">Oscar A Ramirez</a>, <a href="https://openreview.net/profile?id=~Ramki_Gummadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ramki_Gummadi1">Ramki Gummadi</a>, <a href="https://openreview.net/profile?id=~Chris_Harris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chris_Harris1">Chris Harris</a>, <a href="https://openreview.net/profile?id=~Dale_Schuurmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dale_Schuurmans1">Dale Schuurmans</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">30 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#shbAgEsk3qM-details-812" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="shbAgEsk3qM-details-812"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Temporal Difference Learning, Residual Minimization, Value Estimation, Overparameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization.  Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck.  To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \emph{overparameterized} linear representations that provides useful, transferable findings.  First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \emph{different} fixed points than residual minimization (RM) in the overparameterized linear case.  We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints.  A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability.  Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM.  
        Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. 
        In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively.  Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present an analysis of value estimation under overparameterized linear representations, and develop new algorithmic tools for improving recursive value estimation with deep models based on the new findings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dPyRNUlttBv" data-number="4368">
        <h4>
          <a href="https://openreview.net/forum?id=dPyRNUlttBv">
              Optimization and Adaptive Generalization of Three layer Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dPyRNUlttBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Khashayar_Gatmiry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Khashayar_Gatmiry1">Khashayar Gatmiry</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>, <a href="https://openreview.net/profile?id=~Jonathan_Kelner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Kelner1">Jonathan Kelner</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dPyRNUlttBv-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dPyRNUlttBv-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning theory, adaptive kernel, robust deep learning, neural tangent kernel, adaptive generalization, non-convex optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While there has been substantial recent work studying  generalization of neural networks, 
        the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding.  
        As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel. We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes. In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound.  </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Algorithmically obtaining noise-robust and adaptive generalization bounds for a three layer network model by going beyond the linear approximation of the network</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dPyRNUlttBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-TSe5o7STVR" data-number="4365">
        <h4>
          <a href="https://openreview.net/forum?id=-TSe5o7STVR">
              Non-Parallel Text Style Transfer with Self-Parallel Supervision
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-TSe5o7STVR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ruibo_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ruibo_Liu1">Ruibo Liu</a>, <a href="https://openreview.net/profile?id=~Chongyang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chongyang_Gao1">Chongyang Gao</a>, <a href="https://openreview.net/profile?id=~Chenyan_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chenyan_Jia1">Chenyan Jia</a>, <a href="https://openreview.net/profile?id=~Guangxuan_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangxuan_Xu1">Guangxuan Xu</a>, <a href="https://openreview.net/profile?id=~Soroush_Vosoughi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Soroush_Vosoughi1">Soroush Vosoughi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-TSe5o7STVR-details-48" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-TSe5o7STVR-details-48"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">style transfer, non-parallel corpus, imitation learning, language models, political stance transfer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style.
        
        In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment &amp; formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new text style transfer model for non-parallel corpus with supervision from intrinsic parallelism.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=-TSe5o7STVR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qhkFX-HLuHV" data-number="4364">
        <h4>
          <a href="https://openreview.net/forum?id=qhkFX-HLuHV">
              Can an Image Classifier Suffice For Action Recognition?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qhkFX-HLuHV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Quanfu_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Quanfu_Fan1">Quanfu Fan</a>, <a href="https://openreview.net/profile?id=~Chun-Fu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Fu_Chen1">Chun-Fu Chen</a>, <a href="https://openreview.net/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rameswar_Panda1">Rameswar Panda</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qhkFX-HLuHV-details-232" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qhkFX-HLuHV-details-232"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">action recognition, image classifier, super image, vision transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that transformer-based image classifiers alone can suffice for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at \url{https://github.com/IBM/sifar-pytorch}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the idea of super images to re-purpose an image classifer for action recognition.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IK9ap6nxXr2" data-number="4326">
        <h4>
          <a href="https://openreview.net/forum?id=IK9ap6nxXr2">
              Interacting Contour Stochastic Gradient Langevin Dynamics
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IK9ap6nxXr2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Wei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Deng1">Wei Deng</a>, <a href="https://openreview.net/profile?id=~Siqi_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siqi_Liang1">Siqi Liang</a>, <a href="https://openreview.net/profile?id=~Botao_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Botao_Hao1">Botao Hao</a>, <a href="https://openreview.net/profile?id=~Guang_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guang_Lin1">Guang Lin</a>, <a href="https://openreview.net/profile?id=~Faming_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Faming_Liang1">Faming Liang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IK9ap6nxXr2-details-530" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IK9ap6nxXr2-details-530"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stochastic gradient Langevin dynamics, MCMC, importance sampling, Wang-Landau algorithm, Parallel MCMC Methods, stochastic approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an interacting contour stochastic gradient Langevin dynamics sampler and prove it can be theoretically more efficient than a single-chain process with an equivalent computational budget.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=IK9ap6nxXr2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MIX3fJkl_1" data-number="4325">
        <h4>
          <a href="https://openreview.net/forum?id=MIX3fJkl_1">
              NeuPL: Neural Population Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MIX3fJkl_1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siqi_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siqi_Liu1">Siqi Liu</a>, <a href="https://openreview.net/profile?id=~Luke_Marris2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luke_Marris2">Luke Marris</a>, <a href="https://openreview.net/profile?id=~Daniel_Hennes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Hennes1">Daniel Hennes</a>, <a href="https://openreview.net/profile?id=~Josh_Merel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Josh_Merel1">Josh Merel</a>, <a href="https://openreview.net/profile?id=~Nicolas_Heess1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicolas_Heess1">Nicolas Heess</a>, <a href="https://openreview.net/profile?id=~Thore_Graepel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thore_Graepel1">Thore Graepel</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MIX3fJkl_1-details-721" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MIX3fJkl_1-details-721"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-Agent Learning, Game Theory, Population Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose NeuPL, a general and efficient population learning framework that learns and represents diverse policies in symmetric zero-sum games within a single conditional network via self-play.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hniLRD_XCA" data-number="4319">
        <h4>
          <a href="https://openreview.net/forum?id=hniLRD_XCA">
              DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hniLRD_XCA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minghao_Han2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghao_Han2">Minghao Han</a>, <a href="https://openreview.net/profile?id=~Jacob_Euler-Rolle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_Euler-Rolle1">Jacob Euler-Rolle</a>, <a href="https://openreview.net/profile?id=~Robert_K._Katzschmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_K._Katzschmann1">Robert K. Katzschmann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hniLRD_XCA-details-322" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hniLRD_XCA-details-322"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Koopman Operator, Robust Control, Robotics, Model Predictive Control, Soft Robotics</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Koopman operator theory linearly describes nonlinear dynamical systems in a high-dimensional functional space and it allows to apply linear control methods to highly nonlinear systems. However, the Koopman operator does not account for any uncertainty in dynamical systems, causing it to perform poorly in real-world applications.
        Therefore, we propose a deep stochastic Koopman operator (DeSKO) model in a robust learning control framework to guarantee stability of nonlinear stochastic systems. The DeSKO model captures a dynamical system's uncertainty by inferring a distribution of observables. We use the inferred distribution to design a robust, stabilizing closed-loop controller for a dynamical system. Modeling and control experiments on several advanced control benchmarks show that our framework is more robust and scalable than state-of-the-art deep Koopman operators and reinforcement learning methods. Tested control benchmarks include a soft robotic arm, a legged robot, and a biological gene regulatory network. We also demonstrate that this robust control method resists previously unseen uncertainties, such as external disturbances, with a magnitude of up to five times the maximum control input. Our approach opens up new possibilities in learning control for high-dimensional nonlinear systems while robustly managing internal or external uncertainty.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust learning control framework with guarantee stability based on deep stochastic Koopman operator models</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=hniLRD_XCA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oiZJwC_fyS" data-number="4300">
        <h4>
          <a href="https://openreview.net/forum?id=oiZJwC_fyS">
              Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oiZJwC_fyS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Panagiotis_Misiakos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Panagiotis_Misiakos1">Panagiotis Misiakos</a>, <a href="https://openreview.net/profile?id=~Georgios_Smyrnis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Smyrnis1">Georgios Smyrnis</a>, <a href="https://openreview.net/profile?id=~George_Retsinas2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Retsinas2">George Retsinas</a>, <a href="https://openreview.net/profile?id=~Petros_Maragos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Petros_Maragos1">Petros Maragos</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oiZJwC_fyS-details-871" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oiZJwC_fyS-details-871"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Tropical Geometry, Zonotopes, Hausdorff Approximation, Neural Network Compression</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work we theoretically contribute to neural network approximation by providing a novel tropical geometrical viewpoint to structured neural network compression. In particular, we show that the approximation error between two neural networks with ReLU activations and one hidden layer depends on the Hausdorff distance of the tropical zonotopes of the networks. This theorem comes as a first step towards a purely geometrical interpretation of neural network approximation. Based on this theoretical contribution, we propose geometrical methods that employ the K-means algorithm to compress the fully connected parts of ReLU activated deep neural networks. We analyze the error bounds of our algorithms theoretically based on our approximation theorem and evaluate them empirically on neural network compression. Our experiments follow a proof-of-concept strategy and indicate that our geometrical tools achieve improved performance over relevant tropical geometry techniques and can be competitive against non-tropical methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oiZJwC_fyS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hqkhcFHOeKD" data-number="4295">
        <h4>
          <a href="https://openreview.net/forum?id=hqkhcFHOeKD">
              Learning Towards The Largest Margins
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hqkhcFHOeKD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiong_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiong_Zhou3">Xiong Zhou</a>, <a href="https://openreview.net/profile?id=~Xianming_Liu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xianming_Liu5">Xianming Liu</a>, <a href="https://openreview.net/profile?id=~Deming_Zhai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deming_Zhai2">Deming Zhai</a>, <a href="https://openreview.net/profile?id=~Junjun_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junjun_Jiang2">Junjun Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Gao1">Xin Gao</a>, <a href="https://openreview.net/profile?id=~Xiangyang_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiangyang_Ji1">Xiangyang Ji</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hqkhcFHOeKD-details-329" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hqkhcFHOeKD-details-329"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">loss function design, margin-based loss, classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One of the main challenges for feature representation in deep learning-based classification is the design of appropriate loss functions that exhibit strong discriminative power. The classical softmax loss does not explicitly encourage discriminative learning of features. A popular direction of research is to incorporate margins in well-established losses in order to enforce extra intra-class compactness and inter-class separability, which, however, were developed through heuristic means, as opposed to rigorous mathematical principles. In this work, we attempt to address this limitation by formulating the principled optimization objective as learning towards the largest margins. Specifically, we firstly propose to employ the class margin as the measure of inter-class separability, and the sample margin as the measure of intra-class compactness. Accordingly, to encourage discriminative representation of features, the loss function should promote the largest possible margins for both classes and samples. Furthermore, we derive a generalized margin softmax loss to draw general conclusions for the existing margin-based losses. Not only does this principled framework offer new perspectives to understand and interpret existing margin-based losses, but it also provides new insights that can guide the design of new tools, including \textit{sample margin regularization} and \textit{largest margin softmax loss} for class balanced cases, and \textit{zero centroid regularization} for class imbalanced cases. Experimental results demonstrate the effectiveness of our strategy for multiple tasks including visual classification, imbalanced classification, person re-identification, and face verification.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=hqkhcFHOeKD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="28ib9tf6zhr" data-number="4289">
        <h4>
          <a href="https://openreview.net/forum?id=28ib9tf6zhr">
              Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=28ib9tf6zhr" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yonggan_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonggan_Fu1">Yonggan Fu</a>, <a href="https://openreview.net/profile?email=sz74%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="sz74@rice.edu">Shunyao Zhang</a>, <a href="https://openreview.net/profile?email=sw99%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="sw99@rice.edu">Shang Wu</a>, <a href="https://openreview.net/profile?id=~Cheng_Wan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cheng_Wan2">Cheng Wan</a>, <a href="https://openreview.net/profile?id=~Yingyan_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yingyan_Lin1">Yingyan Lin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#28ib9tf6zhr-details-578" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="28ib9tf6zhr-details-578"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision transformer, adversarial examples, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: "Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?" Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs. It can be expected that our Patch-Fool framework will shed light on both future architecture designs and training schemes for robustifying ViTs towards their real-world deployment. Our codes are available at https://github.com/RICE-EIC/Patch-Fool.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the Patch-Fool attack to unveil a vulnerability perspective of ViTs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Q5uh1Nvv5dm" data-number="4287">
        <h4>
          <a href="https://openreview.net/forum?id=Q5uh1Nvv5dm">
              AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Q5uh1Nvv5dm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~David_Berthelot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Berthelot1">David Berthelot</a>, <a href="https://openreview.net/profile?id=~Rebecca_Roelofs1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rebecca_Roelofs1">Rebecca Roelofs</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Nicholas_Carlini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Carlini1">Nicholas Carlini</a>, <a href="https://openreview.net/profile?id=~Alexey_Kurakin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_Kurakin1">Alexey Kurakin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Q5uh1Nvv5dm-details-886" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q5uh1Nvv5dm-details-886"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised domain adaptation, semi-supervised learning, semi-supervised domain adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a unified solution for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA and find that AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce AdaMatch, a unified solution that achieves state-of-the-art results for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="l_amHf1oaK" data-number="4286">
        <h4>
          <a href="https://openreview.net/forum?id=l_amHf1oaK">
              Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=l_amHf1oaK" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Claudio_Ferrari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Claudio_Ferrari2">Claudio Ferrari</a>, <a href="https://openreview.net/profile?id=~Mark_Niklas_Mueller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mark_Niklas_Mueller2">Mark Niklas Mueller</a>, <a href="https://openreview.net/profile?id=~Nikola_Jovanovi%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikola_JovanoviÄ‡1">Nikola JovanoviÄ‡</a>, <a href="https://openreview.net/profile?id=~Martin_Vechev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martin_Vechev1">Martin Vechev</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#l_amHf1oaK-details-794" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l_amHf1oaK-details-794"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Certified Robustness, Branch-and-Bound, Convex Relaxation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new state-of-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We obtain a state-of-the-art GPU-based neural network verifier by leveraging tight multi-neuron constraints in a Branch-and-Bound setting.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="VFBjuF8HEp" data-number="4276">
        <h4>
          <a href="https://openreview.net/forum?id=VFBjuF8HEp">
              Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality
          </a>
        
          
            <a href="https://openreview.net/pdf?id=VFBjuF8HEp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daniel_Watson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Watson1">Daniel Watson</a>, <a href="https://openreview.net/profile?id=~William_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~William_Chan1">William Chan</a>, <a href="https://openreview.net/profile?id=~Jonathan_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Ho1">Jonathan Ho</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Norouzi1">Mohammad Norouzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 26 Feb 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#VFBjuF8HEp-details-633" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VFBjuF8HEp-details-633"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a method to discover fast, high-fidelity samplers for diffusion probabilistic models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="18">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="submitted-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="youe3QQepVB" data-number="4714">
        <h4>
          <a href="https://openreview.net/forum?id=youe3QQepVB">
              Generative Modeling for Multitask Visual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=youe3QQepVB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhipeng_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhipeng_Bao1">Zhipeng Bao</a>, <a href="https://openreview.net/profile?id=~Yu-Xiong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Xiong_Wang1">Yu-Xiong Wang</a>, <a href="https://openreview.net/profile?id=~Martial_Hebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martial_Hebert1">Martial Hebert</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#youe3QQepVB-details-679" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="youe3QQepVB-details-679"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, especially in the low-data regimes, and our model consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a general multi-task oriented generative modeling (MGM) framework that introduces generative models to facilitate multi-task learning and it consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2aC0_RxkBL_" data-number="4707">
        <h4>
          <a href="https://openreview.net/forum?id=2aC0_RxkBL_">
              Where is the bottleneck in long-tailed classification?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2aC0_RxkBL_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaid_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zaid_Khan1">Zaid Khan</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yun_Fu1">Yun Fu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2aC0_RxkBL_-details-584" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2aC0_RxkBL_-details-584"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, bias, long tailed learning, imbalanced learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A commonly held belief in deep-learning based long-tailed classiï¬cation is that the representations learned from long-tailed data are â€good enoughâ€ and the performance bottleneck is the classiï¬cation head atop the representation learner. We design experiments to investigate this folk wisdom, and ï¬nd that representations learned from long-tailed data distributions substantially differ from the representations learned from â€normalâ€ data distributions. We show that the long-tailed representations are volatile and brittle with respect to the true data distribution. Compared to the representations learned from the true, balanced distributions, long-tailed representations fail to localize tail classes and display vastly worse inter-class separation and intra-class compactness when unseen samples from the true data distribution are embedded into the feature space. We provide an explanation for why data augmentation helps long-tailed classiï¬cation despite leaving the dataset imbalance unchanged â€” it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate how learning from long-tailed distributions harms representations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="voEpzgY8gsT" data-number="4704">
        <h4>
          <a href="https://openreview.net/forum?id=voEpzgY8gsT">
              Additive Poisson Process: Learning Intensity of Higher-Order Interaction in Poisson Processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=voEpzgY8gsT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Simon_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Luo1">Simon Luo</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Zhou9">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~lamiae_azizi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~lamiae_azizi1">lamiae azizi</a>, <a href="https://openreview.net/profile?id=~Mahito_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mahito_Sugiyama1">Mahito Sugiyama</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#voEpzgY8gsT-details-110" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="voEpzgY8gsT-details-110"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Poisson Process, Log-Linear Model, Energy-Based Model, Generalized Additive Models, Information Geometry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Additive Poisson Process (APP), a novel framework that can model the higher-order interaction effects of the intensity functions in Poisson processes using projections into lower-dimensional space. Our model combines the techniques in information geometry to model higher-order interactions on a statistical manifold and in generalized additive models to use lower-dimensional projections to overcome the effects from the curse of dimensionality. Our approach solves a convex optimization problem by minimizing the KL divergence from a sample distribution in lower-dimensional projections to the distribution modeled by an intensity function in the Poisson process. Our empirical results show that our model is able to use samples observed in the lower dimensional space to estimate the higher-order intensity function with extremely sparse observations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An efficient technique that uses a log-linear model on a partial order structure to approximate a high-dimensional intensity functions in a Poisson Process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=voEpzgY8gsT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qfLJBJf_DnH" data-number="4701">
        <h4>
          <a href="https://openreview.net/forum?id=qfLJBJf_DnH">
              Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qfLJBJf_DnH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Laureline_Logiaco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Laureline_Logiaco1">Laureline Logiaco</a>, <a href="https://openreview.net/profile?id=~G_Sean_Escola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~G_Sean_Escola1">G Sean Escola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qfLJBJf_DnH-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qfLJBJf_DnH-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuroscience, dynamical systems, thalamocortical architecture, motor preparation, continual learning, hierarchical continuous motor control, out-of-distribution generalization, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of learning dynamics that can produce hierarchically organized continuous outputs consisting of the flexible chaining of re-usable motor â€˜motifsâ€™ from which complex behavior is generated. Can a motif library be efficiently and extendably learned without interference between motifs, and can these motifs be chained in arbitrary orders without first learning the corresponding motif transitions during training? This requires (i) parameter updates while learning a new motif that do not interfere with the parameters used for the previously acquired ones; and (ii) successful motif generation when starting from the network states reached at the end of any of the other motifs, even if these states were not present during training (a case of out-of-distribution generalization). We meet the first requirement by designing recurrent neural networks (RNNs) with specific architectures that segregate motif-dependent parameters (as customary in continual learning works), and try a standard method to address the second by training with random initial states. We find that these standard RNNs are very unreliable during zero-shot transfer to motif chaining. We then use insights from the motor thalamocortical circuit, featuring a specific module that shapes motif transitions. We develop a method to constrain the RNNs to function similarly to the thalamocortical circuit during motif transitions, while preserving the large expressivity afforded by gradient-based training of non-analytically tractable RNNs. We then show that this thalamocortical inductive bias not only acts in synergy with gradient-descent RNN training to improve accuracy during in-training-distribution motif production, but also leads to zero-shot transfer to new motif chains with no performance cost. Besides proposing an efficient, robust and flexible RNN architecture, our results shed new light on the function of motor preparation in the brain.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Motor preparation in nonlinear RNNs supports robust chaining of accurate continuous motor motifs in never-experienced orders.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rF5UoZFrsF4" data-number="4697">
        <h4>
          <a href="https://openreview.net/forum?id=rF5UoZFrsF4">
              VUT: Versatile UI Transformer for Multimodal Multi-Task User Interface Modeling 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rF5UoZFrsF4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Li2">Yang Li</a>, <a href="https://openreview.net/profile?id=~Gang_Li13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Li13">Gang Li</a>, <a href="https://openreview.net/profile?id=~Xin_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Zhou3">Xin Zhou</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rF5UoZFrsF4-details-15" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rF5UoZFrsF4-details-15"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">User Interface Modeling, Multimodal input, Multi-task learning, Transformer, Layout Detection, Language Grounding, Image Captioning, Screen Summarization, Tappability Prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT has achieved accuracy either on par with or exceeding the accuracy when the model is trained for individual tasks separately.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The work addresses unique challenges of multimodal multi-task learning of distinct tasks for user interface modeling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3M3t3tUbA2Y" data-number="4686">
        <h4>
          <a href="https://openreview.net/forum?id=3M3t3tUbA2Y">
              DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3M3t3tUbA2Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fei_Deng1">Fei Deng</a>, <a href="https://openreview.net/profile?id=~Ingook_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingook_Jang1">Ingook Jang</a>, <a href="https://openreview.net/profile?id=~Sungjin_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungjin_Ahn1">Sungjin Ahn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3M3t3tUbA2Y-details-757" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3M3t3tUbA2Y-details-757"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction
        often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions. To address this issue, previous work has proposed to contrastively learn the latent representations and its temporal dynamics, but showed inconsistent performance, often worse than Dreamer. Although, in computer vision, an alternative prototypical approach has often shown to be more accurate and robust, it is elusive how this approach can be combined best with the temporal dynamics learning in MBRL. In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal. Similar to SwAV, by encouraging uniform cluster assignment across the batch, we implicitly push apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details. We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model is consistently better than the previous models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dy8gq-LuckD" data-number="4680">
        <h4>
          <a href="https://openreview.net/forum?id=Dy8gq-LuckD">
              Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dy8gq-LuckD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Wu1">Nan Wu</a>, <a href="https://openreview.net/profile?id=~Stanislaw_Kamil_Jastrzebski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stanislaw_Kamil_Jastrzebski1">Stanislaw Kamil Jastrzebski</a>, <a href="https://openreview.net/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyunghyun_Cho1">Kyunghyun Cho</a>, <a href="https://openreview.net/profile?id=~Krzysztof_J._Geras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krzysztof_J._Geras1">Krzysztof J. Geras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dy8gq-LuckD-details-620" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dy8gq-LuckD-details-620"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-modal learning, deep neural networks, multi-view learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities. We observe empirically that such behavior hurts its overall generalization. We validate our hypothesis by estimating the gain on the accuracy when the model has access to an additional modality. We refer to this gain as the conditional utilization rate of the modality. In the experiments, we consistently observe an imbalance in conditional utilization rate between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. We thus propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the modelâ€™s generalization on three datasets: Colored MNIST (Kim et al., 2019), Princeton ModelNet40 (Wu et al., 2015), and NVIDIA Dynamic Hand Gesture Dataset (Molchanov et al., 2016).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UeE41VsK1KJ" data-number="4677">
        <h4>
          <a href="https://openreview.net/forum?id=UeE41VsK1KJ">
              Subjective Learning for Open-Ended Data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UeE41VsK1KJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tianren_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianren_Zhang1">Tianren Zhang</a>, <a href="https://openreview.net/profile?id=~Yizhou_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yizhou_Jiang1">Yizhou Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Su1">Xin Su</a>, <a href="https://openreview.net/profile?id=~Shangqi_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shangqi_Guo2">Shangqi Guo</a>, <a href="https://openreview.net/profile?id=~Feng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Chen1">Feng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UeE41VsK1KJ-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UeE41VsK1KJ-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Open-ended data, machine learning, supervised learning, data conflict</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional supervised learning typically assumes that the learning task can be solved by learning a single function since the data is sampled from a fixed distribution. However, this assumption is invalid in open-ended environments where no task-level data partitioning is available. In this paper, we present a novel supervised learning framework of learning from open-ended data, which is modeled as data implicitly sampled from multiple domains with the data in each domain obeying a domain-specific target function. Since different domains may possess distinct target functions, open-ended data inherently requires multiple functions to capture all its input-output relations, rendering training a single global model problematic. To address this issue, we devise an Open-ended Supervised Learning (OSL) framework, of which the key component is a subjective function that allocates the data among multiple candidate models to resolve the "conflict'' between the data from different domains, exhibiting a natural hierarchy. We theoretically analyze the learnability and the generalization error of OSL, and empirically validate its efficacy in both open-ended regression and classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We formalize the problem of learning from open-ended data that implicitly comes from multiple domains and inherently requires multiple functions to fully capture its input-output relations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UeE41VsK1KJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2RNpZ8S4alJ" data-number="4675">
        <h4>
          <a href="https://openreview.net/forum?id=2RNpZ8S4alJ">
              KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2RNpZ8S4alJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alireza_Rezazadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Rezazadeh1">Alireza Rezazadeh</a>, <a href="https://openreview.net/profile?email=cchoi%40umn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="cchoi@umn.edu">Changhyun Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2RNpZ8S4alJ-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2RNpZ8S4alJ-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object-centric representation is an essential abstraction for physical reasoning and forward prediction. Most existing approaches learn this representation through extensive supervision (e.g, object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network)---an end-to-end unsupervised framework to reason about object interactions in complex systems based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, and novel object geometries. Experiments demonstrate the effectiveness of our model to accurately perform forward prediction and learn plannable object-centric representations which can also be used in downstream model-based control tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="an_ndI09oVZ" data-number="4667">
        <h4>
          <a href="https://openreview.net/forum?id=an_ndI09oVZ">
              Deep banach space kernels
          </a>
        
          
            <a href="https://openreview.net/pdf?id=an_ndI09oVZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mrityunjay_Bhardwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mrityunjay_Bhardwaj1">Mrityunjay Bhardwaj</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#an_ndI09oVZ-details-653" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="an_ndI09oVZ-details-653"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RKBS, RKHS, concatenated kernel learning, representation learning, deep learning, MLMKL, Deep Gaussian Processes, gaussian processes, kernel machines</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recent success of deep learning has encouraged many researchers to explore the deep/concatenated variants of classical kernel methods. Some of which includes MLMKL, DGP and DKL. Although, These methods have proven to be quite useful in various real-world settings. They still suffer from the limitations of only utilizing kernels from Hilbert spaces. In this paper, we address these shortcomings by introducing a new class of concatenated kernel learning methods that use the kernels from the reproducing kernel Banach spaces(RKBSs). These spaces turned out to be one of the most general spaces where a reproducing Kernel exists. We propose a framework of construction for these Deep RKBS models and then provide a representer theorem for regularized learning problems. We also describe the relationship with its deep RKHS variant as well as standard Deep Gaussian Processes. In the end, we construct and implement a two-layer deep RKBS model and demonstrate it on a range of machine learning tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">a new class of deep kernel methods which uses kernels from reproducing kernel banach spaces (RKBS).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0Kj5mhn6sw" data-number="4650">
        <h4>
          <a href="https://openreview.net/forum?id=0Kj5mhn6sw">
              Gesture2Vec: Clustering Gestures using  Representation Learning Methods for Co-speech Gesture Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0Kj5mhn6sw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Payam_Jome_Yazdian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_Jome_Yazdian1">Payam Jome Yazdian</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?email=angelica%40sfu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="angelica@sfu.ca">Angelica Lim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0Kj5mhn6sw-details-898" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0Kj5mhn6sw-details-898"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation learning, gesture generation, vector quantization, machine translation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans. Similarly, the co-speech gesture is a key ingredient in human-agent interaction including both virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. Furthermore, we use a machine translation model that takes input text and translates it into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated quantized gestures from the input text as an input to the autoencoderâ€™s decoder to produce gesture sequences. The resulting gestures can be applied to both virtual agents and humanoid robots. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0Kj5mhn6sw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WXy4C-RjET" data-number="4644">
        <h4>
          <a href="https://openreview.net/forum?id=WXy4C-RjET">
              Logit Attenuating Weight Normalization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WXy4C-RjET" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aman_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aman_Gupta1">Aman Gupta</a>, <a href="https://openreview.net/profile?id=~Rohan_Ramanath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Ramanath1">Rohan Ramanath</a>, <a href="https://openreview.net/profile?id=~Jun_Shi4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jun_Shi4">Jun Shi</a>, <a href="https://openreview.net/profile?id=~Anika_Ramachandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anika_Ramachandran1">Anika Ramachandran</a>, <a href="https://openreview.net/profile?id=~SIROU_ZHU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SIROU_ZHU1">SIROU ZHU</a>, <a href="https://openreview.net/profile?id=~Mingzhou_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mingzhou_Zhou1">Mingzhou Zhou</a>, <a href="https://openreview.net/profile?id=~Sathiya_Keerthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sathiya_Keerthi1">Sathiya Keerthi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WXy4C-RjET-details-185" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WXy4C-RjET-details-185"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning, gradient methods, stochastic optimization, generalization gap, imagenet, adam, large batch training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over-parameterized deep networks trained using gradient-based optimizers is a popular way of solving classification and ranking problems. Without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that, even with weight decay (WD) and normal hyper-parameter tuning, adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain.
        
        An alternative to WD for improving a network's adaptivity is to directly control the magnitude of the weights and hence the logits. We propose a method called Logit Attenuating Weight Normalization (LAWN), that can be stacked onto any gradient-based optimizer.  LAWN initially starts off training in a free (unregularized) mode and, after some initial epochs, it constrains the weight norms of layers, thereby controlling the logits and improving adaptivity. This is a new regularization approach that does not use WD anywhere; instead, the number of initial free epochs becomes the new hyper-parameter. The resulting LAWN variant of adaptive optimizers gives a solid lift to generalization performance, making their performance equal or even exceed SGD's performance on benchmark image classification and recommender datasets. Another important feature is that LAWN also greatly improves the adaptive optimizers when used with large batch sizes.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An optimizer for deep learning called Logit Attenuating Weight Normalization (LAWN) for superior generalization performance and scaling to large batches</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WXy4C-RjET&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxBH9j8IE_H" data-number="4643">
        <h4>
          <a href="https://openreview.net/forum?id=UxBH9j8IE_H">
              Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxBH9j8IE_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~BITHIKA_PAL1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BITHIKA_PAL1">BITHIKA PAL</a>, <a href="https://openreview.net/profile?id=~Arindam_Biswas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arindam_Biswas1">Arindam Biswas</a>, <a href="https://openreview.net/profile?id=~Pabitra_Mitra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pabitra_Mitra1">Pabitra Mitra</a>, <a href="https://openreview.net/profile?id=~BISWAJIT_BASU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BISWAJIT_BASU1">BISWAJIT BASU</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxBH9j8IE_H-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxBH9j8IE_H-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Neural Networks, Network Pruning, Ramanujan Graphs, Eigenvalue bounds, Spectral Gap</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A Ramanujan graph perspective to explain the lottery ticket hypothesis</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a0SRWViFYW" data-number="4642">
        <h4>
          <a href="https://openreview.net/forum?id=a0SRWViFYW">
              Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a0SRWViFYW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Patrick_R._Johnstone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_R._Johnstone1">Patrick R. Johnstone</a>, <a href="https://openreview.net/profile?id=~Jonathan_Eckstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Eckstein1">Jonathan Eckstein</a>, <a href="https://openreview.net/profile?id=~Thomas_Flynn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Flynn1">Thomas Flynn</a>, <a href="https://openreview.net/profile?id=~Shinjae_Yoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinjae_Yoo1">Shinjae Yoo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a0SRWViFYW-details-897" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a0SRWViFYW-details-897"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">convex optimization, min-max games, saddle-point problems, first-order stochastic methods, proximal methods, operator splitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new, stochastic variant of the projective splitting (PS) family of algorithms for monotone inclusion problems.  It can solve min-max and noncooperative game formulations arising in applications such as robust ML without the convergence issues associated with gradient descent-ascent, the current de facto standard approach in ML applications.  Our proposal is the first version of PS able to use stochastic gradient oracles. It can solve min-max games while handling multiple constraints and nonsmooth regularizers via projection and proximal operators. Unlike other stochastic splitting methods that can solve such problems, our method does not rely on a product-space reformulation of the original problem. We prove almost-sure convergence of the iterates to the solution and a convergence rate for the expected residual.  By working with monotone inclusions rather than variational inequalities, our analysis avoids the drawbacks of measuring convergence through the restricted gap function. We close with numerical experiments on a distributionally robust sparse logistic regression problem.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a stochastic splitting method that can easily handle min-max problems with multiple regularizers and constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=a0SRWViFYW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_K6rwRjW9WO" data-number="4637">
        <h4>
          <a href="https://openreview.net/forum?id=_K6rwRjW9WO">
              RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_K6rwRjW9WO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Victor_Quintas-Martinez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Quintas-Martinez1">Victor Quintas-Martinez</a>, <a href="https://openreview.net/profile?id=~Victor_Chernozhukov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Chernozhukov1">Victor Chernozhukov</a>, <a href="https://openreview.net/profile?id=~Vasilis_Syrgkanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vasilis_Syrgkanis1">Vasilis Syrgkanis</a>, <a href="https://openreview.net/profile?id=~Whitney_Newey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Whitney_Newey1">Whitney Newey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_K6rwRjW9WO-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_K6rwRjW9WO-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="75" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.281em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></mjx-assistive-mml></mjx-container>-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, that is derived based on a functional-specific theoretical derivation of what is known as the influence function and which leads to properties such as double robustness and Neyman orthogonality. We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method solely requires value query oracle access to the linear functional. We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Reisz representer and regression loss, while sharing representation layers for the two functions. We also propose a random forest method which learns a locally linear representation of the Reisz function. Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We implement an automatic debiasing procedure for causal and policy effects based on automatically learning their corresponding Riesz representation, using Neural Nets and Random Forests.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_K6rwRjW9WO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Mo9R9oqzPo" data-number="4636">
        <h4>
          <a href="https://openreview.net/forum?id=Mo9R9oqzPo">
              New Definitions and Evaluations for Saliency Methods: Staying Intrinsic and Sound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Mo9R9oqzPo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arushi_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arushi_Gupta1">Arushi Gupta</a>, <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Dingli_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dingli_Yu1">Dingli Yu</a>, <a href="https://openreview.net/profile?id=~Kaifeng_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaifeng_Lyu2">Kaifeng Lyu</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sanjeev_Arora1">Sanjeev Arora</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Mo9R9oqzPo-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mo9R9oqzPo-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">saliency, masking based methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Saliency methods seek to provide human-interpretable explanations for the output of machine learning model on a given input. A plethora of saliency methods exist, as well as an extensive literature on their justifications/criticisms/evaluations. This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans. It tries to introduce methods and evaluations for masked-based saliency methods that are {\em intrinsic} --- use just the training dataset and the trained net, and do not use separately trained nets, distractor distributions, human evaluations or annotations. Since a mask can be seen as a "certificate" justifying the net's answer, we introduce notions of {\em completeness} and {\em soundness} (the latter being the new contribution) motivated by logical proof systems. These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.V. regularization, upscaling). </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mo9R9oqzPo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GthNKCqdDg" data-number="4623">
        <h4>
          <a href="https://openreview.net/forum?id=GthNKCqdDg">
              Selective Token Generation for Few-shot Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GthNKCqdDg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daejin_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daejin_Jo1">Daejin Jo</a>, <a href="https://openreview.net/profile?id=~Taehwan_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Taehwan_Kwon1">Taehwan Kwon</a>, <a href="https://openreview.net/profile?id=~Sungwoong_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungwoong_Kim2">Sungwoong Kim</a>, <a href="https://openreview.net/profile?id=~Eun-Sol_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eun-Sol_Kim1">Eun-Sol Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GthNKCqdDg-details-571" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GthNKCqdDg-details-571"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Generation, Reinforcement Learning, Few-shot Learning, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among these transfer learning algorithms from PLMs, additive learning that incorporates a task-specific adapter on top of the fixed PLM has been popularly used to alleviate the severe overfitting problem in the few-shot setting. However, this added task-specific adapter is generally trained by maximum likelihood estimation that can easily suffer from the so-called exposure bias problem, especially in sequential text generation. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) for few-shot natural language generation (NLG) tasks. In particular, we propose to use a selective token generation between the transformer-based PLM and the task-specific adapter during both training and inference. This output token selection between the two generators allows the adapter to take into account only on the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, in order to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="INO8hGXD2M" data-number="4614">
        <h4>
          <a href="https://openreview.net/forum?id=INO8hGXD2M">
              Adversarial Distributions Against Out-of-Distribution Detectors
          </a>
        
          
            <a href="https://openreview.net/pdf?id=INO8hGXD2M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sangwoong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sangwoong_Yoon1">Sangwoong Yoon</a>, <a href="https://openreview.net/profile?id=~Jinwon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinwon_Choi1">Jinwon Choi</a>, <a href="https://openreview.net/profile?id=~Yonghyeon_LEE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonghyeon_LEE1">Yonghyeon LEE</a>, <a href="https://openreview.net/profile?id=~Yung-Kyun_Noh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yung-Kyun_Noh1">Yung-Kyun Noh</a>, <a href="https://openreview.net/profile?id=~Frank_C._Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_C._Park1">Frank C. Park</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#INO8hGXD2M-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="INO8hGXD2M-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">out-of-distribution detection, outlier detection, adversarial attack, model evaluation, markov chain monte carlo</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution. As an outlier may deviate from the training distribution in unexpected ways, an ideal OOD detector should be able to detect all types of outliers. However, current evaluation protocols test a detector over OOD datasets that cover only a small fraction of all possible outliers, leading to overly optimistic views of OOD detector performance.  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers.  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector. Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel evaluation method for out-of-distribution detectors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eOdSD0B5TE" data-number="4606">
        <h4>
          <a href="https://openreview.net/forum?id=eOdSD0B5TE">
              On the Implicit Biases of Architecture &amp; Gradient Descent
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eOdSD0B5TE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jeremy_Bernstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeremy_Bernstein1">Jeremy Bernstein</a>, <a href="https://openreview.net/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisong_Yue1">Yisong Yue</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eOdSD0B5TE-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eOdSD0B5TE-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalisation, function space, PAC-Bayes, NNGP, orthants, margin</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Do neural networks generalise because of bias in the functions returned by gradient descent, or bias already present in the network architecture? <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="76" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">Â¿</mjx-utext><mjx-c class="mjx-c1D443 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45E TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">Ã©</mjx-utext><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c3F TEX-MI"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">Â¿Por quÃ© no los dos?</mtext></math></mjx-assistive-mml></mjx-container> This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent. To measure the implicit bias of architecture, new technical tools are developed to both <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="77" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">analytically bound</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="78" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">consistently estimate</mtext></math></mjx-assistive-mml></mjx-container> the average test error of the neural network--Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-PÃ©rez et al. (2019) and underscoring the importance of architecture. Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior. This highlights a curious fact: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="79" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">minimum a posteriori</mtext></math></mjx-assistive-mml></mjx-container> functions can generalise best, and gradient descent can select for those functions. In summary, new technical tools suggest a nuanced portrait of generalisation involving both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">New technical tools suggest a nuanced portrait of generalisation that involves both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=eOdSD0B5TE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vPK-G5HbnWg" data-number="4603">
        <h4>
          <a href="https://openreview.net/forum?id=vPK-G5HbnWg">
              PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vPK-G5HbnWg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zehao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zehao_Dong1">Zehao Dong</a>, <a href="https://openreview.net/profile?id=~Muhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhan_Zhang1">Muhan Zhang</a>, <a href="https://openreview.net/profile?id=~Fuhai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuhai_Li1">Fuhai Li</a>, <a href="https://openreview.net/profile?id=~Yixin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yixin_Chen1">Yixin Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vPK-G5HbnWg-details-659" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vPK-G5HbnWg-details-659"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DAG encoder, graph neural network, Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optimization of directed acyclic graph (DAG) structures has many applications, such as neural architecture search (NAS) and probabilistic graphical model learning. Encoding DAGs into real vectors is a dominant component in most neural-network-based DAG optimization frameworks. Currently, most popular DAG encoders use an asynchronous message passing scheme which sequentially processes nodes according to the dependency between nodes in a DAG. That is, a node must not be processed until all its predecessors are processed. As a result, they are inherently not parallelizable. In this work, we propose a Parallelizable Attention-based Computation structure Encoder (PACE) that processes nodes simultaneously and encodes DAGs in parallel. We demonstrate the superiority of PACE through  encoder-dependent optimization subroutines that search the optimal DAG structure based on the learned DAG embeddings. Experiments show that PACE not only improves the effectiveness over previous sequential DAG encoders with a significantly boosted training and inference speed, but also generates smooth latent (DAG encoding) spaces that are beneficial to downstream optimization subroutines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper introduces a novel DAG encoder based on Transformer to encode the computation structure defined by DAGs in a fully parallelizable manner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vPK-G5HbnWg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hEiwVblq4P" data-number="4602">
        <h4>
          <a href="https://openreview.net/forum?id=hEiwVblq4P">
              Proper Straight-Through Estimator: Breaking symmetry promotes convergence to true minimum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hEiwVblq4P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shinya_Gongyo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinya_Gongyo1">Shinya Gongyo</a>, <a href="https://openreview.net/profile?id=~Kohta_Ishikawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kohta_Ishikawa1">Kohta Ishikawa</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hEiwVblq4P-details-792" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hEiwVblq4P-details-792"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">quantization, binary network, low bit network, Straight through estimator, STE</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the quantized network, its gradient shows either vanishing or diverging. The network thus cannot be learned by the standard back-propagation, so that an alternative approach called Straight Through Estimator (STE), which replaces the part of the gradient with a simple differentiable function, is used. While STE is known to work well for learning the quantized network empirically, it has not been established theoretically. A recent study by Yin et. al. (2019) has provided theoretical support for STE. However, its justification is still limited to the model in the one-hidden layer network with the binary activation where  Gaussian generates the input data, and the true labels are output from the teacher network with the same binary network architecture. In this paper, we discuss the effectiveness of STEs in more general situations without assuming the shape of the input distribution and the labels. By considering the scale symmetry of the network and specific properties of the STEs, we find that STE with clipped Relu is superior to STEs with identity function and vanilla Relu. The clipped Relu STE, which breaks the scale symmetry, may pick up one of the local minima degenerated in scales, while the identity STE and vanilla Relu STE, which keep the scale symmetry, may not pick it up. To confirm this observation, we further present an analysis of a simple misspecified model as an example. We find that all the stationary points are identical with the vanishing points of the cRelu STE gradient, while some of them are not identical with the vanishing points of the identity and Relu STE.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We discuss breaking symmetry embedded in the network by Straight through estimators enhances the possibility of convergence to the true minimum.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="W6BpshgRi0q" data-number="4599">
        <h4>
          <a href="https://openreview.net/forum?id=W6BpshgRi0q">
              Ask2Mask: Guided Data Selection for Masked Speech Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=W6BpshgRi0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Murali_Karthick_Baskar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Murali_Karthick_Baskar1">Murali Karthick Baskar</a>, <a href="https://openreview.net/profile?id=~Andrew_Rosenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Rosenberg1">Andrew Rosenberg</a>, <a href="https://openreview.net/profile?id=~Bhuvana_Ramabhadran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhuvana_Ramabhadran2">Bhuvana Ramabhadran</a>, <a href="https://openreview.net/profile?id=~Yu_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zhang2">Yu Zhang</a>, <a href="https://openreview.net/profile?email=pedro%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="pedro@google.com">Pedro Moreno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#W6BpshgRi0q-details-97" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W6BpshgRi0q-details-97"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM), Data selection, Self-supervision, ASR, Speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work,  we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training.  ATM employs an external ASR model or \textit{scorer} to weight unsupervised input samples in two different ways: 1) A fine-grained data selection is performed by masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. 2) ATM is further extended to focus at utterance-level by weighting the final MSM loss with the utterance-level confidence score.  We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and AMI (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6\% relative) while still yielding modest improvements under matched conditions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Data selection Approach for masked speech model to focus on relevant samples to learn meaningful speech representations</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZWykq5n4zx" data-number="4598">
        <h4>
          <a href="https://openreview.net/forum?id=ZWykq5n4zx">
              Boosting the Confidence of Near-Tight Generalization Bounds for Uniformly Stable Randomized Algorithms
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZWykq5n4zx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaotong_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaotong_Yuan1">Xiaotong Yuan</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZWykq5n4zx-details-544" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZWykq5n4zx-details-544"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uniform stability, Randomized learning algorithms, Bagging, Generalization bounds, Stochastic gradient methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">High probability generalization bounds of uniformly stable learning algorithms have recently been actively studied with a series of near-tight results established by~\citet{feldman2019high,bousquet2020sharper}. However, for randomized algorithms with on-average uniform stability, such as stochastic gradient descent (SGD) with time decaying learning rates, it still remains less well understood if these deviation bounds still hold with high confidence over the internal randomness of algorithm. This paper addresses this open question and makes progress towards answering it inside a classic framework of confidence-boosting. To this end, we first establish an in-expectation first moment generalization error bound for randomized learning algorithm with on-average uniform stability, based on which we then show that a properly designed subbagging process leads to near-tight high probability generalization bounds over the randomness of data and algorithm. We further substantialize these generic results to SGD to derive improved high probability generalization bounds for convex or non-convex optimization with natural time decaying learning rates, which have not been possible to prove with the existing uniform stability results. Specially for deterministic uniformly stable algorithms, our confidence-boosting results improve upon the best known generalization bounds in terms of a logarithmic factor on sample size, which moves a step forward towards resolving an open question raised by~\citet{bousquet2020sharper}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A confidence-boosting method for deriving near-tight generalization bounds with high probability for uniformly stable randomized learning algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EhwEUb2ynIa" data-number="4591">
        <h4>
          <a href="https://openreview.net/forum?id=EhwEUb2ynIa">
              How to Adapt Your Large-Scale Vision-and-Language Model
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EhwEUb2ynIa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konwoo_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Konwoo_Kim1">Konwoo Kim</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepak_Pathak1">Deepak Pathak</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EhwEUb2ynIa-details-550" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EhwEUb2ynIa-details-550"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transfer learning, fine-tuning, layernorm, CLIP, prompt-tuning, adaptation, zero-shot, pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training large-scale vision and language models (e.g. CLIP) has shown promising results in representation and transfer learning. We investigate the question of how to efficiently adapt these models to downstream tasks. For image classification, linear probes have been the standard for ease of use and efficiency, while for language, other approaches like prompt tuning have emerged. We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one. We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board. We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks. Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models. Website at https://sites.google.com/view/adapt-large-scale-models</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a thorough analysis of different methods on how to adapt large-scale pretrained vision-and-language models to several downstream classification tasks, and find that just tuning LayerNorm is an effective fine-tuning baseline.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zLb9oSWy933" data-number="4583">
        <h4>
          <a href="https://openreview.net/forum?id=zLb9oSWy933">
              Fast Finite Width Neural Tangent Kernel
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zLb9oSWy933" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Roman_Novak2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roman_Novak2">Roman Novak</a>, <a href="https://openreview.net/profile?id=~Jascha_Sohl-Dickstein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jascha_Sohl-Dickstein2">Jascha Sohl-Dickstein</a>, <a href="https://openreview.net/profile?id=~Samuel_Stern_Schoenholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samuel_Stern_Schoenholz1">Samuel Stern Schoenholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">29 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zLb9oSWy933-details-277" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zLb9oSWy933-details-277"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Tangent Kernel, NTK, Finite Width, Fast, Algorithm, JAX, Jacobian, Software</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="80" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-msup><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.577em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="normal">Î˜</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>âˆ‚</mi><mi>f</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>âˆ‚</mi><mi>Î¸</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>âˆ‚</mi><mi>f</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>âˆ‚</mi><mi>Î¸</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><mi>T</mi></msup></math></mjx-assistive-mml></mjx-container>, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite-width NTK is notoriously expensive to compute, which severely limits its practical utility. 
        
        We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. 
        Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency.
        
        We open-source (https://github.com/iclr2022anon/fast_finite_width_ntk) our two algorithms as general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc.) and introduce no new hyper-parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop and open-source a new algorithm for fast computation of the finite width Neural Tangent Kernel, the outer product of Jacobians of a neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zLb9oSWy933&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zaALYtvbRlH" data-number="4574">
        <h4>
          <a href="https://openreview.net/forum?id=zaALYtvbRlH">
              SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zaALYtvbRlH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Peng_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Qi1">Peng Qi</a>, <a href="https://openreview.net/profile?id=~Guangtao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangtao_Wang1">Guangtao Wang</a>, <a href="https://openreview.net/profile?id=~Jing_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jing_Huang3">Jing Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zaALYtvbRlH-details-272" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zaALYtvbRlH-details-272"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequential data, sample efficiency, data augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zaALYtvbRlH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PC8u74o7xc2" data-number="4571">
        <h4>
          <a href="https://openreview.net/forum?id=PC8u74o7xc2">
              Embedding models through the lens of Stable Coloring
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PC8u74o7xc2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aditya_Desai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Desai1">Aditya Desai</a>, <a href="https://openreview.net/profile?id=~Shashank_Sonkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shashank_Sonkar1">Shashank Sonkar</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PC8u74o7xc2-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PC8u74o7xc2-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Embedding-based approaches find the semantic meaning of tokens in structured data such as natural language, graphs, and even images. To a great degree, these approaches have developed independently in different domains. However, we find a common principle underlying these formulations, and it is rooted in solutions to the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For instance, we find links between stable coloring, distribution hypothesis in natural language processing, and non-local-means denoising algorithm in image signal processing. We even find that stable coloring has strong connections to a broad class of unsupervised embedding models which is surprising at first since stable coloring is generally applied for combinatorial problems. To establish this connection concretely we define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them. Grounded on this framework, we show that many algorithms ranging across different domains are, in fact, searching for continuous stable coloring solutions of an underlying graph corresponding to the domain.  We show that popular and widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and Vis-Transformer can be understood  as instantiations of our general algorithm that solves the problem of continuous stable coloring. These instantiations offer useful insights into the workings of state-of-the-art models like BERT stimulating new research directions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose unified theoretical framework underlying the state-of-the art embedding models</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XIZaWGCPl0b" data-number="4564">
        <h4>
          <a href="https://openreview.net/forum?id=XIZaWGCPl0b">
              Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XIZaWGCPl0b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Atul_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Atul_Sharma1">Atul Sharma</a>, <a href="https://openreview.net/profile?id=~Wei_Chen26" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Chen26">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Joshua_Christian_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_Christian_Zhao1">Joshua Christian Zhao</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Somali_Chaterji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Somali_Chaterji1">Somali Chaterji</a>, <a href="https://openreview.net/profile?id=~Saurabh_Bagchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Bagchi1">Saurabh Bagchi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">33 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XIZaWGCPl0b-details-574" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XIZaWGCPl0b-details-574"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, aggregation, security, untargeted model poisoning attack</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learningâ€”multi-party, distributed learning in a decentralized environmentâ€”is vulnerable to model poisoning attacks, even more so than centralized learning approaches.  This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Trimmed mean, and FoolsGold.  However, a recently developed targeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be pushed away from the optima to increase the test error rate. In this work, we develop tesseractâ€”a defense against this directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT is based on a simple intuition that in a federated learning setting, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that TESSERACT provides robustness against even an adaptive white-box version of the attack.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">How to defend federated learning against local model poisoning attack, the most effective attack known to date, using the pattern of progression of gradients as each client learns.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XIZaWGCPl0b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rX3rZYP8zZF" data-number="4561">
        <h4>
          <a href="https://openreview.net/forum?id=rX3rZYP8zZF">
              CareGraph: A Graph-based Recommender System for Diabetes Self-Care
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rX3rZYP8zZF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sirinart_Tangruamsub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sirinart_Tangruamsub1">Sirinart Tangruamsub</a>, <a href="https://openreview.net/profile?id=~Karthik_Kappaganthu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Kappaganthu1">Karthik Kappaganthu</a>, <a href="https://openreview.net/profile?email=jodonovan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="jodonovan@teladochealth.com">John O'Donovan</a>, <a href="https://openreview.net/profile?email=anmol.madan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="anmol.madan@teladochealth.com">Anmol Madan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rX3rZYP8zZF-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rX3rZYP8zZF-details-370"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graph, knowledge graph embedding, recommendation system</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management.  We propose a Deep Neural Network-based recommender that uses the knowledge graph embeddings to recommend health nudges for maximizing engagement by combating the cold-start and sparsity problems. We use a leave-one-out approach to evaluate the model. We compare the proposed model performance with a text similarity and Deep-and-Cross Network-based approach as the baseline. The overall improvement in Click-Through-Rate prediction AUC for the Knowledge-Graph-based model was 11%. We also observe that our model improved the average AUC by 5% in cold-start situations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nct9j3BVswZ" data-number="4558">
        <h4>
          <a href="https://openreview.net/forum?id=Nct9j3BVswZ">
              Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nct9j3BVswZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Sercan_O_Arik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sercan_O_Arik1">Sercan O Arik</a>, <a href="https://openreview.net/profile?id=~Chen-Yu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen-Yu_Lee2">Chen-Yu Lee</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tomas_Pfister1">Tomas Pfister</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nct9j3BVswZ-details-45" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nct9j3BVswZ-details-45"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Anomaly detection, Data refinement, Iterative training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classification method by 6.3 AUC and 12.5 average precision / 22.9 F1-score.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QKEkEFpKBBv" data-number="4557">
        <h4>
          <a href="https://openreview.net/forum?id=QKEkEFpKBBv">
              DNBP: Differentiable Nonparametric Belief Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QKEkEFpKBBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anthony_Opipari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anthony_Opipari1">Anthony Opipari</a>, <a href="https://openreview.net/profile?id=~Jana_Pavlasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jana_Pavlasek1">Jana Pavlasek</a>, <a href="https://openreview.net/profile?email=joecc%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="joecc@umich.edu">Chao Chen</a>, <a href="https://openreview.net/profile?email=shoutian%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shoutian@umich.edu">Shoutian Wang</a>, <a href="https://openreview.net/profile?id=~Karthik_Desingh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Desingh1">Karthik Desingh</a>, <a href="https://openreview.net/profile?id=~Odest_Jenkins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Odest_Jenkins1">Odest Jenkins</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QKEkEFpKBBv-details-265" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QKEkEFpKBBv-details-265"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Belief Propagation, Bayesian Inference, Nonparametric Inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. Existing nonparametric belief propagation methods rely on domain-specific features encoded in the probabilistic factors of a graphical model. In this work, we replace each crafted factor with a differentiable neural network enabling the factors to be learned using an efficient optimization routine from labeled data. By combining differentiable neural networks with an efficient belief propagation algorithm, our method learns to maintain a set of marginal posterior samples using end-to-end training. We evaluate our differentiable nonparametric belief propagation (DNBP) method on a set of articulated pose tracking tasks and compare performance with learned baselines. Results from these experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches. The project webpage is available at: https://sites.google.com/view/diff-nbp</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=QKEkEFpKBBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TEt7PsVZux6" data-number="4540">
        <h4>
          <a href="https://openreview.net/forum?id=TEt7PsVZux6">
              I-PGD-AT: Efficient Adversarial Training via Imitating Iterative PGD Attack 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TEt7PsVZux6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaosen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaosen_Wang1">Xiaosen Wang</a>, <a href="https://openreview.net/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>, <a href="https://openreview.net/profile?id=~Krishnaram_Kenthapadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krishnaram_Kenthapadi1">Krishnaram Kenthapadi</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TEt7PsVZux6-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TEt7PsVZux6-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Single-step Adversarial Training, Catastrophic Overfitting, Adversarial Robustness, Adversarial Example</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial training has been widely used in various machine learning paradigms to improve the robustness; while it would increase the training cost due to the perturbation optimization process. To improve the efficiency, recent studies leverage Fast Gradient Sign Method with Random Start (FGSM-RS) for adversarial training. However, such methods would lead to relatively low robustness and catastrophic overfitting, which means the robustness against iterative attacks (e.g. Projected Gradient Descent (PGD)) would suddenly drop to 0%. Different approaches have been proposed to address this problem, while later studies show that catastrophic overfitting still remains. In this paper, motivated by the fact that expensive iterative adversarial training methods achieve high robustness without catastrophic overfitting, we aim to ask: Can we perform iterative adversarial training in an efficient way? To this end, we first analyze the difference of perturbation generated by FGSM-RS and PGD and find that PGD tends to craft diverse discrete values instead of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="81" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Â±</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> in FGSM-RS. Based on this observation, we propose an efficient single-step adversarial training method I-PGD-AT by adopting I-PGD attack for training, in which I-PGD imitates PGD virtually. Unlike FGSM that crafts the perturbation directly using the sign of gradient, I-PGD imitates the perturbation of PGD based on the magnitude of gradient. Extensive empirical evaluations on CIFAR-10 and Tiny ImageNet demonstrate that our I-PGD-AT can improve the robustness compared with the baselines and significantly delay catastrophic overfitting. Moreover, we explore and discuss the factors that affect catastrophic overfitting. Finally, to demonstrate the generality of I-PGD-AT, we integrate it into PGD adversarial training and show that it can even further improve the robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an efficient adversarial training approach I-PGD-AT by imitating PGD virtually to improve single-step adversarial training effectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TEt7PsVZux6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wQ7RCayXUSl" data-number="4539">
        <h4>
          <a href="https://openreview.net/forum?id=wQ7RCayXUSl">
              Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters.
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wQ7RCayXUSl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seyed_Kamyar_Seyed_Ghasemipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyed_Kamyar_Seyed_Ghasemipour1">Seyed Kamyar Seyed Ghasemipour</a>, <a href="https://openreview.net/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Nachum1">Ofir Nachum</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wQ7RCayXUSl-details-874" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQ7RCayXUSl-details-874"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline reinforcement learning, batch reinforcement learning, ensembles, uncertainty estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order to achieve strong performance in offline reinforcement learning (RL),  it is necessary to act conservatively with respect to confident lower-bounds on anticipated values of actions. Thus, a valuable approach would be to obtain high quality uncertainty estimates on action values. In current supervised learning literature, state-of-the-art approaches to uncertainty estimation and calibration rely on ensembling methods. In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL. We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble. Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods. Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains. In the more challenging domains of the D4RL offline RL benchmark, MSG significantly surpasses highly well-tuned state-of-the-art methods in batch RL. Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective. We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles. We hope our work engenders increased focus into deep network uncertainty estimation techniques directed for reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We demonstrate how significantly beneficial uncertainty estimation through ensembles can be for offline RL and demonstrate much work is still needed for efficient ensembles to be as effective as deep ensembles.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQ7RCayXUSl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="E9e18Ms5TeV" data-number="4537">
        <h4>
          <a href="https://openreview.net/forum?id=E9e18Ms5TeV">
              A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=E9e18Ms5TeV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zachary_Nado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zachary_Nado1">Zachary Nado</a>, <a href="https://openreview.net/profile?id=~Justin_Gilmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Justin_Gilmer1">Justin Gilmer</a>, <a href="https://openreview.net/profile?id=~Christopher_J_Shallue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_J_Shallue1">Christopher J Shallue</a>, <a href="https://openreview.net/profile?id=~Rohan_Anil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Anil1">Rohan Anil</a>, <a href="https://openreview.net/profile?id=~George_Edward_Dahl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Edward_Dahl1">George Edward Dahl</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#E9e18Ms5TeV-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E9e18Ms5TeV-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural networks, deep learning, neural network optimization, hyperparameter tuning, optimizer comparison</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We retune the Nesterov/Adam optimizers on pipelines where LARS/LAMB are commonly used and achieve similar or better performance, providing competitive baselines for the large batch training setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=E9e18Ms5TeV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ueTHF0yAlZ" data-number="4533">
        <h4>
          <a href="https://openreview.net/forum?id=5ueTHF0yAlZ">
              Improving greedy core-set configurations for active learning with uncertainty-scaled distances
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ueTHF0yAlZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuchen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuchen_Li1">Yuchen Li</a>, <a href="https://openreview.net/profile?id=~Frank_Rudzicz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_Rudzicz2">Frank Rudzicz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ueTHF0yAlZ-details-478" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ueTHF0yAlZ-details-478"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Active learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We scale perceived distances of the core-set algorithm by a factor of uncertainty and search for low-confidence configurations, finding significant improvements in sample efficiency across CIFAR10/100 and SVHN image classification, especially in larger acquisition sizes. We show the necessity of our modifications and explain how the improvement is due to a probabilistic quadratic speed-up in the convergence of core-set loss, under assumptions about the relationship of model uncertainty and misclassification.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Improved core-set for active learning using confidence-scaled distances.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ueTHF0yAlZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nus6fOfh1HW" data-number="4531">
        <h4>
          <a href="https://openreview.net/forum?id=Nus6fOfh1HW">
              On the Relationship between Heterophily and Robustness of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nus6fOfh1HW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiong_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiong_Zhu1">Jiong Zhu</a>, <a href="https://openreview.net/profile?id=~Junchen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junchen_Jin1">Junchen Jin</a>, <a href="https://openreview.net/profile?id=~Donald_Loveland2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Donald_Loveland2">Donald Loveland</a>, <a href="https://openreview.net/profile?id=~Michael_T_Schaub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_T_Schaub1">Michael T Schaub</a>, <a href="https://openreview.net/profile?id=~Danai_Koutra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danai_Koutra1">Danai Koutra</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nus6fOfh1HW-details-825" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nus6fOfh1HW-details-825"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph neural networks, adversarial attacks, heterophily, structural perturbation, robustness, relation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels). In this work, we formalize the relation between heterophily and robustness, bridging two topics previously investigated by separate lines of research. We theoretically and empirically show that for graphs exhibiting homophily (low heterophily), impactful structural attacks always lead to increased levels of heterophily, while for graph with heterophily the change in the homophily level depends on the node degrees. By leveraging these insights, we deduce that a design principle identified to significantly improve predictive performance under heterophilyâ€”separate aggregators for ego- and neighbor-embeddingsâ€”can also inherently offer increased robustness to GNNs. Our extensive empirical analysis shows that GNNs adopting this design alone can achieve significantly improved empirical and certifiable robustness compared to the best-performing unvaccinated model. Furthermore, models with this design can be readily combined with explicit defense mechanisms to yield improved robustness with up to 18.33% increase in performance under attacks compared to the best-performing vaccinated model.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We explore the interplay between heterophily &amp; robustness in GNNs, and show that 1) effective structural attacks on homophilous graphs increase heterophily, 2) heterophilous GNN designs can be combined with defense mechanisms for improved robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Nus6fOfh1HW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-29uFS4FiDZ" data-number="4524">
        <h4>
          <a href="https://openreview.net/forum?id=-29uFS4FiDZ">
              Word Sense Induction with Knowledge Distillation from BERT
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-29uFS4FiDZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anik_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anik_Saha1">Anik Saha</a>, <a href="https://openreview.net/profile?id=~Alex_Gittens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Gittens1">Alex Gittens</a>, <a href="https://openreview.net/profile?id=~Bulent_Yener2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bulent_Yener2">Bulent Yener</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-29uFS4FiDZ-details-338" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-29uFS4FiDZ-details-338"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embeddings, sense embeddings, word sense induction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems.  Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Effective approach to distil word meaning from contextual embeddings to word sense embeddings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PGGjnBiQ84G" data-number="4520">
        <h4>
          <a href="https://openreview.net/forum?id=PGGjnBiQ84G">
              Learning Surface Parameterization for Document Image Unwarping
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PGGjnBiQ84G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sagnik_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sagnik_Das1">Sagnik Das</a>, <a href="https://openreview.net/profile?id=~Ke_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ke_Ma3">Ke Ma</a>, <a href="https://openreview.net/profile?id=~Zhixin_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhixin_Shu1">Zhixin Shu</a>, <a href="https://openreview.net/profile?id=~Dimitris_Samaras3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dimitris_Samaras3">Dimitris Samaras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PGGjnBiQ84G-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PGGjnBiQ84G-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit functions, texture mapping, surface parameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping. We propose an efficient method to learn surface parameterization by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates. Our surface parameterization network can be conveniently plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Recent work on differentiable rendering techniques for implicit surfaces has shown high-quality 3D scene reconstruction and view synthesis results. However, these methods typically learn the appearance color as a function of the surface points and lack explicit surface parameterization. Thus they do not allow texture map extraction or texture editing. By introducing explicit surface parameterization and learning with a recent differentiable renderer for implicit surfaces, we demonstrate state-of-the-art document-unwarping via texture extraction. We show that our approach can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. We also demonstrate the usefulness of our system by applying it to document texture editing.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning surface parameterization using rendering loss and multiview images</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PGGjnBiQ84G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4JlwgTbmzXQ" data-number="4519">
        <h4>
          <a href="https://openreview.net/forum?id=4JlwgTbmzXQ">
              EqR: Equivariant Representations for Data-Efficient Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4JlwgTbmzXQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnab_Kumar_Mondal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnab_Kumar_Mondal1">Arnab Kumar Mondal</a>, <a href="https://openreview.net/profile?id=~Vineet_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vineet_Jain1">Vineet Jain</a>, <a href="https://openreview.net/profile?id=~Kaleem_Siddiqi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaleem_Siddiqi1">Kaleem Siddiqi</a>, <a href="https://openreview.net/profile?id=~Siamak_Ravanbakhsh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siamak_Ravanbakhsh1">Siamak Ravanbakhsh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4JlwgTbmzXQ-details-529" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4JlwgTbmzXQ-details-529"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Equivariance, Invariance, Representation learning, Reinforcement learning, Symmetric MDPs, MDP homomorphism, Lie parameterization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agentâ€™s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Equivariant representation learning for data-efficient reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=4JlwgTbmzXQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JHXjK94yH-y" data-number="4518">
        <h4>
          <a href="https://openreview.net/forum?id=JHXjK94yH-y">
              Explore and Control with Adversarial Surprise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JHXjK94yH-y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnaud_Fickinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnaud_Fickinger1">Arnaud Fickinger</a>, <a href="https://openreview.net/profile?id=~Natasha_Jaques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Natasha_Jaques1">Natasha Jaques</a>, <a href="https://openreview.net/profile?id=~Samyak_Parajuli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samyak_Parajuli1">Samyak Parajuli</a>, <a href="https://openreview.net/profile?id=~Michael_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Chang1">Michael Chang</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stuart_Russell1">Stuart Russell</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JHXjK94yH-y-details-658" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JHXjK94yH-y-details-658"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, intrinsic motivation, exploration, multi-agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering. However, a central challenge in unsupervised RL is to extract behaviors that meaningfully affect the world and cover the range of possible outcomes, without getting distracted by inherently unpredictable, uncontrollable, and stochastic elements in the environment. To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences. The Explore agent seeks out states that maximally surprise the Control agent, which in turn aims to minimize surprise, and thereby manipulate the environment to return to familiar and predictable states. The competition between these two policies drives them to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions. Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Two policies play a multi-agent adversarial game over the amount of surprise or observation entropy an agent experiences, leading the agent to fully explore the underlying state space and learn meaningful behaviors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ck_iw4jMC4l" data-number="4508">
        <h4>
          <a href="https://openreview.net/forum?id=Ck_iw4jMC4l">
              Logical Activation Functions: Logit-space equivalents of Boolean Operators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ck_iw4jMC4l" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Scott_C_Lowe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Scott_C_Lowe1">Scott C Lowe</a>, <a href="https://openreview.net/profile?email=robearle11%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="robearle11@gmail.com">Robert Earle</a>, <a href="https://openreview.net/profile?id=~Jason_d%26%23x27%3BEon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_d&#39;Eon1">Jason d'Eon</a>, <a href="https://openreview.net/profile?id=~Thomas_Trappenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Trappenberg1">Thomas Trappenberg</a>, <a href="https://openreview.net/profile?id=~Sageev_Oore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sageev_Oore1">Sageev Oore</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ck_iw4jMC4l-details-257" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ck_iw4jMC4l-details-257"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">activation functions, logits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence (versus absence) of features within the stimulus. Under this interpretation, we can derive the probability <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="82" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>âˆ©</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> that a pair of independent features are both present in the stimulus from their logits. By converting the resulting probability back into a logit, we obtain a logit-space equivalent of the AND operation. However, since this function involves taking multiple exponents and logarithms, it is not well suited to be directly used within neural networks. We thus constructed an efficient approximation named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="83" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> (the AND operator Approximate for Independent Logits) utilizing only comparison and addition operations, which can be deployed as an activation function in neural networks. Like MaxOut, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="84" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> is a generalization of ReLU to two-dimensions. Additionally, we constructed efficient approximations of the logit-space equivalents to the OR and XNOR operators. We deployed these new activation functions, both in isolation and in conjunction, and demonstrated their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We motivate novel activation functions which are logit-space equivalents to boolean operations, and find they work well on a wide variety of tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ofLwshMBL_H" data-number="4507">
        <h4>
          <a href="https://openreview.net/forum?id=ofLwshMBL_H">
              Continual Learning Using Task Conditional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ofLwshMBL_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Honglin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Honglin_Li1">Honglin Li</a>, <a href="https://openreview.net/profile?id=~Frieder_Ganz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frieder_Ganz1">Frieder Ganz</a>, <a href="https://openreview.net/profile?id=~David_J._Sharp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_J._Sharp1">David J. Sharp</a>, <a href="https://openreview.net/profile?id=~Payam_M._Barnaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_M._Barnaghi1">Payam M. Barnaghi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ofLwshMBL_H-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ofLwshMBL_H-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">catastrophic forgetting, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning changes, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. Dynamic approaches, which assign new neuron resources to the upcoming tasks, are introduced to address this issue. However, most of the dynamic methods need task information about the upcoming tasks during the inference phase to activate the corresponding neurons. To address this issue, we introduce Task Conditional Neural Network which allows the model to identify the task information automatically. The proposed model can continually learn and embed new tasks into the model without losing the information about previously learned tasks. We evaluate the proposed model combined with the mixture of experts approach on the MNIST and CIFAR100 datasets and show how it significantly improves the continual learning process without requiring task information in advance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A dynamic approach for continual learning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xspalMXAB0M" data-number="4483">
        <h4>
          <a href="https://openreview.net/forum?id=xspalMXAB0M">
              A Boosting Approach to Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xspalMXAB0M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nataly_Brukhim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nataly_Brukhim1">Nataly Brukhim</a>, <a href="https://openreview.net/profile?id=~Elad_Hazan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elad_Hazan1">Elad Hazan</a>, <a href="https://openreview.net/profile?id=~Karan_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karan_Singh1">Karan Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xspalMXAB0M-details-921" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xspalMXAB0M-details-921"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form.
            Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function, or by relaxing the solution guarantee to a local optimality condition.
        
            We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an effective policy. The notion of weak learning we study is that of sampled-based approximate optimization of linear functions over policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods iteratively. We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular, our bound does not explicitly depend on the number of states.
        
            A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global optimality guarantees.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xspalMXAB0M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HUeyM2qVey2" data-number="4482">
        <h4>
          <a href="https://openreview.net/forum?id=HUeyM2qVey2">
              Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HUeyM2qVey2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michael_Anthony_Puthawala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Anthony_Puthawala1">Michael Anthony Puthawala</a>, <a href="https://openreview.net/profile?id=~Matti_Lassas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matti_Lassas1">Matti Lassas</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_DokmaniÄ‡1">Ivan DokmaniÄ‡</a>, <a href="https://openreview.net/profile?id=~Maarten_V._de_Hoop2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maarten_V._de_Hoop2">Maarten V. de Hoop</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HUeyM2qVey2-details-492" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HUeyM2qVey2-details-492"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Universality, Flow Networks, Manifold Learning, Density Estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements. We find that such networks universally approximate a large class of manifolds simultaneously with densities supported on them. Among others, our results apply to the well-known coupling and autoregressive flows. We build on the work of Teshima et al. 2020 on bijective flows and study injective architectures proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a new theoretical device called the \emph{embedding gap}, which measures how far one continuous manifold is from embedding another. We relate the embedding gap to a relaxation of universally we call the \emph{manifold embedding property}, capturing the geometric part of universality. Our proof also establishes that optimality of a network can be established ``in reverse,''  resolving a conjecture made in Brehmer et al. 2020 and opening the door for simple layer-wise training schemes. Finally, we show that the studied networks admit an exact layer-wise projection result, Bayesian uncertainty quantification, and black-box recovery of network weights.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements and find that such networks universally approximate a large class of manifolds and densities there on.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zrdUVVAvcP2" data-number="4477">
        <h4>
          <a href="https://openreview.net/forum?id=zrdUVVAvcP2">
              GrASP: Gradient-Based Affordance Selection for Planning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zrdUVVAvcP2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vivek_Veeriah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vivek_Veeriah2">Vivek Veeriah</a>, <a href="https://openreview.net/profile?id=~Zeyu_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zeyu_Zheng1">Zeyu Zheng</a>, <a href="https://openreview.net/profile?id=~Richard_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Lewis1">Richard Lewis</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satinder_Singh2">Satinder Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zrdUVVAvcP2-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zrdUVVAvcP2-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, affordances</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for \emph{selecting} affordances useful for planning---for learning which small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option  affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning to select affordances in the form of options and primitive actions for lookahead planning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="14kbUbOaZUc" data-number="4476">
        <h4>
          <a href="https://openreview.net/forum?id=14kbUbOaZUc">
              Metric Learning on Temporal Graphs via Few-Shot Examples
          </a>
        
          
            <a href="https://openreview.net/pdf?id=14kbUbOaZUc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongqi_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongqi_Fu1">Dongqi Fu</a>, <a href="https://openreview.net/profile?id=~Liri_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liri_Fang1">Liri Fang</a>, <a href="https://openreview.net/profile?id=~Ross_Maciejewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ross_Maciejewski1">Ross Maciejewski</a>, <a href="https://openreview.net/profile?id=~Vetle_I_Torvik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vetle_I_Torvik1">Vetle I Torvik</a>, <a href="https://openreview.net/profile?id=~Jingrui_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingrui_He1">Jingrui He</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#14kbUbOaZUc-details-155" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="14kbUbOaZUc-details-155"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Metric Learning, Few-Shot Learning, Temporal Graph</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs; Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales. Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The first attempt to learn temporal graph representations, on the graph-level, covering the whole lifetime, and only consuming a few labeled samples. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=14kbUbOaZUc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a61qArWbjw_" data-number="4475">
        <h4>
          <a href="https://openreview.net/forum?id=a61qArWbjw_">
              Scalable multimodal variational autoencoders with surrogate joint posterior
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a61qArWbjw_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Masahiro_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Masahiro_Suzuki1">Masahiro Suzuki</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yutaka_Matsuo1">Yutaka Matsuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a61qArWbjw_-details-465" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a61qArWbjw_-details-465"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep generative models, multimodal learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To obtain a joint representation from multimodal data in variational autoencoders (VAEs), it is important to infer the representation from arbitrary subsets of modalities after learning.  A scalable way to achieve this is to aggregate the inferences of each modality as experts. A state-of-the-art approach to learning this aggregation of experts is to encourage all modalities to be reconstructed and cross-generated from arbitrary subsets. However, this learning may be insufficient if cross-generation is difficult. Furthermore, to evaluate its objective function, exponential generation paths concerning the number of modalities are required. To alleviate these problems, we propose to explicitly minimize the divergence between inferences from arbitrary subsets and the surrogate joint posterior that approximates the true joint posterior. We also proposed using a gradient origin network, a deep generative model that learns inferences without using an inference network, thereby reducing the need for additional parameters by introducing the surrogate posterior. We demonstrate that our method performs better than existing scalable multimodal VAEs in inference and generation.    
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We proposed a scalable and high performance multimodal VAE in the framework of approximating inferences from arbitrary subsets of modalities to a surrogate joint posterior.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fwJWhOxuzV9" data-number="4471">
        <h4>
          <a href="https://openreview.net/forum?id=fwJWhOxuzV9">
              Semi-supervised Offline Reinforcement Learning with Pre-trained Decision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fwJWhOxuzV9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Catherine_Cang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Catherine_Cang1">Catherine Cang</a>, <a href="https://openreview.net/profile?id=~Kourosh_Hakhamaneshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Hakhamaneshi1">Kourosh Hakhamaneshi</a>, <a href="https://openreview.net/profile?id=~Ryan_Rudes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryan_Rudes1">Ryan Rudes</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Aravind_Rajeswaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aravind_Rajeswaran1">Aravind Rajeswaran</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fwJWhOxuzV9-details-870" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fwJWhOxuzV9-details-870"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-task RL, Decision Transformer, self-supervised RL, Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training deep neural network models using large unlabelled datasets followed by fine-tuning them on small task-specific datasets has emerged as a dominant paradigm in natural language processing (NLP) and computer vision (CV). Despite the widespread success, such a paradigm has remained atypical in reinforcement learning (RL).
        In this paper, we investigate how we can leverage large reward-free (i.e. task-agnostic) offline datasets of prior interactions to pre-train agents that can then be fine-tuned using a small reward-annotated dataset. To this end, we present Pre-trained Decision Transformer (PDT), a simple yet powerful algorithm for semi-supervised Offline RL. By masking reward tokens during pre-training, the transformer learns to autoregressivley predict actions based on previous state and action context and effectively extracts behaviors present in the dataset. During fine-tuning, rewards are un-masked and the agent learns the set of skills that should be invoked for the desired behavior as per the reward function. We demonstrate the efficacy of this simple and flexible approach on tasks from the D4RL benchmark with limited reward annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce Pre-trained Decision Transformers, a simple and flexible architecture that can be pre-trained on unlabeled environment interactions and can quickly adapt to several downstream tasks with just a small reward-annotated fine-tuning dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_Ko4kT3ckWy" data-number="4470">
        <h4>
          <a href="https://openreview.net/forum?id=_Ko4kT3ckWy">
              Increase and Conquer: Training Graph Neural Networks on Growing Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_Ko4kT3ckWy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Juan_Cervino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_Cervino1">Juan Cervino</a>, <a href="https://openreview.net/profile?id=~Luana_Ruiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luana_Ruiz1">Luana Ruiz</a>, <a href="https://openreview.net/profile?id=~Alejandro_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alejandro_Ribeiro1">Alejandro Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_Ko4kT3ckWy-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_Ko4kT3ckWy-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Graph Neural Networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) use graph convolutions to exploit network invariances and learn meaningful features from network data. However, on large-scale graphs convolutions incur in high computational cost, leading to scalability limitations. Leveraging the graphon --- the limit object of a graph --- in this paper we consider the problem of learning a graphon neural network (WNN) --- the limit object of a GNN --- by training GNNs on graphs sampled Bernoulli from the graphon. Under smoothness conditions, we show that: (i) the expected distance between the learning steps on the GNN and on the WNN decreases asymptotically with the size of the graph, and (ii) when training on a sequence of growing graphs, gradient descent follows the learning direction of the WNN. Inspired by these results, we propose a novel algorithm to learn GNNs on large-scale graphs that, starting from a moderate number of nodes, successively increases the size of the graph during training. This algorithm is benchmarked on both a recommendation system and a decentralized control problem where it is shown to retain comparable performance, to its large-scale counterpart, at a reduced computational cost.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper describes a way to train GNNs on a sequence of growing graphs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_Ko4kT3ckWy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MQuxKr2F1Xw" data-number="4468">
        <h4>
          <a href="https://openreview.net/forum?id=MQuxKr2F1Xw">
              Multi-Trigger-Key: Towards Multi-Task Privacy-Preserving In Deep Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MQuxKr2F1Xw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ren_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ren_Wang1">Ren Wang</a>, <a href="https://openreview.net/profile?id=~Zhe_Xu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhe_Xu7">Zhe Xu</a>, <a href="https://openreview.net/profile?id=~Alfred_Hero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alfred_Hero1">Alfred Hero</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MQuxKr2F1Xw-details-953" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MQuxKr2F1Xw-details-953"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning-based Multi-Task Classification (MTC) is widely used in applications like facial attribute and healthcare that warrant strong privacy guarantees. In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective. MTK associates each secured task in the multi-task dataset with a specifically designed trigger-key. The true information can be revealed by adding the trigger-key if the user is authorized. We obtain such an MTK model by training it with a newly generated training set. To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance. Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=MQuxKr2F1Xw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="31">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="desk-rejected-withdrawn-submissions">

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="XNYOJD0QdBD" data-number="4713">
        <h4>
          <a href="https://openreview.net/forum?id=XNYOJD0QdBD">
              Personalized PageRank meets Graph Attention Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XNYOJD0QdBD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Julie_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julie_Choi1">Julie Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XNYOJD0QdBD-details-676" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XNYOJD0QdBD-details-676"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GNN, Personalized PageRank, Graph Attention Network, Graph Neural Network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rising interest in graph neural networks (GNNs) for representation learning over the past few years. GNNs provide a general and efficient framework to learn from graph-structured data. However, GNNs typically only use the information of a very limited neighborhood for each node. A larger neighborhood would be desirable to provide the model with more information. However, increasing the size of the neighborhood is not trivial since neighborhood aggregation over many layers leads to over-smoothing. In this work, we incorporate the limit distribution of Personalized PageRank (PPR) into graph attention networks (GATs) to address this issue. Intuitively, message aggregation based on Personalized PageRank corresponds to infinitely many neighborhood aggregation layers. We show that our models outperform a variety ofbaseline models across all datasets used for our experiments. Our implementation is publicly available online.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Personalized PageRank meets Graph Attention Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HLTLhiBtUcW" data-number="4712">
        <h4>
          <a href="https://openreview.net/forum?id=HLTLhiBtUcW">
              Enhanced neural network regularization with macro-block dropout
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HLTLhiBtUcW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HLTLhiBtUcW-details-188" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HLTLhiBtUcW-details-188"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">macro block dropout, regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> This paper proposes a new regularization algorithm referred to as macro-block dropout. The overfitting issue has been a difficult problem in training large network models. The dropout technique has proven to be simple yet very effective for regularization by preventing complex co-adaptations on training data.  In this work, we observe that in the hidden outputs, the correlations between geometrically close elements are usually stronger than those between distant elements. Motivated by this observation, we define a macro-block that contains multiple elements of the hidden output layer in order to reduce co-adaptations more effectively. Rather than applying dropout to each element, we apply random dropout to each macro-block. In our experiments with  image classification tasks on the MNIST and the ImageNet datasets as well as a speech recognition task on the LibriSpeech set, this simple algorithm has shown a quite significant improvement over the conventional dropout approach</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a macro-block dropout for better regularization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XY1DWeh58WR" data-number="4709">
        <h4>
          <a href="https://openreview.net/forum?id=XY1DWeh58WR">
              Deep Recurrent Neural Network Layers with Layerwise Loss
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XY1DWeh58WR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XY1DWeh58WR-details-82" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XY1DWeh58WR-details-82"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning techniques have brought significant performance improvement to various areas of machine learning. Especially in the computer vision area, very deep networks such as ResNet have shown notable performance improvement. However, in speech recognition or language processing, such kinds of a very deep network have not been extensively employed. In this paper, we propose a very deep LSTM structure and their training strategy. In our training strategy, we first start training a conventional model with several LSTM layers. One notable difference is that for the top LSTM layer of the initial model, the Connectionist Temporal Classification (CTC) loss is applied both to the input and output of this top LSTM layer. Once this initial model is sufficiently layered, this top layer is copied to construct a very deep LSTM stack. For this newly constructed stack, the CTC loss is applied to every output of the LSTM layer as well as the top of the stack. Experimental results show that this deep LSTM structure shows significantly better results than the conventional model with 5 ~ 6 layers with a comparable number of parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a very deep neural network with layerwise loss</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="np5BgCFSsbm" data-number="4433">
        <h4>
          <a href="https://openreview.net/forum?id=np5BgCFSsbm">
              Neocortical cell type classification from electrophysiology recordings using deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=np5BgCFSsbm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raymond_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raymond_Wang1">Raymond Wang</a>, <a href="https://openreview.net/profile?id=~Sang_Min_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sang_Min_Han1">Sang Min Han</a>, <a href="https://openreview.net/profile?id=~Marta_Agnieszka_Gajowa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marta_Agnieszka_Gajowa1">Marta Agnieszka Gajowa</a>, <a href="https://openreview.net/profile?id=~Chunlei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chunlei_Liu1">Chunlei Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#np5BgCFSsbm-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="np5BgCFSsbm-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuron type classification, convolutional neural network, electrophysiology</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the neural code requires identifying different functional units involved in the neural circuits. One way to identify these functional units is to solve a neuron type classification problem. For decades, current clamp electrophysiology recordings have provided the means to classify the neurons based on subtle differences in action potential shapes and spiking patterns. However, significant variations in neuronal type definitions, classification pipelines, and variability in the neuronal activities make unambiguous determination of neuron type challenging. Previous solutions to this electrophysiology-based cell type classification problem consisted of dimensionality reduction juxtaposed with clustering using hand-crafted action potential features. Recent discoveries have allowed genetic-based cell-type classifications, which have fewer ambiguities, but they are less practical in vivo and have even lower throughput. Leveraging the unprecedented ground truth data published in the Allen Institute Cell Types Database, which contains anatomical, genetic, and electrophysiology characterizations of neurons in the mouse neocortex, we construct a robust and efficient convolutional neural network (CNN) that successfully classifies neurons according to their genetic label or broad type (excitatory or inhibitory) solely using current-clamp electrophysiology recordings. The CNN is configured as a multiple-input single-output network consisting of three subnetworks that take in the raw time series electrophysiology recording as well as the real and imaginary components of its Fourier coefficients. Our single pipeline method is fast and streamlined while simultaneously outperforming previous methods and achieving more classification classes using only single current-clamp trace as the input. This end-to-end convolutional neural network-based classification method removes the need for hand-crafted features, specific knowledge, or human intervention for quick identification of the cell type with high accuracy, enabling interpretation of the experimental data in a bias-free manner and a much broader scientific context.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust and efficient convolutional neural network successfully classifies neurons according to their genetic label and broad type using only current-clamp electrophysiology recordings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NMSugaVzIT" data-number="4294">
        <h4>
          <a href="https://openreview.net/forum?id=NMSugaVzIT">
              Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMSugaVzIT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Meena_Jagadeesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Meena_Jagadeesan1">Meena Jagadeesan</a>, <a href="https://openreview.net/profile?id=~Ilya_Razenshteyn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ilya_Razenshteyn1">Ilya Razenshteyn</a>, <a href="https://openreview.net/profile?id=~Suriya_Gunasekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Suriya_Gunasekar1">Suriya Gunasekar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMSugaVzIT-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMSugaVzIT-details-264"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">minimizing parameter l2 norm, representation cost, implicit bias</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We provide a function space characterization of the inductive bias resulting from minimizing the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="85" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of the weights in multi-channel linear convolutional networks. We define an \textit{induced regularizer} in the function space as the minimum <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="86" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of weights of a network required to realize a function.  For two layer linear convolutional networks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="87" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> output channels and kernel size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="88" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, we show the following: (a) If the inputs to the network have a single channel, the induced regularizer for any <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="89" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is \textit{independent} of the number of output channels <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="90" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for networks with multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias \emph{does} depend on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="91" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. However, for sufficiently large <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="92" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>, the induced regularizer is again given by an SDP that is independent of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="93" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. In particular, the induced regularizer for  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="94" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="95" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mi>D</mi></math></mjx-assistive-mml></mjx-container> are given in closed form as the nuclear norm and the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="96" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> group-sparse norm, respectively, of the Fourier coefficients.
        We investigate the applicability of our theoretical results to a broader scope of ReLU convolutional networks through experiments on MNIST and CIFAR-10 datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the function space view of minimizing l2 norm of weights in multi-channel linear convolutional networks, uncovering an invariance to the number of output channels. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMSugaVzIT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DVSN9nJB1_" data-number="3792">
        <h4>
          <a href="https://openreview.net/forum?id=DVSN9nJB1_">
              E-LANG: Energy-based Joint Inferencing of Super and Swift Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DVSN9nJB1_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mohammad_Akbari3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Akbari3">Mohammad Akbari</a>, <a href="https://openreview.net/profile?id=~Amin_Banitalebi-Dehkordi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amin_Banitalebi-Dehkordi1">Amin Banitalebi-Dehkordi</a>, <a href="https://openreview.net/profile?id=~Yong_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yong_Zhang2">Yong Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DVSN9nJB1_-details-385" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DVSN9nJB1_-details-385"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based models, dynamic inference, joint language models, super model optimization, NLP, BERT, T5</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building very large and highly capable language models has been a trend in the past several years. Despite their great performance, they incur a high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the incoming samples to one of the two models based on the energy characteristics of the representations in the latent space. The proposed approach is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. Unlike existing methods that are for the most part only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The performance of the proposed Energy-based joint inferencing of LANGuage models, E-LANG, is verified through an extensive set of experiments with T5 and BERT architectures on GLUE, SuperGLUE, and WMT benchmarks. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA (state-of-the-art) on GLUE with 3.2X less computations. Code is available in the supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we present E-LANG, an energy-based joint inference approach, which combines Super and Swift language models for achieving efficient inference without sacrificing the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=DVSN9nJB1_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="k_Zy6glYaqc" data-number="3557">
        <h4>
          <a href="https://openreview.net/forum?id=k_Zy6glYaqc">
              Quantum Alphatron
          </a>
        
          
            <a href="https://openreview.net/pdf?id=k_Zy6glYaqc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyi_Yang1">Siyi Yang</a>, <a href="https://openreview.net/profile?id=~Patrick_Rebentrost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_Rebentrost1">Patrick Rebentrost</a>, <a href="https://openreview.net/profile?id=~Miklos_Santha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miklos_Santha1">Miklos Santha</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#k_Zy6glYaqc-details-310" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="k_Zy6glYaqc-details-310"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finding provably efficient algorithms for learning neural networks is a fundamental challenge in the theory of machine learning. The Alphatron of Goel and Klivans is the first provably efficient algorithm for learning neural networks with more than one nonlinear layer. The algorithm succeeds with any distribution on the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="97" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-dimensional unit ball and without any assumption on the structure of the network. In this work, we refine the original Alphatron by a pre-computing phase for its most time-consuming part, the evaluation of the kernel function. This refined algorithm improves the run time of the original Alphatron, while retaining the same learning guarantee. Based on the refined algorithm, we quantize the pre-computing phase with provable learning guarantee in the fault-tolerant quantum computing model. In a well-defined learning model, this quantum algorithm is able to provide a quadratic speedup in the data dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="98" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>. In addition, we discuss the second type of speedup, quantizing the evaluation of the gradient in the stochastic gradient descent procedure. Our work contributes to the study of quantum learning with kernels and from samples.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QWc35QxXPzZ" data-number="3480">
        <h4>
          <a href="https://openreview.net/forum?id=QWc35QxXPzZ">
              The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QWc35QxXPzZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chi_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chi_Jin1">Chi Jin</a>, <a href="https://openreview.net/profile?id=~Qinghua_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghua_Liu1">Qinghua Liu</a>, <a href="https://openreview.net/profile?id=~Tiancheng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tiancheng_Yu1">Tiancheng Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QWc35QxXPzZ-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QWc35QxXPzZ-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">theoretical reinforcement learning, Markov games with general function approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern reinforcement learning (RL) commonly engages practical problems with large state spaces, where function approximation must be deployed to approximate either the value function or the policy. While recent progresses in RL theory address a rich set of RL problems with general function approximation, such successes are mostly restricted to the single-agent setting. It remains elusive how to extend these results to multi-agent RL, especially due to the new challenges arising from its game-theoretical nature. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algorithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension -- a new complexity measure adapted from its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter, which facilitates the learning of the main player by deliberately exploiting her weakness. Our theoretical framework is generic, which applies to a wide range of models including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper studies sample-efficient learning of Markov Games with general function approximation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6Q5RdltG3L" data-number="3243">
        <h4>
          <a href="https://openreview.net/forum?id=6Q5RdltG3L">
              Human imperceptible attacks and applications to improve fairness
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6Q5RdltG3L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xinru_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xinru_Hua1">Xinru Hua</a>, <a href="https://openreview.net/profile?id=~Huanzhong_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huanzhong_Xu1">Huanzhong Xu</a>, <a href="https://openreview.net/profile?id=~Jose_Blanchet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jose_Blanchet1">Jose Blanchet</a>, <a href="https://openreview.net/profile?id=~Viet_Anh_Nguyen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Viet_Anh_Nguyen2">Viet Anh Nguyen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6Q5RdltG3L-details-494" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Q5RdltG3L-details-494"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural networks are able to perform at least as well as humans in numerous tasks involving object classification and image generation. However, there is also evidence that perturbations which are imperceptible to humans may significantly degrade the performance of well-trained deep neural networks. We provide a Distributionally Robust Optimization (DRO) framework which integrates human-based image quality assessment methods to design optimal attacks that are imperceptible to humans but significantly damaging to deep neural networks. Our attack algorithm can generate better-quality (less perceptible to humans) attacks than other state-of-the-art human imperceptible attack methods. We provide an algorithmic implementation of independent interest which can speed up DRO training significantly. Finally, we demonstrate how the use of optimally designed human imperceptible attacks can improve group fairness in image classification while maintaining a similar accuracy.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=6Q5RdltG3L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vNDHZZa-Q92" data-number="3220">
        <h4>
          <a href="https://openreview.net/forum?id=vNDHZZa-Q92">
              Neural Extensions: Training Neural Networks with Set Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vNDHZZa-Q92" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nikolaos_Karalias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikolaos_Karalias1">Nikolaos Karalias</a>, <a href="https://openreview.net/profile?id=~Joshua_David_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_David_Robinson1">Joshua David Robinson</a>, <a href="https://openreview.net/profile?id=~Andreas_Loukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Loukas1">Andreas Loukas</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vNDHZZa-Q92-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vNDHZZa-Q92-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continuous extensions, algorithmic reasoning, set functions, machine learning, combinatorial optimization, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integrating discrete computational steps into deep learning architectures is an important consideration when learning to reason over discrete items. However, many tasks that involve discrete choices are defined via (combinatorial) set functions, and thereby pose challenges for end-to-end training. In this work, we explore a general framework to construct continuous extensions of such discrete functions that enables training via gradient methods. Our framework includes well-known extensions such as the Lovasz extension of submodular set functions and facilitates the design of novel continuous extensions based on problem-specific considerations, including constraints. We demonstrate the versatility of our framework on tasks ranging from combinatorial optimization to image classification. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A principled framework for continuous extensions of set functions in machine learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oykI6Kmq3Xi" data-number="3194">
        <h4>
          <a href="https://openreview.net/forum?id=oykI6Kmq3Xi">
              Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oykI6Kmq3Xi" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ryann_Sim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryann_Sim1">Ryann Sim</a>, <a href="https://openreview.net/profile?id=~EFSTRATIOS_PANTELEIMON_SKOULAKIS2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~EFSTRATIOS_PANTELEIMON_SKOULAKIS2">EFSTRATIOS PANTELEIMON SKOULAKIS</a>, <a href="https://openreview.net/profile?id=~Lillian_J_Ratliff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lillian_J_Ratliff1">Lillian J Ratliff</a>, <a href="https://openreview.net/profile?id=~Georgios_Piliouras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Piliouras1">Georgios Piliouras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oykI6Kmq3Xi-details-940" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oykI6Kmq3Xi-details-940"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">extensive form games, network extensive form games, online learning, optimistic gradient descent ascent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFG) and particularly in EFGs with many agents lags far behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="99" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> rate of convergence to the set of Nash equilibria. Moreover, we show that the day-to-day behavior also converges to Nash with rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="100" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mrow data-mjx-texclass="ORD"><mo>âˆ’</mo><mi>t</mi></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for some game-dependent constant <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="101" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mo>&gt;</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide a formulation of network zero-sum extensive form games and show that optimistic gradient ascent admits fast convergence to Nash, both in time average and in the day-to-day sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oykI6Kmq3Xi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JvVFSmFV8G" data-number="3163">
        <h4>
          <a href="https://openreview.net/forum?id=JvVFSmFV8G">
              Which model to trust: assessing the influence of models on the performance of reinforcement learning algorithms for continuous control tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JvVFSmFV8G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Giacomo_Arcieri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Giacomo_Arcieri1">Giacomo Arcieri</a>, <a href="https://openreview.net/profile?email=woelfle%40fzi.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="woelfle@fzi.de">David WÃ¶lfle</a>, <a href="https://openreview.net/profile?id=~Eleni_Chatzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleni_Chatzi1">Eleni Chatzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JvVFSmFV8G-details-602" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JvVFSmFV8G-details-602"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, model-based reinforcement learning, deep learning, bayesian deep learning, gaussian processes, continuous control, model uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JvVFSmFV8G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ulj0tR-k7q" data-number="3070">
        <h4>
          <a href="https://openreview.net/forum?id=Ulj0tR-k7q">
              On strong convergence of the two-tower model for recommender system
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ulj0tR-k7q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~SHIRONG_XU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SHIRONG_XU1">SHIRONG XU</a>, <a href="https://openreview.net/profile?id=~Junhui_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junhui_Wang3">Junhui Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ulj0tR-k7q-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ulj0tR-k7q-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Artificial neural networks, Collaborative filtering, Empirical process, Recommender system, Two-tower model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recommender system is capable of predicting preferred items for a user by integrating information from similar users or items. A popular model in recommender system is the so-called two-tower model, which employs two deep neural networks to embed users and items into a low-dimensional space, and predicts ratings via the geometrical relationship of the embeddings of user and item in the embedded space. Even though it is popularly used for recommendations, its theoretical properties remain largely unknown. In this paper, we establish some asymptotic results of the two-tower model in terms of its strong convergence to the optimal recommender system, showing that it achieves a fast convergence rate depending on the intrinsic dimensions of inputs features. To the best of our knowledge, this is among the first attempts to establish the statistical guarantee of the two-tower model. Through numerical experiments, we also demonstrate that the two-tower model is capable of capturing the effects of users' and items' features on ratings, leading to higher prediction accuracy over its competitors in both simulated examples and a real application data set. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Establishing theoretical guarantee for the two-tower model in recommender system</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ulj0tR-k7q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UquMPXFTpgp" data-number="2976">
        <h4>
          <a href="https://openreview.net/forum?id=UquMPXFTpgp">
              Cluster Tree for Nearest Neighbor Search
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UquMPXFTpgp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dan_Kushnir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Kushnir1">Dan Kushnir</a>, <a href="https://openreview.net/profile?id=~Sandeep_Silwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandeep_Silwal1">Sandeep Silwal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UquMPXFTpgp-details-391" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UquMPXFTpgp-details-391"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nearest neighbor search, tree algorithms, graph cuts, random projections</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Tree-based algorithms are an important and widely used class of algorithms for  Nearest Neighbor Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in spite of possessing theoretical guarantees and strong practical performance, a major drawback of the RP tree is its lack of adaptability to the input dataset.
        
        Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing ClusterTree, a new tree based algorithm. Our approach utilizes randomness as in RP trees while adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees and other tree based methods for NNS while maintaining efficient construction time. In addition, we show theoretically and empirically that ClusterTree finds partitions which are superior to those found by RP trees in preserving the cluster structure of the input dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel tree-based algorithm for nearest neighbor search which adapts to the cluster structure of the input dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xGZcxaYbJBF" data-number="2193">
        <h4>
          <a href="https://openreview.net/forum?id=xGZcxaYbJBF">
              A Multi-Task Learning Algorithm for Non-personalized Recommendations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xGZcxaYbJBF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Zhang8">Jiawei Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xGZcxaYbJBF-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xGZcxaYbJBF-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Recommendation and Ranking, Non-personalized Recommendations, Multitask Learning, collaborative filtering, Two-tower DNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce a multi-task learning (MTL) algorithm for recommending non-personalized videos to watch next on industrial video sharing platforms. Personalized recommendations have been studied for decades, while researches on non-personalized solutions are very rare to be seen, which still remain a huge portion in industry. As an indispensable part in recommender system, non-personalized video recommender system also faces several real-world challenges, including maintaining high relevance between source item and target items, as well as achieving multiple competing ranking objectives. To solve these, we largely extended model-based collaborative filtering algorithm by adding related candidate generation stage, Two-tower DNN structure and a multi-task learning mechanism. Compared with typical baseline solutions, our proposed algorithm can capture both linear and non-linear relationships from user-item interactions, and live experiments demonstrate that it can significantly advance the state of the art on recommendation quality.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A multi-task learning (MTL) algorithm is introduced for recommending non-personalized videos to watch next on industrial video sharing platforms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XTzAhbVbKgq" data-number="2123">
        <h4>
          <a href="https://openreview.net/forum?id=XTzAhbVbKgq">
              Batched Lipschitz Bandits
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XTzAhbVbKgq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yasong_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yasong_Feng1">Yasong Feng</a>, <a href="https://openreview.net/profile?id=~Zengfeng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zengfeng_Huang1">Zengfeng Huang</a>, <a href="https://openreview.net/profile?id=~Tianyu_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyu_Wang4">Tianyu Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XTzAhbVbKgq-details-375" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XTzAhbVbKgq-details-375"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-armed bandits, online learning, batched bandits, Lipschitz bandits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we study the batched Lipschitz bandit problem, where the expected reward is Lipschitz and the reward observations are collected in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback setting. In particular, we show that for a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="102" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>-step problem with Lipschitz reward of zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="103" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, our algorithm achieves theoretically optimal regret rate of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="104" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.509em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.501em; margin-left: 0.056em;"><mjx-texatom size="s" texclass="ORD"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mi>T</mi><mrow data-mjx-texclass="ORD"><mfrac><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>2</mn></mrow></mfrac></mrow></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> using only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="105" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" style="font-size: 83.3%;"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mrow><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>T</mi></mrow><msub><mi>d</mi><mi>z</mi></msub></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> batches. For the lower bound, we show that in an environment with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="106" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>-batches, for any policy <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="107" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Ï€</mi></math></mjx-assistive-mml></mjx-container>, there exists a problem instance such that the expected regret is lower bounded by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="108" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.361em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c28" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 1.233em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-msup><mjx-mrow><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.763em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c29" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi mathvariant="normal">Î©</mi><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>âˆ’</mo><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow><mi>B</mi></msup></mrow></mfrac></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="109" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="110" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="111" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the dimension of the arm space. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel landscape-aware algorithm to solve the batched Lipschitz bandit problem, and show that our algorithm matches the optimal regret upper bound using less than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="112" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">â¡</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> batches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XTzAhbVbKgq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CdqsSPLNx-" data-number="2094">
        <h4>
          <a href="https://openreview.net/forum?id=CdqsSPLNx-">
              Deep Dynamic Attention Model with Gate Mechanism for Solving Time-dependent Vehicle Routing Problems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CdqsSPLNx-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Feng_Guo7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Guo7">Feng Guo</a>, <a href="https://openreview.net/profile?id=~Qu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qu_Wei1">Qu Wei</a>, <a href="https://openreview.net/profile?id=~Miao_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miao_Wang2">Miao Wang</a>, <a href="https://openreview.net/profile?id=~Zhaoxia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhaoxia_Guo1">Zhaoxia Guo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CdqsSPLNx--details-819" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CdqsSPLNx--details-819"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vehicle routing problems (VRPs) are a type of classical combinatorial optimization problems widely existing in logistics and transportation operations. There has been an increasing interest to use deep reinforcement learning (DRL) techniques to tackle VRPs, and previous DRL-based studies assumed time-independent travel times between customers. However, travel times in real-world road networks are time-varying, which need to be considered in practical VRPs. We thus propose a Deep Dynamic Attention Models with Gate Mechanisms (DDAM-GM) to learn heuristics for time-dependent VRPs (TDVRPs) in real-world road networks. It extracts the information of node location, node demand, and time-varying travel times between nodes to obtain enhanced node embeddings through a dimension-reducing MHA layer and a synchronous encoder. In addition, we use a gate mechanism to obtain better context embedding. On the basis of a 110-day travel time dataset with 240 time periods per day from an urban road network with 408 nodes and 1250 directed links, we conduct a series of experiments to validate the effectiveness of the proposed model on TDVRPs without and with consideration of time windows, respectively. Experimental results show that our model outperforms significantly two state-of-the-art DRL-based models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lDvJM5XUyrx" data-number="1436">
        <h4>
          <a href="https://openreview.net/forum?id=lDvJM5XUyrx">
              Towards Understanding Catastrophic Overfitting in Fast Adversarial Training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lDvJM5XUyrx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Renjie_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Renjie_Chen2">Renjie Chen</a>, <a href="https://openreview.net/profile?id=~Yuan_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan_Luo1">Yuan Luo</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisen_Wang1">Yisen Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lDvJM5XUyrx-details-600" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lDvJM5XUyrx-details-600"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robustness, Fast Adversarial Training, Catastrophic Overfitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">After adversarial training was proposed, a series of works focus on improving the compunational efficiency of adversarial training for deep neural networks (DNNs). Recently, FGSM based single-step adversarial training has been found to be able to train a robust model with the robustness comparable to the one trained by multi-step PGD, but it is an order of magnitude faster. However, there exists a failure mode called Catastrophic Overfitting (CO) where the network loses its robustness against PGD attack suddenly and can be hardly recovered by itself during the training process. In this paper, we identify that CO is closely related to the high-order terms in Taylor expansion after rethinking and decomposing the min-max problem in adversarial training. The negative high-order terms lead to a phenomenon called Perturbation Loss Distortion, which is the underlying cause of CO. Based on the observations, we propose a simple but effective regularization method named Fast Linear Adversarial Training (FLAT) to avoid CO in the single-step adversarial training by making the loss surface flat.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Fast Linear Adversarial Training (FLAT) can help prevent the Catastrophic Overfitting in single-step adversarial training.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lDvJM5XUyrx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZVqsBl2HapR" data-number="1434">
        <h4>
          <a href="https://openreview.net/forum?id=ZVqsBl2HapR">
              Error-based or target-based? A unifying framework for learning in recurrent spiking networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZVqsBl2HapR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cristiano_Capone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cristiano_Capone1">Cristiano Capone</a>, <a href="https://openreview.net/profile?id=~Paolo_Muratore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paolo_Muratore1">Paolo Muratore</a>, <a href="https://openreview.net/profile?id=~Pier_Stanislao_Paolucci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pier_Stanislao_Paolucci1">Pier Stanislao Paolucci</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZVqsBl2HapR-details-915" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZVqsBl2HapR-details-915"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">target-based, error-based, recurrent neural network, spiking neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in biological or artificial networks means changing the laws governing the network dynamics in order to better behave in a specific situation. In the field of supervised learning, two complementary approaches stand out: error-based and target-based learning. However, there exists no consensus on which is better suited for which task, and what is the most biologically plausible. Here we propose a comprehensive theoretical framework that includes these two frameworks as special cases. This novel theoretical formulation offers major insights into the differences between the two approaches. In particular, we show how target-based naturally emerges from error-based when the number of constraints on the target dynamics, and as a consequence on the internal network dynamics, is comparable to the degrees of freedom of the network. Moreover, given the experimental evidences on the relevance that spikes have in biological networks, we investigate the role of coding with specific patterns of spikes by introducing a parameter that defines the tolerance to precise spike timing during learning. Our approach naturally lends itself to Imitation Learning (and Behavioral Cloning in particular) and we apply it to solve relevant closed-loop tasks such as the button-and-food task, and the 2D Bipedal Walker. We show that a high dimensionality feedback structure is extremely important when it is necessary to solve a task that requires retaining memory for a long time (button-and-food). On the other hand, we find that coding with specific patterns of spikes enables optimal performances in a motor task (the 2D Bipedal Walker). Finally, we show that our theoretical formulation suggests protocols to deduce the structure of learning feedback in biological networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new learning framework that includes target- and error-based approches as special cases. It allows to understand their relationship and to explore what learning rule is optimal in the different tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZVqsBl2HapR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="x8l2miKNqPb" data-number="1409">
        <h4>
          <a href="https://openreview.net/forum?id=x8l2miKNqPb">
              Generate Triggers  in Neural Relation Extraction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=x8l2miKNqPb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Yujiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Yujiang1">Liu Yujiang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#x8l2miKNqPb-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x8l2miKNqPb-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">relation triggersï¼Œevolutive maskï¼Œ pointer network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the relation extraction task, the relationship between two entities is determined by some specific words in their source text. These words are called relation triggers, which are the evidence to explain the relationship; other words are called ir-relevant words. The current relationship extraction neural network model aims at identifying the relation type between two entities mentioned in source text by encoding the text and entities. However, these models cannot output the relation triggers, but only gives the result of relation classification. Although models can generate weights for every single word through the improvement of attention mechanism, the weights will be affected by irrelevant words essentially, which are not required by the relation extraction task. In order to output re-lation triggers accurately, we propose a novel training frame-work for Relation Extraction (RE) that reduces the negative effect of irrelevant words on them in the encoding stage. In specific, we leverage Evolutive Mask based Point Network (EMPN) as a decoder to generate relation triggers and encode these words again. For an ordered output in relation triggers, we utilize order loss to constrain the output order in them. Ex-tensive experiment results demonstrate that the effectiveness of our proposed model achieves state-of-the-art performance on three RE benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Predict relation type and generate trigger words to make the results reasonable with Evolutive Mask based Point Network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=x8l2miKNqPb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IPwwNwMvHFW" data-number="1384">
        <h4>
          <a href="https://openreview.net/forum?id=IPwwNwMvHFW">
              Multi-Agent Decentralized Belief Propagation on Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IPwwNwMvHFW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yitao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yitao_Chen1">Yitao Chen</a>, <a href="https://openreview.net/profile?id=~Deepanshu_Vasal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepanshu_Vasal1">Deepanshu Vasal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IPwwNwMvHFW-details-428" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IPwwNwMvHFW-details-428"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">I-pomdps, Belief propagation, Multi-agent control</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the problem of interactive partially observable Markov decision processes (I-POMDPs),where the agents are located at the nodes of a communication network.  Specifically, we assume a certain message type for all messages.  Moreover, each agent makes individual decisions based on the interactive belief states, the information observed locally and the messages received from its neighbors over the network.Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors.  We propose a decentralized belief propagation algorithm for the problem,  and prove the convergence of our algorithm.Finally we show multiple applications of our framework. Our work appears to be the first study of decentralized belief propagation algorithm for networked multi-agent I-POMDPs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a methodology to do multi agent belief propagation on grahps</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PU3VGS93gxD" data-number="1202">
        <h4>
          <a href="https://openreview.net/forum?id=PU3VGS93gxD">
              Sample Complexity of Deep Active Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PU3VGS93gxD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhao_Song6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Song6">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Baocheng_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Baocheng_Sun1">Baocheng Sun</a>, <a href="https://openreview.net/profile?id=~Danyang_Zhuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danyang_Zhuo1">Danyang Zhuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PU3VGS93gxD-details-988" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PU3VGS93gxD-details-988"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning algorithms require large numbers of labeled training data to deliver state-of-the-art results. However, in many domains of AI, there are abundant unlabeled data but it is costly to get data labeled by experts, such as medical diagnosis and fraud detection. In these domains, active learning, where an algorithm maximizes model accuracy while requiring the least number of labeled data, is appealing.
        Active learning uses both labeled and unlabeled data to train models, and the learning algorithm decides which subset of data should acquire labels.
        Due to the costly label acquisition, it is interesting to know whether it is possible from a theoretical perspective to understand how many labeled data are actually needed to train a machine learning model. This question is known as the sample complexity problem, and it has been extensively explored for training linear machine learning models (e.g., linear regression). Today, deep learning has become the de facto method for machine learning, but the sample complexity problem for deep active learning remains unsolved. This problem is challenging due to the non-linear nature of neural networks.
        In this paper, we present the first deep active learning algorithm which has a provable sample complexity. Using this algorithm, we have derived the first upper bound on the number of required labeled data for training neural networks. 
        Our upper bound shows that the minimum number of labeled data a neural net needs does not depend on the data distribution or the width of the neural network but is determined by the smoothness of non-linear activation and the dimension of the input data.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove the first upper bound on the sample complexity of active learning for training one-hidden layer neural networks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PU3VGS93gxD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xO4xryFQltO" data-number="943">
        <h4>
          <a href="https://openreview.net/forum?id=xO4xryFQltO">
              A new perspective on probabilistic image modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xO4xryFQltO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Gepperth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Gepperth1">Alexander Gepperth</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xO4xryFQltO-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xO4xryFQltO-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep mixture models, sum-product networks, probabilistic circuits, image modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new probabilistic approach for image modeling capable of density estimation, sampling and tractable inference. DCGMM instances exhibit a CNN-like layered structure, in which the principal building  blocks are convolutional Gaussian Mixture (cGMM) layers. A key innovation w.r.t. related models lile sum-produdct networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer optimizes an independent loss function and therefore has an independent probabilistic interpretation. This modular approach permits intervening transformation layers to harness the full spectrum of 
        (potentially non-invertible) mappings available to CNNs, e.g., max-pooling or (half-)convolutions. DCGMM sampling and inference are realized by a deep chain of hierarchical priors, where samples generated by each cGMM layer parameterize sampling in the next-lower cGMM layer. For sampling through non-invertible transformation layers, we introduce a new gradient-based sharpening technique that exploits redundancy (overlap) in, e.g., half-convolutions. The basic quantities forward-transported through a DCGMM instance are the posterior probabilities of cGMM layers, which ensures numerical stability and facilitates the selection of learning rates.
        DCGMMs can be trained end-to-end by SGD from random initial conditions, much like CNNs. We experimentally show that DCGMMs compare favorably to several recent PC and SPN models in terms of inference, classification and sampling, the latter particularly for challenging datasets such as SVHN. A public TF2 implementation is provided as well.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A conceptually new approach for probabilistic image modeling based on multiple linked GMMs, which can generate samples of excellent quality w.r.t. related approaches, particularly for SVHN.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UajXTGRjuKB" data-number="840">
        <h4>
          <a href="https://openreview.net/forum?id=UajXTGRjuKB">
              Sampling Before Training: Rethinking the Effect of Edges in the Process of Training Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UajXTGRjuKB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hengyuan_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hengyuan_Ma1">Hengyuan Ma</a>, <a href="https://openreview.net/profile?email=xianmu.yq%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="xianmu.yq@antgroup.com">Qi Yang</a>, <a href="https://openreview.net/profile?email=wenxi.sbw%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="wenxi.sbw@antgroup.com">Bowen Sun</a>, <a href="https://openreview.net/profile?email=shunlong.wxd%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shunlong.wxd@antgroup.com">Long Shun</a>, <a href="https://openreview.net/profile?email=kui.lijk%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="kui.lijk@antgroup.com">Junkui Li</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Feng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianfeng_Feng2">Jianfeng Feng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UajXTGRjuKB-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UajXTGRjuKB-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNN) demonstrate excellent performance on many graph-based tasks; however, they also impose a heavy computational burden when trained on a large-scale graph. Although various sampling methods have been proposed to speed up training GNN by shrinking the scale of the graph during training, they become unavailable if we need to perform sampling before training. In this paper, we quantify the importance of every edge for training in the graph with the extra information they convey in addition to the node features, as inspired by a manifold learning algorithm called diffusion map. Based on this calculation, we propose Graph Diffusion Sampling (GDS), a simple but effective sampling method for shrinking the size of the edge set before training. GDS prefers to sample edges with high importance, and edges dropped by GDS will never be used in the training procedure. We empirically show that GDS preserves the edges crucial for training in a variety of models (GCN, GraphSAGE, GAT, and JKNet). Compared to training on the full graph, GDS can guarantee the performance of the model while only samples a small fraction of the edges.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UajXTGRjuKB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xREjEGUoY4c" data-number="761">
        <h4>
          <a href="https://openreview.net/forum?id=xREjEGUoY4c">
              Robot Intent Recognition Method Based on State Grid Business Office
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xREjEGUoY4c" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lanfang_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lanfang_Dong1">Lanfang Dong</a>, <a href="https://openreview.net/profile?id=~Zhao_Pu_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Pu_Hu1">Zhao Pu Hu</a>, <a href="https://openreview.net/profile?id=~Hanchao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanchao_Liu1">Hanchao Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xREjEGUoY4c-details-785" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xREjEGUoY4c-details-785"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial intelligence is currently in an era of change, not only changing the artificial intelligence technology itself, but also changing human society. It has become more and more common to use artificial intelligence as the core human-computer interaction technology to replace manpower. Intention recognition is an important part of the human-machine dialogue system, and deep learning technology is gradually being applied to the task of intent recognition. However, intent recognition based on deep learning often has problems such as low recognition accuracy and slow recognition speed. In response to these problems, this paper designs a BERT fine-tuning to improve the network structure based on the pre-training model and proposes new continuous pre-training goals. To improve the accuracy of intent recognition, a method based on multi-teacher model compression is proposed to compress the pre-training model, which reduces the time consumption of model inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gTdmGt48ht1" data-number="691">
        <h4>
          <a href="https://openreview.net/forum?id=gTdmGt48ht1">
              On the Double Descent of Random Features Models Trained with SGD
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gTdmGt48ht1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fanghui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fanghui_Liu1">Fanghui Liu</a>, <a href="https://openreview.net/profile?id=~Johan_Suykens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johan_Suykens1">Johan Suykens</a>, <a href="https://openreview.net/profile?id=~Volkan_Cevher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Volkan_Cevher1">Volkan Cevher</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gTdmGt48ht1-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gTdmGt48ht1-details-805"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random features, over-parameterized model, double descent, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study generalization properties of random features (RF) regression in high dimensions  optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds of RF regression under both constant and adaptive step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well in the interpolation setting, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm interpolator, as a theoretical justification of using SGD in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that, random features models trained with SGD in high dimensions still generalizes well for interpolation learning, recovers double descent, and incurs no loss in the excess risk when compared to the exact closed-form solution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fG9WttDhAaa" data-number="4723">
        <h4>
          <a href="https://openreview.net/forum?id=fG9WttDhAaa">
              Rethinking Positional Encoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fG9WttDhAaa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jianqiao_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianqiao_Zheng1">Jianqiao Zheng</a>, <a href="https://openreview.net/profile?id=~Sameera_Ramasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sameera_Ramasinghe1">Sameera Ramasinghe</a>, <a href="https://openreview.net/profile?email=simon.lucey%40adelaide.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="simon.lucey@adelaide.edu.au">Simon Lucey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fG9WttDhAaa-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fG9WttDhAaa-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions.  Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=fG9WttDhAaa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LZVXOnSrD0Y" data-number="4720">
        <h4>
          <a href="https://openreview.net/forum?id=LZVXOnSrD0Y">
              Pareto Frontier Approximation Network (PA-Net) Applied to Multi-objective TSP
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LZVXOnSrD0Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ishaan_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ishaan_Mehta1">Ishaan Mehta</a>, <a href="https://openreview.net/profile?email=s.saeedi%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="s.saeedi@ryerson.ca">Sajad Saeedi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LZVXOnSrD0Y-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LZVXOnSrD0Y-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotics, planning, TSP, RL, Multi Objective Optimization, Pareto Optimality</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-objective optimization is used in various areas of robotics like control, planning etc. Their solutions are dependent on multiple objective functions, which can be conflicting in nature. In such cases, the optimality is defined in terms of Pareto optimality. A set of these Pareto Optimal solutions in the objective space form a Pareto front (or frontier). Each solution has its own trade off. For instance, the travelling salesman problem (TSP) is used in robotics for task/resource allocation. Often this allocation is influenced by multiple objective functions and is solved using Multi-objective travelling salesman problem (MOTSP). In this work, we present PA-Net, a network that generates good approximations of the Pareto front for the multi-objective optimization problems. Our training framework is applicable to other multi-objective optimization problems; however, in this work, we focus on solving MOTSP.  Firstly, MOTSP is converted into a constrained optimization problem. We then train our network to solve this constrained problem using the Lagrangian relaxation and policy gradient. With PA-Net we are able to generate better quality Pareto fronts with fast inference times as compared to other learning based and classical methods. Finally, we present the application of PA-Net to find optimal visiting order in coverage planning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">PA-Net: a network that approximates Pareto Frontier for Multi Objective TSP problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LZVXOnSrD0Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aJORhCrlYqu" data-number="4718">
        <h4>
          <a href="https://openreview.net/forum?id=aJORhCrlYqu">
              ARMCMC:  Online Bayesian Density Estimation of Model Parameters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aJORhCrlYqu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedram_Agand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedram_Agand1">Pedram Agand</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?id=~Hamid_Taghirad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hamid_Taghirad1">Hamid Taghirad</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aJORhCrlYqu-details-622" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aJORhCrlYqu-details-622"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian, Probabilistic approaches, MCMC, Hunt Crossley, parameter identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although the Bayesian paradigm provides a rigorous framework to estimate the full probability distribution over unknown parameters,  its online  implementation can be challenging due to heavy computational costs. This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches. These shortcomings include: being solely able to account for Gaussian noise, being applicable to systems with linear in the parameters (LIP) constraint, or having requirements on persistence excitation (PE). In ARMCMC, we propose a variable jump distribution, which depends on a temporal forgetting factor.  This allows one to adjust the trade-off between exploitation and exploration, depending on whether there is an abrupt change to the parameter being estimated. We prove that ARMCMC requires fewer samples to achieve the same precision and reliability compared to conventional MCMC approaches.  We demonstrate our approach on two challenging benchmarks:  the estimation of parameters in a soft bending actuator and the Hunt-Crossley dynamic model. Our method shows at-least 70\% improvement in parameter point estimation accuracy and approximately 55\% reduction in tracking error of the value of interest compared to recursive least squares and conventional MCMC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=aJORhCrlYqu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Xd6T7cT7vwj" data-number="4673">
        <h4>
          <a href="https://openreview.net/forum?id=Xd6T7cT7vwj">
              Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Xd6T7cT7vwj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marcus_L%C3%A5ng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marcus_LÃ¥ng1">Marcus LÃ¥ng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Xd6T7cT7vwj-details-839" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xd6T7cT7vwj-details-839"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent studies have show that wide neural networks with orthogonal linear layers and Gaussian PoincarÃ© normalized activation functions avoid vanishing and exploding gradients for input vectors with the correct magnitude. This paper introduces a strengthening of the condition that the activation function must be Gaussian PoincarÃ© normalized which creates robustness to deviations from standard normal distribution in the pre-activations, thereby reducing the dependence on the requirement that the network is wide and that the input vector has the correct magnitude. In implicit representation learning this allows the training of deep networks of this type where the linear layers are no longer constrained to be orthogonal linear transformations. Networks of this type can be fitted to a reference image to 1/10th the mean square error achievable with previous methods. Herein is also given an improved positional encoding for implicit representation learning of two-dimensional images and a small-batch training procedure for fitting of neural networks to images which allows fitting in fewer epochs, leading to substantial improvement in training time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Similar to SIREN, but able to fit images to higher accuracy (PSNR=67 instead PSNR=50 for a specific reference image).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sHUFhv03qX_" data-number="4671">
        <h4>
          <a href="https://openreview.net/forum?id=sHUFhv03qX_">
              Q-Learning Scheduler for Multi-Task Learning through the use of Histogram of Task Uncertainty
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sHUFhv03qX_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kourosh_Meshgi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Meshgi2">Kourosh Meshgi</a>, <a href="https://openreview.net/profile?id=~Maryam_Sadat_Mirzaei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maryam_Sadat_Mirzaei1">Maryam Sadat Mirzaei</a>, <a href="https://openreview.net/profile?id=~Satoshi_Sekine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satoshi_Sekine1">Satoshi Sekine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sHUFhv03qX_-details-63" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sHUFhv03qX_-details-63"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Q-learning, Multi-Task Learning, MTL Scheduling, Histogram of Task Uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Simultaneous training of a multi-task learning network on different domains or tasks is not always straightforward. It could lead to inferior performance or generalization compared to the corresponding single-task networks. To maximally taking advantage of the benefits of multi-task learning, an effective training scheduling method is deemed necessary. Traditional schedulers follow a heuristic or prefixed strategy, ignoring the relation of the tasks, their sample complexities, and the state of the emergent shared features. We proposed a deep Q-Learning Scheduler (QLS) that monitors the state of the tasks and the shared features using a novel histogram of task uncertainty, and through trial-and-error, learns an optimal policy for task scheduling. Extensive experiments on multi-domain and multi-task settings with various task difficulty profiles have been conducted, the proposed method is benchmarked against other schedulers, its superior performance has been demonstrated, and results are discussed.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A deep Q-learning-based task scheduling method to improve multi-tasking learning based on a novel histogram of task uncertainty.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lTiW8Jet8t" data-number="4664">
        <h4>
          <a href="https://openreview.net/forum?id=lTiW8Jet8t">
              Efficient Ensembles of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lTiW8Jet8t" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Amrit_Nagarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amrit_Nagarajan1">Amrit Nagarajan</a>, <a href="https://openreview.net/profile?id=~Jacob_R._Stevens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_R._Stevens1">Jacob R. Stevens</a>, <a href="https://openreview.net/profile?id=~Anand_Raghunathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anand_Raghunathan1">Anand Raghunathan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lTiW8Jet8t-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lTiW8Jet8t-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) have enabled the power of deep learning to be applied to inputs beyond the Euclidean domain, with applications ranging from social networks and product recommendation engines to the life sciences. GNNs, like other classes of machine learning models, benefit from ensemble learning, wherein multiple models are combined to provide higher accuracy and robustness than single models. However, ensembles suffer from significantly higher inference processing and storage requirements, limiting their use in practical applications. In this work, we leverage the unique characteristics of GNNs to overcome these overheads, creating efficient ensemble GNNs that are faster than even single models at inference time. We observe that during message passing, nodes that are incorrectly classified (error nodes) also end up adversely affecting the representations of other nodes in their neighborhood. This error propagation also makes GNNs more difficult to approximate (e.g., through pruning) for efficient inference. We propose a technique to create ensembles of diverse models, and further propose Error Node Isolation (ENI), which prevents error nodes from sending messages to (and thereby influencing) other nodes. In addition to improving accuracy, ENI also leads to a significant reduction in the memory footprint and the number of arithmetic operations required to evaluate the computational graphs of all neighbors of error nodes. Remarkably, these savings outweigh even the overheads of using multiple models in the ensemble. A second key benefit of ENI is that it  enhances the resilience of GNNs to approximations. Consequently, we propose Edge Pruning and Network Pruning techniques that target both the input graph and the neural networks used to process the graph. Our experiments on GNNs for transductive and inductive node classification demonstrate that ensembles with ENI are simultaneously more accurate (by up to 4.6% and 3.8%) and  faster (by up to 2.8<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="113" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> and 5.7<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="114" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container>) when compared to the best-performing single models and ensembles without ENI, respectively. In addition, GNN ensembles with ENI are consistently more accurate than single models and ensembles without ENI when subject to pruning, leading to additional speedups of up to 5<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="115" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>Ã—</mo></math></mjx-assistive-mml></mjx-container> with no loss in accuracy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ndffz5uo6H" data-number="4656">
        <h4>
          <a href="https://openreview.net/forum?id=Ndffz5uo6H">
              Updater-Extractor Architecture for Inductive World State Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ndffz5uo6H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arsenii_Kirillovich_Moskvichev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arsenii_Kirillovich_Moskvichev1">Arsenii Kirillovich Moskvichev</a>, <a href="https://openreview.net/profile?id=~James_A_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_A_Liu1">James A Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ndffz5uo6H-details-122" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ndffz5uo6H-details-122"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformers, long-term-memory, sequential processing, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Developing sequential models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure. Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Proposing a theoretically and practically justified way to introduce persistent world state representations into transformer architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ndffz5uo6H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ys-bh0Eer_" data-number="4654">
        <h4>
          <a href="https://openreview.net/forum?id=ys-bh0Eer_">
              Block Contextual MDPs for Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ys-bh0Eer_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Franziska_Meier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Franziska_Meier2">Franziska Meier</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joelle_Pineau1">Joelle Pineau</a>, <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amy_Zhang1">Amy Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ys-bh0Eer_-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ys-bh0Eer_-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, MDP, Block Contextual MDP, Continual Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In reinforcement learning (RL), when defining a Markov Decision Process (MDP), the environment dynamics is implicitly assumed to be stationary. This assumption of stationarity, while simplifying, can be unrealistic in many scenarios. In the continual reinforcement learning scenario, the sequence of tasks is another source of nonstationarity. In this work, we propose to examine this continual reinforcement learning setting through the block contextual MDP (BC-MDP) framework, which enables us to relax the assumption of stationarity. This framework challenges RL algorithms to handle both nonstationarity and rich observation settings and, by additionally leveraging smoothness properties, enables us to study generalization bounds for this setting. Finally, we take inspiration from adaptive control to propose a novel algorithm that addresses the challenges introduced by this more realistic BC-MDP setting, allows for zero-shot adaptation at evaluation time, and achieves strong performance on several nonstationary environments.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce the Lipschitz Block Contextual MDP framework for the continual RL setting and propose a representation learning algorithm that enables RL agents to generalize to non-stationary environments.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FeaitX_a5Av" data-number="4645">
        <h4>
          <a href="https://openreview.net/forum?id=FeaitX_a5Av">
              GSD: Generalized Stochastic Decoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FeaitX_a5Av" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ning_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ning_Gong1">Ning Gong</a>, <a href="https://openreview.net/profile?id=~Nianmin_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nianmin_Yao1">Nianmin Yao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FeaitX_a5Av-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FeaitX_a5Av-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Processing, Decoding Algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although substantial progress has been made in various text generation tasks, there remains a vast gap between current generations and human languages. One reason is that virtually all decoding methods currently developed are pragmatic to address the text degeneration problem, which exists in both deterministic and stochastic decoding algorithms. So, why text generated from these algorithms are divergent? What is the critical difference between these algorithms? Moreover, is it possible to design a generalized framework where existing decoding algorithms can be naturally connected, uniformly described, and mutually inspired?
        In this paper, we try to explore answers to these intriguing questions. Correctly, we propose a generalized decoding framework that can be used to describe and connect existing popular decoding algorithms. Based on the framework, we propose a novel implementation with a distinctive core from existing decoding algorithms. As far as we know, this is the first work trying to propose a generalized framework to bridge these decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms. Experiments show that text produced by our method is closest to the characteristics of human languages. Source code and the generated text can be accessed from https://github.com/ginoailab/gsd.git.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel work proposing a generalized framework to connect existing decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JgmY4TUgznC" data-number="4641">
        <h4>
          <a href="https://openreview.net/forum?id=JgmY4TUgznC">
              Few-Shot Multi-task Learning via Implicit regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JgmY4TUgznC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsung_Huh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongsung_Huh1">Dongsung Huh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JgmY4TUgznC-details-11" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JgmY4TUgznC-details-11"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Few Shot Learning, Learning Instability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern machine learning is highly data-intensive. Few-shot learning (FSL) aims to resolve this sample efficiency problem by learning from multiple tasks and quickly adapt to new tasks containing only a few samples. However,  FSL problems proves to be significantly more challenging and require more compute expensive process to optimize. In this work, we consider multi-task linear regression (MTLR) as a canonical problem for few-shot learning, and investigate the source of challenge of FSL. We find that the MTLR exhibits local minimum problems that are not present in single-task problem, and thus making the learning much more challenging. We also show that the problem can be resolved by  overparameterizing the  model by increasing both the width and depth of the linear network and initializing the weights with small values, exploiting the implicit regularization bias of gradient descent-based learning.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UzOEYQM-xTg" data-number="4635">
        <h4>
          <a href="https://openreview.net/forum?id=UzOEYQM-xTg">
              Robust Long-Tailed Learning under Label Noise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UzOEYQM-xTg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tong_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tong_Wei1">Tong Wei</a>, <a href="https://openreview.net/profile?id=~Jiang-Xin_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiang-Xin_Shi1">Jiang-Xin Shi</a>, <a href="https://openreview.net/profile?id=~Wei-Wei_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei-Wei_Tu1">Wei-Wei Tu</a>, <a href="https://openreview.net/profile?id=~Yu-Feng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Feng_Li1">Yu-Feng Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UzOEYQM-xTg-details-779" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UzOEYQM-xTg-details-779"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weakly-supervised learning, long-tailed learning, learning with noisy labels, semi-supervised learning, multi-label learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework,~\algo, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on both benchmark and real-world datasets demonstrate substantial improvement over many existing methods. For example, \algo\ outperforms baselines by more than 5\% in test accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UzOEYQM-xTg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a3NaSCJ20V" data-number="4634">
        <h4>
          <a href="https://openreview.net/forum?id=a3NaSCJ20V">
              Equivariant Grasp learning In Real Time
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a3NaSCJ20V" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xupeng_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xupeng_Zhu1">Xupeng Zhu</a>, <a href="https://openreview.net/profile?id=~Dian_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dian_Wang1">Dian Wang</a>, <a href="https://openreview.net/profile?id=~Ondrej_Biza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ondrej_Biza1">Ondrej Biza</a>, <a href="https://openreview.net/profile?id=~Robert_Platt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_Platt1">Robert Platt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a3NaSCJ20V-details-55" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a3NaSCJ20V-details-55"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotic Grasping, Equivariance, Reinforcement Leanring</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual grasp detection is a key problem in robotics where the agent must learn to model the grasp function, a mapping from an image of a scene onto a set of feasible grasp poses. In this paper, we recognize that the grasp function is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="116" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">SE</mi></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>-equivariant and that it can be modeled using an equivariant convolutional neural network. As a result, we are able to significantly improve the sample efficiency of grasp learning to the point where we can learn a good approximation of the grasp function within only 500 grasp experiences. This is fast enough that we can learn to grasp completely on a physical robot in about an hour. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="I13PP8-cdvz" data-number="4632">
        <h4>
          <a href="https://openreview.net/forum?id=I13PP8-cdvz">
              SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=I13PP8-cdvz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheng_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheng_Cheng1">Sheng Cheng</a>, <a href="https://openreview.net/profile?id=~Yi_Ren3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Ren3">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yezhou_Yang1">Yezhou Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#I13PP8-cdvz-details-859" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I13PP8-cdvz-details-859"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stroke-based representation, Spatial robustness, Robust feature learning, Novel pattern generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing end-to-end visual recognition models do not possess innate spatial invariance and are thus vulnerable to out-of-training attacks. This suggests the need of a better representation design. This paper follows existing cognitive studies to investigate a sketch representation that specify stroke information on vertices and inter-stroke information on edges. The resultant representation, combined with a graph neural network, achieves both high classification accuracy and high robustness against translation, rotation, and stroke-wise parametric and topological attacks thanks to the use of spatially invariant stroke features and GNN architecture. While prior studies demonstrated similar sketch representations for classification and generation, these attempts heavily relied on run-time statistical inference rather than more efficient bottom-up computation via GNN. The presented sketch representation poses good structured expression capability as it enables generation of sketches semantically different from the training dataset.  Lastly, we show SSR-GNNs are able to accomplish all tasks (classification, robust feature learning, and novel pattern generation), which shows that the representation is task-agnostic. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper presents a Stroke-based Sketch Representation with Graph Neural Networks which is spatially robust, with structured expression capability and is task-agnostic.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RSd79AULOu" data-number="4625">
        <h4>
          <a href="https://openreview.net/forum?id=RSd79AULOu">
              Fairness-aware Federated Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RSd79AULOu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhuozhuo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuozhuo_Tu1">Zhuozhuo Tu</a>, <a href="https://openreview.net/profile?id=~zhiqiang_xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~zhiqiang_xu1">zhiqiang xu</a>, <a href="https://openreview.net/profile?id=~Tairan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tairan_Huang1">Tairan Huang</a>, <a href="https://openreview.net/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dacheng_Tao1">Dacheng Tao</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RSd79AULOu-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RSd79AULOu-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Learning Theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized. In such a setting, naively minimizing an aggregate loss may introduce bias and disadvantage model performance on certain clients. To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set. By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable. We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees. We further prove generalization bounds for learning with this objective. Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new framework to address the fairness issues in federated learning and provide theoretical guarantees.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="edqz84cQ79T" data-number="4622">
        <h4>
          <a href="https://openreview.net/forum?id=edqz84cQ79T">
              Shaping latent representations using Self-Organizing Maps with Relevance Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=edqz84cQ79T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedro_Braga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedro_Braga1">Pedro Braga</a>, <a href="https://openreview.net/profile?email=hrm%40cin.ufpe.br" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="hrm@cin.ufpe.br">Heitor Medeiros</a>, <a href="https://openreview.net/profile?id=~Hansenclever_Bassani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hansenclever_Bassani1">Hansenclever Bassani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#edqz84cQ79T-details-367" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="edqz84cQ79T-details-367"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Clustering, Learning Prototypes, Topological Representations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work indicates that Deep Clustering (DC) methods are a viable option for unsupervised representations learning of visual features. By combining representation learning and clustering, traditional approaches have been shown to build latent representations that capture essential features of the data while preserving topological characteristics. In this sense, models based on Self-Organizing Maps models with relevance learning (SOMRL) were considered as they perform well in clustering besides being able to create a map that learns the relevance of each input dimension for each cluster, preserving the original relations and topology of the data. We hypothesize that this type of model can produce a more intuitive and disentangled representation in the latent space by promoting smoother transitions between cluster points over time. This work proposes a representation learning framework that combines a new gradient-based SOMRL model and autoencoders. The SOMRL learns the relevance weights for each input dimension of each cluster. It creates a tendency to separate the information into subspaces. To achieve this, we designed a new loss function term that weighs these learned relevances and provides an estimated unsupervised error to be used in combination with a reconstruction loss. The model is evaluated in terms of clustering performance and quality of the learned representations and then compared with start-of-the-art models, showing competitive results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work proposes a representation learning framework that combines a new Self-Organizing Maps with autoencoders to shape their latent spaces into cluster prototypes living in separate subspaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=edqz84cQ79T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eAEcdRkcMHh" data-number="4611">
        <h4>
          <a href="https://openreview.net/forum?id=eAEcdRkcMHh">
              HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N:M Sparsity and Integer Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eAEcdRkcMHh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minjia_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minjia_Zhang1">Minjia Zhang</a>, <a href="https://openreview.net/profile?id=~Connor_Holmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Connor_Holmes1">Connor Holmes</a>, <a href="https://openreview.net/profile?id=~Yuxiong_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuxiong_He1">Yuxiong He</a>, <a href="https://openreview.net/profile?id=~Bo_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Wu1">Bo Wu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eAEcdRkcMHh-details-651" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eAEcdRkcMHh-details-651"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Efficient Inference, N:M Sparsification, Quantization, Transformer networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks. However, the huge size of these models brings significant challenges to fine-tuning and online deployment due to latency and cost constraints. Recently, hardware manufacturers have released new architectures that support efficient N:M sparsity and low-precision integer computation for fast inferencing. In contrast to unstructured sparsity, N:M sparsity specifies that out of each chunk of N contiguous weight parameters, exactly M parameters are non-zero. Moreover, these architectures also support processing data with reduced precision, such as INT8. Prior work often considers inducing N:M sparsity and integer quantization in isolation or as independent pieces of a compression pipeline. However, there lacks a systematic investigation towards how N:M sparsity and integer quantization can be effectively combined to exploit the maximum degree of redundancy and enable even faster acceleration for pre-trained Transformer networks.
        
        In this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM). We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and
        solved in a unified manner. When evaluated across the GLUE suite of NLP benchmarks, our approach outperforms baselines that consider each of these problems independently, retaining 99.4\% accuracy of the dense baseline while being able to execute on newly released hardware effectively. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">HoloFormer is a unified and systematic approach to learn N:M sparsity and integer quantization for compressing pre-trained Transformer networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ad_F_z27pCx" data-number="4595">
        <h4>
          <a href="https://openreview.net/forum?id=ad_F_z27pCx">
              A Discussion On the Validity of Manifold Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ad_F_z27pCx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dai_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dai_Shi1">Dai Shi</a>, <a href="https://openreview.net/profile?id=~Andi_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andi_Han1">Andi Han</a>, <a href="https://openreview.net/profile?id=~Yi_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Guo3">Yi Guo</a>, <a href="https://openreview.net/profile?id=~Junbin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junbin_Gao1">Junbin Gao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 30 Sept 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ad_F_z27pCx-details-132" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ad_F_z27pCx-details-132"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Manifold learning, Dimensionality Reduction, Computational Geometry, Simplicial Complex</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition (orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any -simplex decomposition result on a -manifold.  However,  constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SK1nec-Ehd" data-number="4584">
        <h4>
          <a href="https://openreview.net/forum?id=SK1nec-Ehd">
              PulseImpute: A Novel Benchmark Task and Architecture for Imputation of Physiological Signals
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SK1nec-Ehd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Maxwell_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maxwell_Xu1">Maxwell Xu</a>, <a href="https://openreview.net/profile?id=~Alexander_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Moreno1">Alexander Moreno</a>, <a href="https://openreview.net/profile?id=~James_Matthew_Rehg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Matthew_Rehg1">James Matthew Rehg</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SK1nec-Ehd-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SK1nec-Ehd-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">missingness, imputation, mHealth, sensors, transformer, self-attention</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Providing care for patients with chronic diseases is one of the biggest drivers of the nationâ€™s rising healthcare costs, but many of these diseases are linked to mutable health behaviors. Mobile health (mHealth) biophysical sensors that continuously measure our current conditions provide the framework for a personalized guidance system for the maintenance of healthy behaviors. However, this physiological sensor data is plagued with missingness due to insecure attachments, wireless dropout, battery, and adherence issues. These issues cripple their rich diagnostic utility as well as their ability to enable temporally-precise interventions. While there is a sizable amount of research focusing on imputation methods, surprisingly, no works have addressed the patterns of missingness, quasi-periodic signal structure, and the between subject heterogeneity that characterizes physiological signals in mHealth applications. We present the PulseImpute Challenge, the first challenge dataset for physiological signal imputation which includes a large set of baselines' performances on realistic missingness models and data. Next, we demonstrate the potential to address this quasi-periodic structure and heterogeneity with our Dilated Convolution Bottleneck (DCB) Transformer, a transformer architecture with a self-attention mechanism that is able to attend to corresponding waveform features in quasi-periodic signals. By utilizing stacked dilated convolutions with bottleneck layers for query and key transformations, we visually demonstrate that the kernel similarity in the attention model gives high similarity to similar temporal features across quasi-periodic periods. We hope the release of our challenge task definitions and baseline implementations will spur the community to address this challenging and important problem. 
         </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present PulseImpute, a benchmarking challenge for the imputation of biophysical signals, and propose a novel self-attention module for attending over quasi-periodic signals.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dhLChxJwgMR" data-number="4578">
        <h4>
          <a href="https://openreview.net/forum?id=dhLChxJwgMR">
              HFSP: A Hardware-friendly Soft Pruning Framework for Vision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dhLChxJwgMR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhenglun_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhenglun_Kong1">Zhenglun Kong</a>, <a href="https://openreview.net/profile?id=~Peiyan_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peiyan_Dong1">Peiyan Dong</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaolong_Ma2">Xiaolong Ma</a>, <a href="https://openreview.net/profile?id=~Xin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Meng1">Xin Meng</a>, <a href="https://openreview.net/profile?id=~Mengshu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengshu_Sun1">Mengshu Sun</a>, <a href="https://openreview.net/profile?id=~Wei_Niu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Niu3">Wei Niu</a>, <a href="https://openreview.net/profile?id=~Bin_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Ren1">Bin Ren</a>, <a href="https://openreview.net/profile?id=~Minghai_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghai_Qin1">Minghai Qin</a>, <a href="https://openreview.net/profile?id=~Hao_Tang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Tang6">Hao Tang</a>, <a href="https://openreview.net/profile?id=~Yanzhi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanzhi_Wang3">Yanzhi Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dhLChxJwgMR-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dhLChxJwgMR-details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Hardware-friendly, Soft Token Pruning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViT, and the related edge device deployment, we leverage the input token sparsity and propose a hardware-friendly soft pruning framework (HFSP), which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique to package the pruned tokens, which integrate the less informative tokens generated by the selector module into a package token, and participates in subsequent calculations rather than being discarded completely.  From a hardware standpoint, our framework is bound to the tradeoff between accuracy and specific hardware constraints through our proposed hardware-oriented progressive training, and all the operators embedded in the framework have been well-supported. Experimental results demonstrate that the proposed framework significantly reduces the computational costs of ViTs while maintaining comparable performance on image classification. For example, our method reduces the FLOPs of DeiT-S by over 42.6% while only sacrificing 0.46% top-1 accuracy. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. Code will be publicly released.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A vision transformer pruning framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dhLChxJwgMR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="znpOLJUYGcA" data-number="4568">
        <h4>
          <a href="https://openreview.net/forum?id=znpOLJUYGcA">
              Automatic Integration for Neural Temporal Point Process
          </a>
        
          
            <a href="https://openreview.net/pdf?id=znpOLJUYGcA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zihao_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zihao_Zhou1">Zihao Zhou</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rose_Yu1">Rose Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#znpOLJUYGcA-details-34" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="znpOLJUYGcA-details-34"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point process</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integration lies at the heart of the temporal point process. Due to the intrinsic mathematical difficulty of symbolic integration, neural temporal point process models either constrain the intensity function to an integrable functional form or apply certain numerical methods. However, the former type of model has limited expressive power, and the latter type of model suffers additional numerical errors and high computational costs. In this paper, we introduce automatic integration with neural point process models, a new paradigm for efficient, closed-form nonparametric inference of temporal point process characterized by any intensity function. We test the model against a variety of synthetic temporal point process datasets and show that the model can better capture inter-event intensity changes than state-of-the-art methods. We also identify certain model settings that would lead the MLE estimator for the temporal point process to be inconsistent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Leveraging automatic integration for more efficient and accurate recovery of temporal point process's intensity</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZV7MoEj44Et" data-number="4556">
        <h4>
          <a href="https://openreview.net/forum?id=ZV7MoEj44Et">
              Measuring the Effectiveness of Self-Supervised Learning using Calibrated Learning Curves
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZV7MoEj44Et" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrei_Atanov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrei_Atanov1">Andrei Atanov</a>, <a href="https://openreview.net/profile?id=~Shijian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shijian_Xu1">Shijian Xu</a>, <a href="https://openreview.net/profile?email=onur.beker%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="onur.beker@epfl.ch">Onur Beker</a>, <a href="https://openreview.net/profile?id=~Andrey_Filatov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrey_Filatov1">Andrey Filatov</a>, <a href="https://openreview.net/profile?id=~Amir_Zamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Zamir1">Amir Zamir</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZV7MoEj44Et-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZV7MoEj44Et-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Self-Supervised Learning, Transfer Learning, Metric, Evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning has witnessed remarkable progress in recent years, in particular with the introduction of augmentation-based contrastive methods. While a number of large-scale empirical studies on the performance of self-supervised pre-training have been conducted, there isn't yet an agreed upon set of control baselines, evaluation practices, and metrics to report. We identify this as an important angle of investigation and propose an evaluation standard that aims to quantify and communicate transfer learning performance in an informative yet accessible setup. This is done by baking in a number of key control baselines in the evaluation method, particularly the blind guess (quantifying the dataset bias), the scratch model (quantifying the architectural contribution), and the gold standard (quantifying the upper-bound). We further provide a number of experiments to demonstrate how the proposed evaluation can be employed in empirical studies of basic questions -- for example, whether the effectiveness of existing self-supervised learning methods is skewed towards image classification versus other tasks, such as dense pixel-wise predictions. 
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an evaluation standard for measuring the effectiveness of self-supervised learning based on incorporating important control baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxTR9Z2DW8R" data-number="4552">
        <h4>
          <a href="https://openreview.net/forum?id=UxTR9Z2DW8R">
              Reinforcement Learning State Estimation for High-Dimensional Nonlinear Systems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxTR9Z2DW8R" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Saviz_Mowlavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saviz_Mowlavi1">Saviz Mowlavi</a>, <a href="https://openreview.net/profile?id=~Mouhacine_Benosman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mouhacine_Benosman1">Mouhacine Benosman</a>, <a href="https://openreview.net/profile?id=~Saleh_Nabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saleh_Nabi1">Saleh Nabi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxTR9Z2DW8R-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxTR9Z2DW8R-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, partial differential equation, reduced order modeling, closure models, state prediction, state estimation, dynamic mode decomposition.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RL-ROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="V2WidtMGSRG" data-number="4516">
        <h4>
          <a href="https://openreview.net/forum?id=V2WidtMGSRG">
              Provable Identifiability of ReLU Neural Networks via Lasso Regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=V2WidtMGSRG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gen_Li2">Gen Li</a>, <a href="https://openreview.net/profile?id=~Ganghua_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ganghua_Wang1">Ganghua Wang</a>, <a href="https://openreview.net/profile?id=~Yuantao_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuantao_Gu1">Yuantao Gu</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jie_Ding2">Jie Ding</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#V2WidtMGSRG-details-268" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V2WidtMGSRG-details-268"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Lasso, nonlinear regression, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">LASSO regularization is a popular regression tool to enhance the prediction accuracy of statistical models by performing variable selection through the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="117" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>â„“</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> penalty, initially formulated for the linear model and its variants. In this paper, the territory of LASSO is extended to the neural network model, a fashionable and powerful nonlinear regression model. Specifically, given a neural network whose output <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="118" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container> depends only on a small subset of input <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="119" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-b mjx-i"><mjx-c class="mjx-c1D499 TEX-BI"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math></mjx-assistive-mml></mjx-container>, denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="120" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>â‹†</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, we prove that the LASSO estimator can stably reconstruct the neural network and identify <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="121" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>â‹†</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> when the number of samples scales logarithmically with the input dimension. This challenging regime has been well understood for linear models while barely studied for neural networks. Our theory lies in an extended Restricted Isometry Property (RIP)-based analysis framework for two-layer ReLU neural networks, which may be of independent interest to other LASSO or neural network settings. Based on the result, we further propose a neural network-based variable selection method. Experiments on simulated and real-world datasets show the promising performance of our variable selection approach compared with classical techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We theoretically show that the Lasso estimator can stably identify ReLU neural networks and then propose to use neural networks as vehicles to perform variable selection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=V2WidtMGSRG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Jh9VxCkrEZn" data-number="4515">
        <h4>
          <a href="https://openreview.net/forum?id=Jh9VxCkrEZn">
              Spatiotemporal Representation Learning on Time Series with Dynamic Graph ODEs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Jh9VxCkrEZn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_spotlight_3_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ming_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming_Jin3">Ming Jin</a>, <a href="https://openreview.net/profile?id=~Yuan-Fang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan-Fang_Li1">Yuan-Fang Li</a>, <a href="https://openreview.net/profile?id=~Yu_Zheng5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zheng5">Yu Zheng</a>, <a href="https://openreview.net/profile?id=~Bin_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Yang4">Bin Yang</a>, <a href="https://openreview.net/profile?id=~Shirui_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shirui_Pan1">Shirui Pan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Jh9VxCkrEZn-details-327" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jh9VxCkrEZn-details-327"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Spatiotemporal representation learning on multivariate time series has received tremendous attention in forecasting traffic and energy data. Recent works either rely on complicated discrete neural architectures or graph priors, hindering their effectiveness and applications in the real world. In this paper, inspired by neural ordinary differential equations and graph structure learning, we propose a fully continuous model named Dynamic Graph ODE (DyG-ODE) to capture both long-range spatial and temporal dependencies to learn expressive representations on arbitrary multivariate time series data without being restricted by rigid preconditions (e.g., graph priors). For modeling the continuous dynamics of spatiotemporal clues, we design a simple yet powerful dynamic graph ODE by coupling the proposed spatial and temporal ODEs, which not only allows the model to obtain infinite spatial and temporal receptive fields but also reduces numerical errors and model complexity significantly. Our empirical evaluations demonstrate the superior effectiveness and efficiency of DyG-ODE on a number of benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a fully continuous model named DyG-ODE to learn expressive spatiotemporal representations on arbitrary multivariate time series data</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>Â«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>â€¹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">â€º</a>
      </li>
      <li class="  right-arrow" data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">Â»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="recent-activity">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="https://openreview.net/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review, with legal nonprofit status through<!-- --> <a href="https://codeforscience.org/" target="_blank" rel="noopener noreferrer">Code for Science &amp; Society</a>. We gratefully acknowledge the support of the<!-- --> <a href="https://openreview.net/sponsors" target="_blank">OpenReview Sponsors</a>.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible. To submit a bug report or feature request, you can use the official OpenReview GitHub repository:<br><a href="https://github.com/openreview/openreview/issues/new/choose" target="_blank" rel="noreferrer">Report an issue</a></p><form><div class="form-group"><input type="email" id="feedback-from" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" id="feedback-subject" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea id="feedback-message" name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2022/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2022/Conference\n$(function() {\nvar args = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar group = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar document = null;\nvar window = null;\n\n// TODO: remove these vars when all old webfields have been archived\nvar model = {\n  tokenPayload: function() {\n    return { user: user }\n  }\n};\nvar controller = {\n  get: Webfield.get,\n  addHandler: function(name, funcMap) {\n    Object.values(funcMap).forEach(function(func) {\n      func();\n    });\n  },\n};\n\n$('#group-container').empty();\n// START GROUP CODE\n// ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar CONFERENCE_ID = 'ICLR.cc/2022/Conference';\nvar PARENT_GROUP_ID = 'ICLR.cc/2022';\nvar SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';\nvar REVIEWERS_NAME = 'Reviewers';\nvar AREA_CHAIRS_NAME = 'Area_Chairs';\nvar AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';\nvar REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';\nvar PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';\nvar AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';\nvar HEADER = {\"title\": \"The Tenth International Conference on Learning Representations \", \"subtitle\": \"ICLR 2022\", \"location\": \"Virtual\", \"date\": \"Apr 25 2022\", \"website\": \"https://iclr.cc/Conferences/2022\", \"instructions\": \"\", \"deadline\": \"Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0\", \"contact\": \"iclr2022pc@gmail.com\"};\nvar PUBLIC = true;\n\nvar WILDCARD_INVITATION = CONFERENCE_ID + '/.*';\nvar BUFFER = 0;  // deprecated\nvar PAGE_SIZE = 50;\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true,\n  showTags: false\n};\nvar commentDisplayOptions = {\n  pdfLink: false,\n  replyCount: true,\n  showContents: false,\n  showParent: true\n};\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderSubmissionButton();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\n// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/\nfunction load() {\n\n  var spotlightNotesP = $.Deferred().resolve([]);\n  var oralNotesP = $.Deferred().resolve([]);\n  var posterNotesP = $.Deferred().resolve([]);\n  \n  \n  var activityNotesP = $.Deferred().resolve([]);\n  var authorNotesP = $.Deferred().resolve([]);\n  var userGroupsP = $.Deferred().resolve([]);\n  var withdrawnNotesP = $.Deferred().resolve([]);\n  var deskRejectedNotesP = $.Deferred().resolve([]);\n  var submittedNotesP = $.Deferred().resolve([]);\n\n  if (PUBLIC) {\n      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      //   pageSize: PAGE_SIZE,\n      //   details: 'replyCount',\n      //   includeCount: true\n      // });\n\n    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Spotlight',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Oral',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Poster',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Submitted',\n      details: 'replyCount',\n      includeCount: true\n    });\n\n    if (WITHDRAWN_SUBMISSION_ID) {\n      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount',\n        includeCount: true\n      });\n    }\n\n    if (DESK_REJECTED_SUBMISSION_ID) {\n      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount,invitation,original',\n        includeCount: true\n      });\n    }\n  }\n\n  if (user \u0026\u0026 !_.startsWith(user.id, 'guest_')) {\n    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {\n      pageSize: PAGE_SIZE,\n      details: 'forumContent,invitation,writable'\n    });\n\n    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });\n\n    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.authorids': user.profile.id\n    });\n  }\n\n  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);\n}\n\n// Render functions\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction renderSubmissionButton() {\n  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})\n    .then(function(invitation) {\n      Webfield.ui.submissionButton(invitation, user, {\n        onNoteCreated: function() {\n          // Callback funtion to be run when a paper has successfully been submitted (required)\n          if (PUBLIC) {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'A list of all submissions will be available after the deadline.');\n          } else {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'The author console page for managing your submissions will be available soon.');\n          }\n\n          load().then(renderContent).then(function() {\n            $('.tabs-container a[href=\"#your-consoles\"]').click();\n          });\n        }\n      });\n    });\n}\n\nfunction renderConferenceTabs() {\n  var sections = [\n    {\n      heading: 'Your Consoles',\n      id: 'your-consoles',\n    }\n  ];\n\n  if (PUBLIC) {\n    // sections.push({\n    //   heading: 'All Submissions',\n    //   id: 'all-submissions',\n    // });\n    sections.push({\n      heading: 'Oral Presentations',\n      id: 'oral-submissions',\n    });\n    sections.push({\n      heading: 'Spotlight Presentations',\n      id: 'spotlight-submissions',\n    });\n    sections.push({\n      heading: 'Poster Presentations',\n      id: 'poster-submissions',\n    });\n    sections.push({\n      heading: 'Rejected Submissions',\n      id: 'submitted-submissions',\n    });\n    // if (WITHDRAWN_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Withdrawn Submissions',\n    //     id: 'withdrawn-submissions',\n    //   })\n    // }\n    // if (DESK_REJECTED_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Desk Rejected Submissions',\n    //     id: 'desk-rejected-submissions',\n    //   })\n    // }\n    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {\n      sections.push({\n        heading: 'Desk Rejected/Withdrawn Submissions',\n        id: 'desk-rejected-withdrawn-submissions',\n      })\n    }\n  }\n\n  sections.push({\n    heading: 'Recent Activity',\n    id: 'recent-activity',\n  }\n)\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortBy(_.uniq(allGroups));\n  var links = [];\n  uniqueGroups.forEach(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n    links.push(\n      [\n        '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n        '\u003c/li\u003e'\n      ].join('')\n    );\n  });\n\n  $('#your-consoles .submissions-list').append(links);\n}\n\nfunction renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {\n\n  // Your Consoles tab\n  if (userGroups.length || authorNotes.length) {\n\n    var $container = $('#your-consoles').empty();\n    $container.append('\u003cul class=\"list-unstyled submissions-list\"\u003e');\n\n    var allConsoles = [];\n    if (authorNotes.length) {\n      allConsoles.push(AUTHORS_ID);\n    }\n    userGroups.forEach(function(group) {\n      allConsoles.push(group.id);\n    });\n\n    // Render all console links for the user\n    createConsoleLinks(allConsoles);\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n\n  // Oral Papers tab\n  var oralNotesCount = oralNotes.count || 0;\n  oralNotes = oralNotes.notes || [];\n\n  $('#oral-submissions').empty();\n\n  if (oralNotesCount) {\n    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#oral-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(oralNotes, {\n      heading: null,\n      container: '#oral-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Oral',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);\n          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: oralNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Oral',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#oral-submissions\"]').parent().hide();\n  }\n\n  // Spotlight Papers tab\n  var spotlightNotesCount = spotlightNotes.count || 0;\n  spotlightNotes = spotlightNotes.notes || [];\n\n  $('#spotlight-submissions').empty();\n\n  if (spotlightNotesCount) {\n    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#spotlight-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(spotlightNotes, {\n      heading: null,\n      container: '#spotlight-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Spotlight',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);\n          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: spotlightNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Spotlight',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#spotlight-submissions\"]').parent().hide();\n  }\n\n  // Poster Papers tab\n  var posterNotesCount = posterNotes.count || 0;\n  posterNotes = posterNotes.notes || [];\n\n  $('#poster-submissions').empty();\n\n  if (posterNotesCount) {\n    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#poster-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(posterNotes, {\n      heading: null,\n      container: '#poster-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Poster',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);\n          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: posterNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Poster',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#poster-submissions\"]').parent().hide();\n  }\n\n  // Rejected Papers tab\n  var submittedNotesCount = submittedNotes.count || 0;\n  submittedNotes = submittedNotes.notes || [];\n\n  $('#submitted-submissions').empty();\n\n  if (submittedNotesCount) {\n    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#submitted-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(submittedNotes, {\n      heading: null,\n      container: '#submitted-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Submitted',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);\n          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: submittedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Submitted',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#submitted-submissions\"]').parent().hide();\n  }\n\n  // Activity Tab\n  if (activityNotes.length) {\n    var displayOptions = {\n      container: '#recent-activity',\n      user: user \u0026\u0026 user.profile,\n      showActionButtons: true\n    };\n\n    $(displayOptions.container).empty();\n\n    Webfield.ui.activityList(activityNotes, displayOptions);\n\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().hide();\n  }\n  \n  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))\n  \n  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);\n  if (removedNotesCount) {\n    $('#desk-rejected-withdrawn-submissions').empty();\n\n    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);\n    Webfield.ui.submissionList(removedNotesArray, {\n      heading: null,\n      container: '#desk-rejected-withdrawn-submissions',\n      search: {\n        enabled: false\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: removedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        if (offset \u003c deskRejectedNotes.count) {\n          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset\n          });\n        } else {\n          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset - roundedDeskRejectedNotes\n          });\n        }\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#desk-rejected-withdrawn-submissions\"]').parent().hide();\n  }\n\n  // var withdrawnNotesCount = withdrawnNotes.count || 0;\n  // if (withdrawnNotesCount) {\n  //   $('#withdrawn-submissions').empty();\n\n  //   var withdrawnNotesArray = withdrawnNotes.notes || [];\n  //   Webfield.ui.submissionList(withdrawnNotesArray, {\n  //     heading: null,\n  //     container: '#withdrawn-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: withdrawnNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#withdrawn-submissions\"]').parent().hide();\n  // }\n\n  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;\n  // if (deskRejectedNotesCount) {\n  //   $('#desk-rejected-submissions').empty();\n\n  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];\n  //   Webfield.ui.submissionList(deskRejectedNotesArray, {\n  //     heading: null,\n  //     container: '#desk-rejected-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: deskRejectedNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#desk-rejected-submissions\"]').parent().hide();\n  // }\n\n  $('#notes .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n// END GROUP CODE\n});\n//# sourceURL=webfieldCode.js","writable":false,"query":{"id":"ICLR.cc/2022/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2022/Conference"},"buildId":"v1.5.0-1-g2b07894","isFallback":false,"gip":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;">ICLR 2022 Conference | OpenReview</p></next-route-announcer><script>// Webfield Code for ICLR.cc/2022/Conference
$(function() {
var args = {"id":"ICLR.cc/2022/Conference"};
var group = {"id":"ICLR.cc/2022/Conference"};
var document = null;
var window = null;

// TODO: remove these vars when all old webfields have been archived
var model = {
  tokenPayload: function() {
    return { user: user }
  }
};
var controller = {
  get: Webfield.get,
  addHandler: function(name, funcMap) {
    Object.values(funcMap).forEach(function(func) {
      func();
    });
  },
};

$('#group-container').empty();
// START GROUP CODE
// ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var CONFERENCE_ID = 'ICLR.cc/2022/Conference';
var PARENT_GROUP_ID = 'ICLR.cc/2022';
var SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';
var REVIEWERS_NAME = 'Reviewers';
var AREA_CHAIRS_NAME = 'Area_Chairs';
var AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';
var REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';
var PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';
var AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';
var HEADER = {"title": "The Tenth International Conference on Learning Representations ", "subtitle": "ICLR 2022", "location": "Virtual", "date": "Apr 25 2022", "website": "https://iclr.cc/Conferences/2022", "instructions": "", "deadline": "Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0", "contact": "iclr2022pc@gmail.com"};
var PUBLIC = true;

var WILDCARD_INVITATION = CONFERENCE_ID + '/.*';
var BUFFER = 0;  // deprecated
var PAGE_SIZE = 50;

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true,
  showTags: false
};
var commentDisplayOptions = {
  pdfLink: false,
  replyCount: true,
  showContents: false,
  showParent: true
};

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }

  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderSubmissionButton();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/
function load() {

  var spotlightNotesP = $.Deferred().resolve([]);
  var oralNotesP = $.Deferred().resolve([]);
  var posterNotesP = $.Deferred().resolve([]);
  
  
  var activityNotesP = $.Deferred().resolve([]);
  var authorNotesP = $.Deferred().resolve([]);
  var userGroupsP = $.Deferred().resolve([]);
  var withdrawnNotesP = $.Deferred().resolve([]);
  var deskRejectedNotesP = $.Deferred().resolve([]);
  var submittedNotesP = $.Deferred().resolve([]);

  if (PUBLIC) {
      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      //   pageSize: PAGE_SIZE,
      //   details: 'replyCount',
      //   includeCount: true
      // });

    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Spotlight',
      details: 'replyCount',
      includeCount: true
    });
  
    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Oral',
      details: 'replyCount',
      includeCount: true
    });
  
    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Poster',
      details: 'replyCount',
      includeCount: true
    });
  
    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Submitted',
      details: 'replyCount',
      includeCount: true
    });

    if (WITHDRAWN_SUBMISSION_ID) {
      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount',
        includeCount: true
      });
    }

    if (DESK_REJECTED_SUBMISSION_ID) {
      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount,invitation,original',
        includeCount: true
      });
    }
  }

  if (user && !_.startsWith(user.id, 'guest_')) {
    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {
      pageSize: PAGE_SIZE,
      details: 'forumContent,invitation,writable'
    });

    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });

    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.authorids': user.profile.id
    });
  }

  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);
}

// Render functions
function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);

  Webfield.ui.spinner('#notes', { inline: true });
}

function renderSubmissionButton() {
  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})
    .then(function(invitation) {
      Webfield.ui.submissionButton(invitation, user, {
        onNoteCreated: function() {
          // Callback funtion to be run when a paper has successfully been submitted (required)
          if (PUBLIC) {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'A list of all submissions will be available after the deadline.');
          } else {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'The author console page for managing your submissions will be available soon.');
          }

          load().then(renderContent).then(function() {
            $('.tabs-container a[href="#your-consoles"]').click();
          });
        }
      });
    });
}

function renderConferenceTabs() {
  var sections = [
    {
      heading: 'Your Consoles',
      id: 'your-consoles',
    }
  ];

  if (PUBLIC) {
    // sections.push({
    //   heading: 'All Submissions',
    //   id: 'all-submissions',
    // });
    sections.push({
      heading: 'Oral Presentations',
      id: 'oral-submissions',
    });
    sections.push({
      heading: 'Spotlight Presentations',
      id: 'spotlight-submissions',
    });
    sections.push({
      heading: 'Poster Presentations',
      id: 'poster-submissions',
    });
    sections.push({
      heading: 'Rejected Submissions',
      id: 'submitted-submissions',
    });
    // if (WITHDRAWN_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Withdrawn Submissions',
    //     id: 'withdrawn-submissions',
    //   })
    // }
    // if (DESK_REJECTED_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Desk Rejected Submissions',
    //     id: 'desk-rejected-submissions',
    //   })
    // }
    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {
      sections.push({
        heading: 'Desk Rejected/Withdrawn Submissions',
        id: 'desk-rejected-withdrawn-submissions',
      })
    }
  }

  sections.push({
    heading: 'Recent Activity',
    id: 'recent-activity',
  }
)

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortBy(_.uniq(allGroups));
  var links = [];
  uniqueGroups.forEach(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }
    links.push(
      [
        '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
        '</li>'
      ].join('')
    );
  });

  $('#your-consoles .submissions-list').append(links);
}

function renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {

  // Your Consoles tab
  if (userGroups.length || authorNotes.length) {

    var $container = $('#your-consoles').empty();
    $container.append('<ul class="list-unstyled submissions-list">');

    var allConsoles = [];
    if (authorNotes.length) {
      allConsoles.push(AUTHORS_ID);
    }
    userGroups.forEach(function(group) {
      allConsoles.push(group.id);
    });

    // Render all console links for the user
    createConsoleLinks(allConsoles);

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }


  // Oral Papers tab
  var oralNotesCount = oralNotes.count || 0;
  oralNotes = oralNotes.notes || [];

  $('#oral-submissions').empty();

  if (oralNotesCount) {
    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#oral-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(oralNotes, {
      heading: null,
      container: '#oral-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Oral',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);
          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: oralNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Oral',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#oral-submissions"]').parent().hide();
  }

  // Spotlight Papers tab
  var spotlightNotesCount = spotlightNotes.count || 0;
  spotlightNotes = spotlightNotes.notes || [];

  $('#spotlight-submissions').empty();

  if (spotlightNotesCount) {
    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#spotlight-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(spotlightNotes, {
      heading: null,
      container: '#spotlight-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Spotlight',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);
          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: spotlightNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Spotlight',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#spotlight-submissions"]').parent().hide();
  }

  // Poster Papers tab
  var posterNotesCount = posterNotes.count || 0;
  posterNotes = posterNotes.notes || [];

  $('#poster-submissions').empty();

  if (posterNotesCount) {
    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#poster-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(posterNotes, {
      heading: null,
      container: '#poster-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Poster',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);
          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: posterNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Poster',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#poster-submissions"]').parent().hide();
  }

  // Rejected Papers tab
  var submittedNotesCount = submittedNotes.count || 0;
  submittedNotes = submittedNotes.notes || [];

  $('#submitted-submissions').empty();

  if (submittedNotesCount) {
    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#submitted-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(submittedNotes, {
      heading: null,
      container: '#submitted-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Submitted',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);
          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: submittedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Submitted',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#submitted-submissions"]').parent().hide();
  }

  // Activity Tab
  if (activityNotes.length) {
    var displayOptions = {
      container: '#recent-activity',
      user: user && user.profile,
      showActionButtons: true
    };

    $(displayOptions.container).empty();

    Webfield.ui.activityList(activityNotes, displayOptions);

    $('.tabs-container a[href="#recent-activity"]').parent().show();
  } else {
    $('.tabs-container a[href="#recent-activity"]').parent().hide();
  }
  
  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))
  
  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);
  if (removedNotesCount) {
    $('#desk-rejected-withdrawn-submissions').empty();

    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);
    Webfield.ui.submissionList(removedNotesArray, {
      heading: null,
      container: '#desk-rejected-withdrawn-submissions',
      search: {
        enabled: false
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: removedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        if (offset < deskRejectedNotes.count) {
          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset
          });
        } else {
          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset - roundedDeskRejectedNotes
          });
        }
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#desk-rejected-withdrawn-submissions"]').parent().hide();
  }

  // var withdrawnNotesCount = withdrawnNotes.count || 0;
  // if (withdrawnNotesCount) {
  //   $('#withdrawn-submissions').empty();

  //   var withdrawnNotesArray = withdrawnNotes.notes || [];
  //   Webfield.ui.submissionList(withdrawnNotesArray, {
  //     heading: null,
  //     container: '#withdrawn-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: withdrawnNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#withdrawn-submissions"]').parent().hide();
  // }

  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;
  // if (deskRejectedNotesCount) {
  //   $('#desk-rejected-submissions').empty();

  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];
  //   Webfield.ui.submissionList(deskRejectedNotesArray, {
  //     heading: null,
  //     container: '#desk-rejected-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: deskRejectedNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#desk-rejected-submissions"]').parent().hide();
  // }

  $('#notes .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();
// END GROUP CODE
});
//# sourceURL=webfieldCode.js</script><script src="./ICLR2022_spotlight_3_files/index-55b7d6149a41ddec.js.download"></script><script src="./ICLR2022_spotlight_3_files/about-77a61c48f23a1315.js.download"></script><script src="./ICLR2022_spotlight_3_files/venues-458c67c21955226a.js.download"></script><script src="./ICLR2022_spotlight_3_files/contact-2c775b351d0a5421.js.download"></script><script src="./ICLR2022_spotlight_3_files/sponsors-987fb223230996d5.js.download"></script><script src="./ICLR2022_spotlight_3_files/terms-e2f16e0dce69665f.js.download"></script><script src="./ICLR2022_spotlight_3_files/privacy-276f217f8a2a3840.js.download"></script><script src="./ICLR2022_spotlight_3_files/login-6d19e702f9bd1dcc.js.download"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>