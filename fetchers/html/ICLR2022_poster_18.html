<!DOCTYPE html>
<!-- saved from url=(0074)https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://openreview.net/favicon.ico"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><script async="" src="./ICLR2022_poster_18_files/js"></script><script>window.dataLayer = window.dataLayer || [];
function gtag() { dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-108703919-1', {
  page_path: window.location.pathname + window.location.search,
  transport_type: 'beacon'
});</script><title>ICLR 2022 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta property="og:title" content="ICLR 2022 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2022 Conference"><meta name="next-head-count" content="14"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preload" href="./ICLR2022_poster_18_files/ed513f7ce02040ba.css" as="style"><link rel="stylesheet" href="./ICLR2022_poster_18_files/ed513f7ce02040ba.css" data-n-g=""><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./ICLR2022_poster_18_files/polyfills-5cd94c89d3acac5f.js.download"></script><script src="./ICLR2022_poster_18_files/webpack-363f7b452897525e.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/framework-79bce4a3a540b080.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/main-aaf30a81beffaaa5.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/_app-dd140e4fb7437f25.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/6253-86ab2843b0544962.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/group-18de1916b352abe5.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/_buildManifest.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/_ssgManifest.js.download" defer=""></script><script src="./ICLR2022_poster_18_files/_middlewareManifest.js.download" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap">@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4DRG.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZtyH.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNb4Q.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFlYA.woff) format('woff')}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARPQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARGQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARDQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4AROQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARBQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARNQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARMQ_mu72BiBLE.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0OIpQlx3QUlC5A4PNr4ARCQ_mu72Bi.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyOzW1IPriezag.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyHzW1IPriezag.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyCzW1IPriezag.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyPzW1IPriezag.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyAzW1IPriezag.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyMzW1IPriezag.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyNzW1IPriezag.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0TIpQlx3QUlC5A4PNr4Az5ZuyDzW1IPrie.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr4TRASf6M7VBj.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5DRASf6M7VBj.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6TRASf6M7VBj.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6jRASf6M7VBj.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr6zRASf6M7VBj.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0IIpQlx3QUlC5A4PNr5TRASf6M7Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVadyBx2pqPIif.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVYNyBx2pqPIif.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZdyBx2pqPIif.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVaNyBx2pqPIif.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZ9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVa9yBx2pqPIif.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVatyBx2pqPIif.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Noto Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/notosans/v27/o-0NIpQlx3QUlC5A4PNjXhFVZNyBx2pqPA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><script src="./ICLR2022_poster_18_files/tex-chtml-full.js.download" async="" crossorigin="anonymous"></script><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/index-55b7d6149a41ddec.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/about-77a61c48f23a1315.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/venues-458c67c21955226a.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/contact-2c775b351d0a5421.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/sponsors-987fb223230996d5.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/terms-e2f16e0dce69665f.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/privacy-276f217f8a2a3840.js.download"><link as="script" rel="prefetch" href="./ICLR2022_poster_18_files/login-6d19e702f9bd1dcc.js.download"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-stretchy-v.mjx-c28 mjx-beg mjx-c::before {
  content: "\239B";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c28 mjx-ext mjx-c::before {
  content: "\239C";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c28 mjx-end mjx-c::before {
  content: "\239D";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c28 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c28 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-stretchy-v.mjx-c29 mjx-beg mjx-c::before {
  content: "\239E";
  padding: 1.154em 0.875em 0.655em 0;
}

mjx-stretchy-v.mjx-c29 mjx-ext mjx-c::before {
  content: "\239F";
  width: 0.875em;
}

mjx-stretchy-v.mjx-c29 mjx-end mjx-c::before {
  content: "\23A0";
  padding: 1.165em 0.875em 0.644em 0;
}

mjx-stretchy-v.mjx-c29 > mjx-end {
  margin-top: -1.809em;
}

mjx-stretchy-v.mjx-c29 > mjx-ext {
  border-top-width: 1.779em;
  border-bottom-width: 1.779em;
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c1D44C.TEX-I::before {
  padding: 0.683em 0.763em 0 0;
  content: "Y";
}

mjx-c.mjx-c1D44B.TEX-I::before {
  padding: 0.683em 0.852em 0 0;
  content: "X";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c2218::before {
  padding: 0.444em 0.5em 0 0;
  content: "\2218";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D435.TEX-I::before {
  padding: 0.683em 0.759em 0 0;
  content: "B";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c1D70C.TEX-I::before {
  padding: 0.442em 0.517em 0.216em 0;
  content: "\3C1";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c5E::before {
  padding: 0.694em 0.5em 0 0;
  content: "^";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c2225::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D41B.TEX-B::before {
  padding: 0.694em 0.639em 0.006em 0;
  content: "b";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c1D714.TEX-I::before {
  padding: 0.443em 0.622em 0.011em 0;
  content: "\3C9";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c7E::before {
  padding: 0.318em 0.5em 0 0;
  content: "~";
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c1D45E.TEX-I::before {
  padding: 0.442em 0.46em 0.194em 0;
  content: "q";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c3F.TEX-MI::before {
  padding: 0.716em 0.551em 0 0;
  content: "?";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c398::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "\398";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c1D715::before {
  padding: 0.715em 0.566em 0.022em 0;
  content: "\2202";
}

mjx-c.mjx-c2F.TEX-S1::before {
  padding: 0.85em 0.578em 0.349em 0;
  content: "/";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-c2229::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2229";
}

mjx-c.mjx-c41::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c4E::before {
  padding: 0.683em 0.75em 0 0;
  content: "N";
}

mjx-c.mjx-c44::before {
  padding: 0.683em 0.764em 0 0;
  content: "D";
}

mjx-c.mjx-c49::before {
  padding: 0.683em 0.361em 0 0;
  content: "I";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c2DC.TEX-S1::before {
  padding: 0.722em 0.556em 0 0;
  content: "\2DC";
}

mjx-c.mjx-c28.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: "(";
}

mjx-c.mjx-c29.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: ")";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c53::before {
  padding: 0.705em 0.556em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c45::before {
  padding: 0.68em 0.681em 0 0;
  content: "E";
}

mjx-c.mjx-c1D499.TEX-BI::before {
  padding: 0.452em 0.659em 0.008em 0;
  content: "x";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c22C6::before {
  padding: 0.486em 0.5em 0 0;
  content: "\22C6";
}
</style></head><body data-new-gr-c-s-check-loaded="14.1068.0" data-gr-ext-installed=""><div id="__next" data-reactroot=""><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off" autocorrect="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login?redirect=%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%23poster-submissions&amp;noprompt=true">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2022"><img class="icon" src="./ICLR2022_poster_18_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2022</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>The Tenth International Conference on Learning Representations </h1>
<h3>ICLR 2022</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Virtual
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> Apr 25 2022
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/Conferences/2022" title="The Tenth International Conference on Learning Representations  Homepage" target="_blank">https://iclr.cc/Conferences/2022</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2022pc@gmail.com" target="_blank">iclr2022pc@gmail.com</a>
      </span>
  </h4>

<div class="description">
    <p class="no-margin">Please see the venue website for more information.</p>
  <p>Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0</p>
</div>
</div><div id="invitation" style="display: block;"></div><div id="notes">
<div class="tabs-container " style=""><div class="mobile-full-width">
  <ul class="nav nav-tabs" role="tablist">
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
          Your Consoles
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oral-submissions" aria-controls="oral-submissions" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true" aria-expanded="false">
          Oral Presentations
        </a>
      </li>
      <li role="presentation" class="">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#spotlight-submissions" aria-controls="spotlight-submissions" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true" aria-expanded="false">
          Spotlight Presentations
        </a>
      </li>
      <li role="presentation" class="active">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#poster-submissions" aria-controls="poster-submissions" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true" aria-expanded="true">
          Poster Presentations
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#submitted-submissions" aria-controls="submitted-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
          Rejected Submissions
        </a>
      </li>
      <li role="presentation">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#desk-rejected-withdrawn-submissions" aria-controls="desk-rejected-withdrawn-submissions" role="tab" data-toggle="tab" data-tab-index="5" data-modify-history="true">
          Desk Rejected/Withdrawn Submissions
        </a>
      </li>
      <li role="presentation" style="display: none;">
        <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#recent-activity" aria-controls="recent-activity" role="tab" data-toggle="tab" data-tab-index="6" data-modify-history="true">
          Recent Activity
        </a>
      </li>
  </ul>
</div>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade" id="oral-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="NMEceG4v69Y" data-number="224">
        <h4>
          <a href="https://openreview.net/forum?id=NMEceG4v69Y">
              CycleMLP: A MLP-like Architecture for Dense Prediction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMEceG4v69Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shoufa_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoufa_Chen1">Shoufa Chen</a>, <a href="https://openreview.net/profile?id=~Enze_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enze_Xie1">Enze Xie</a>, <a href="https://openreview.net/profile?id=~Chongjian_GE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjian_GE1">Chongjian GE</a>, <a href="https://openreview.net/profile?id=~Runjian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runjian_Chen1">Runjian Chen</a>, <a href="https://openreview.net/profile?id=~Ding_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ding_Liang1">Ding Liang</a>, <a href="https://openreview.net/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMEceG4v69Y-details-500" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMEceG4v69Y-details-500"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">MLP, Dense Prediction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope
        with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A versatile MLP-like architecture for both recognition and dense prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMEceG4v69Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0xiJLKH-ufZ" data-number="222">
        <h4>
          <a href="https://openreview.net/forum?id=0xiJLKH-ufZ">
              Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0xiJLKH-ufZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fan_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fan_Bao1">Fan Bao</a>, <a href="https://openreview.net/profile?id=~Chongxuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongxuan_Li1">Chongxuan Li</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>, <a href="https://openreview.net/profile?id=~Bo_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhang2">Bo Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0xiJLKH-ufZ-details-491" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0xiJLKH-ufZ-details-491"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">diffusion probabilistic models, generative models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an analytic framework of estimating the optimal reverse variance in DPMs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0xiJLKH-ufZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="uSE03demja" data-number="208">
        <h4>
          <a href="https://openreview.net/forum?id=uSE03demja">
              RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=uSE03demja" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pingchuan_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pingchuan_Ma3">Pingchuan Ma</a>, <a href="https://openreview.net/profile?id=~Tao_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Du1">Tao Du</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Wojciech_Matusik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Matusik2">Wojciech Matusik</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#uSE03demja-details-17" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uSE03demja-details-17"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">differentiable rendering, differentiable simulation, system identification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel approach to address the problem of identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3wU2UX0voE" data-number="154">
        <h4>
          <a href="https://openreview.net/forum?id=3wU2UX0voE">
              The Information Geometry of Unsupervised Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3wU2UX0voE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3wU2UX0voE-details-575" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3wU2UX0voE-details-575"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised skill learning, reward-free RL, mutual information, DIAYN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that mutual information skill learning is optimal in one sense but not optimal in another sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=3wU2UX0voE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="pMQwKL1yctf" data-number="86">
        <h4>
          <a href="https://openreview.net/forum?id=pMQwKL1yctf">
              Language modeling via stochastic processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=pMQwKL1yctf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rose_E_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rose_E_Wang1">Rose E Wang</a>, <a href="https://openreview.net/profile?id=~Esin_Durmus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Esin_Durmus1">Esin Durmus</a>, <a href="https://openreview.net/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Oral</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#pMQwKL1yctf-details-408" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pMQwKL1yctf-details-408"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">contrastive learning, language modelling, stochastic processes</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a language model that implicitly plans via a latent stochastic process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=pMQwKL1yctf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>

<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class=" active " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="3">
          <span>›</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="2">
          <span>»</span>
      </li>
  </ul>
</nav>

</div>
    <div role="tabpanel" class="tab-pane fade" id="spotlight-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="dFbKQaRk15w" data-number="693">
        <h4>
          <a href="https://openreview.net/forum?id=dFbKQaRk15w">
              Equivariant Subgraph Aggregation Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dFbKQaRk15w" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Beatrice_Bevilacqua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beatrice_Bevilacqua1">Beatrice Bevilacqua</a>, <a href="https://openreview.net/profile?id=~Fabrizio_Frasca1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabrizio_Frasca1">Fabrizio Frasca</a>, <a href="https://openreview.net/profile?id=~Derek_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Lim1">Derek Lim</a>, <a href="https://openreview.net/profile?id=~Balasubramaniam_Srinivasan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balasubramaniam_Srinivasan1">Balasubramaniam Srinivasan</a>, <a href="https://openreview.net/profile?id=~Chen_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Cai1">Chen Cai</a>, <a href="https://openreview.net/profile?id=~Gopinath_Balamurugan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gopinath_Balamurugan1">Gopinath Balamurugan</a>, <a href="https://openreview.net/profile?id=~Michael_M._Bronstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_M._Bronstein1">Michael M. Bronstein</a>, <a href="https://openreview.net/profile?id=~Haggai_Maron1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haggai_Maron1">Haggai Maron</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dFbKQaRk15w-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dFbKQaRk15w-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Expressive power, Equivariance, Weisfeiler-Leman</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a provably expressive graph learning framework based on representing graphs as multisets of subgraphs and processing them with an equivariant architecture.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="D78Go4hVcxO" data-number="676">
        <h4>
          <a href="https://openreview.net/forum?id=D78Go4hVcxO">
              How Do Vision Transformers Work?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=D78Go4hVcxO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Namuk_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Namuk_Park1">Namuk Park</a>, <a href="https://openreview.net/profile?id=~Songkuk_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songkuk_Kim1">Songkuk Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#D78Go4hVcxO-details-939" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D78Go4hVcxO-details-939"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">vision transformer, self-attention, multi-head self-attention, loss landscape</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that (1) multi-head self-attentions (MSAs) for computer vision flatten the loss landscapes, (2) MSAs are low-pass filters as opposed to Convs, and (3) MSAs at the end of a stage significantly improve the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=D78Go4hVcxO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kZ0UYdhqkNY" data-number="670">
        <h4>
          <a href="https://openreview.net/forum?id=kZ0UYdhqkNY">
              Variational methods for simulation-based inference
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kZ0UYdhqkNY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Manuel_Gl%C3%B6ckler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manuel_Glöckler1">Manuel Glöckler</a>, <a href="https://openreview.net/profile?id=~Michael_Deistler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Deistler1">Michael Deistler</a>, <a href="https://openreview.net/profile?id=~Jakob_H._Macke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_H._Macke1">Jakob H. Macke</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 13 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kZ0UYdhqkNY-details-943" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kZ0UYdhqkNY-details-943"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">likelihood-free inference, simulation-based inference, variational inference, neural density estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We combine likelihood-estimation with variational inference to achieve a scalable approach for simulation-based inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JprM0p-q0Co" data-number="590">
        <h4>
          <a href="https://openreview.net/forum?id=JprM0p-q0Co">
              Tackling the Generative Learning Trilemma with Denoising Diffusion GANs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JprM0p-q0Co" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhisheng_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhisheng_Xiao1">Zhisheng Xiao</a>, <a href="https://openreview.net/profile?id=~Karsten_Kreis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karsten_Kreis1">Karsten Kreis</a>, <a href="https://openreview.net/profile?id=~Arash_Vahdat3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arash_Vahdat3">Arash Vahdat</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JprM0p-q0Co-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JprM0p-q0Co-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">To reduce the number of sampling steps in diffusion models, we propose to model the denoising distribution with conditional GANs. We show our model tackles the generative learning trilemma &amp; achieves high sample quality, diversity &amp; fast sampling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="yKIAXjkJc2F" data-number="562">
        <h4>
          <a href="https://openreview.net/forum?id=yKIAXjkJc2F">
              Imbedding Deep Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=yKIAXjkJc2F" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrew_Corbett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Corbett1">Andrew Corbett</a>, <a href="https://openreview.net/profile?id=~Dmitry_Kangin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Kangin1">Dmitry Kangin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#yKIAXjkJc2F-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yKIAXjkJc2F-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural ODEs, Optimal Control, Deep Neural Networks, Invariant Imbedding</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.
        Our new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Invariant imbedding solution for (Bolza) optimal control problem derived and proved to yield new architectures of imbedded deep neural networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=yKIAXjkJc2F&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZkC8wKoLbQ7" data-number="560">
        <h4>
          <a href="https://openreview.net/forum?id=ZkC8wKoLbQ7">
              Understanding and Preventing Capacity Loss in Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZkC8wKoLbQ7" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Clare_Lyle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Clare_Lyle1">Clare Lyle</a>, <a href="https://openreview.net/profile?id=~Mark_Rowland1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Rowland1">Mark Rowland</a>, <a href="https://openreview.net/profile?id=~Will_Dabney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Will_Dabney1">Will Dabney</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZkC8wKoLbQ7-details-199" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZkC8wKoLbQ7-details-199"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.
        We identify a key mechanism by which this occurs in agents using neural networks as function approximators: \textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.
        We demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that RL agents experience representation collapse in sparse reward environments and propose an auxiliary task that prevents this from happening and outperforms the state of the art on the Atari benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZkC8wKoLbQ7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="1JDiK_TbV4S" data-number="550">
        <h4>
          <a href="https://openreview.net/forum?id=1JDiK_TbV4S">
              Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration
          </a>
        
          
            <a href="https://openreview.net/pdf?id=1JDiK_TbV4S" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cian_Eastwood1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cian_Eastwood1">Cian Eastwood</a>, <a href="https://openreview.net/profile?id=~Ian_Mason1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Mason1">Ian Mason</a>, <a href="https://openreview.net/profile?id=~Chris_Williams1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Williams1">Chris Williams</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#1JDiK_TbV4S-details-316" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1JDiK_TbV4S-details-316"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transfer learning, dataset shift, unsupervised domain adaptation, source-free domain adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We identify a type of domain shift which can be resolved by restoring the *same* features and address it in the source-free setting by using softly-binned histograms to cheaply and flexibly align the marginal feature distributions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=1JDiK_TbV4S&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lnEaqbTJIRz" data-number="544">
        <h4>
          <a href="https://openreview.net/forum?id=lnEaqbTJIRz">
              The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lnEaqbTJIRz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yoav_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoav_Levine1">Yoav Levine</a>, <a href="https://openreview.net/profile?id=~Noam_Wies1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noam_Wies1">Noam Wies</a>, <a href="https://openreview.net/profile?id=~Daniel_Jannai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jannai1">Daniel Jannai</a>, <a href="https://openreview.net/profile?email=dan.nav%40mail.huji.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="dan.nav@mail.huji.ac.il">Dan Navon</a>, <a href="https://openreview.net/profile?id=~Yedid_Hoshen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yedid_Hoshen3">Yedid Hoshen</a>, <a href="https://openreview.net/profile?id=~Amnon_Shashua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amnon_Shashua1">Amnon Shashua</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lnEaqbTJIRz-details-890" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lnEaqbTJIRz-details-890"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Language Modeling, Pretraining, Self-attention, Transformers, Expressivity, Separation Rank, Sentence Embeddings</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.	This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove that pertained LMs model stronger dependencies between sentences that were shown in same training example, thus indicating benefits of better informed "pretraining example design"</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AUGBfDIV9rL" data-number="521">
        <h4>
          <a href="https://openreview.net/forum?id=AUGBfDIV9rL">
              Emergent Communication at Scale
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AUGBfDIV9rL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Rahma_Chaabouni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rahma_Chaabouni1">Rahma Chaabouni</a>, <a href="https://openreview.net/profile?id=~Florian_Strub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Strub1">Florian Strub</a>, <a href="https://openreview.net/profile?id=~Florent_Altch%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florent_Altché1">Florent Altché</a>, <a href="https://openreview.net/profile?id=~Eugene_Tarassov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Tarassov1">Eugene Tarassov</a>, <a href="https://openreview.net/profile?id=~Corentin_Tallec2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Corentin_Tallec2">Corentin Tallec</a>, <a href="https://openreview.net/profile?id=~Elnaz_Davoodi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elnaz_Davoodi2">Elnaz Davoodi</a>, <a href="https://openreview.net/profile?id=~Kory_Wallace_Mathewson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kory_Wallace_Mathewson1">Kory Wallace Mathewson</a>, <a href="https://openreview.net/profile?id=~Olivier_Tieleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Tieleman1">Olivier Tieleman</a>, <a href="https://openreview.net/profile?id=~Angeliki_Lazaridou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angeliki_Lazaridou1">Angeliki Lazaridou</a>, <a href="https://openreview.net/profile?id=~Bilal_Piot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bilal_Piot1">Bilal Piot</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AUGBfDIV9rL-details-133" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AUGBfDIV9rL-details-133"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">emergent communication, multi-agent reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques.
        We also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work argues the importance of scaling up the emergent communication framework and investigates the impact of three scaling up aspects, namely the dataset, task complexity, and population size.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vds4SNooOe" data-number="496">
        <h4>
          <a href="https://openreview.net/forum?id=vds4SNooOe">
              Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vds4SNooOe" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jingchao_Ni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingchao_Ni1">Jingchao Ni</a>, <a href="https://openreview.net/profile?id=~Wei_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Cheng1">Wei Cheng</a>, <a href="https://openreview.net/profile?email=zchen%40nec-labs.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zchen@nec-labs.com">Zhengzhang Chen</a>, <a href="https://openreview.net/profile?email=takayoshi.asakura%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="takayoshi.asakura@nec.com">Takayoshi Asakura</a>, <a href="https://openreview.net/profile?email=tomoya-s%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomoya-s@nec.com">Tomoya Soma</a>, <a href="https://openreview.net/profile?email=kato%40renascience.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="kato@renascience.co.jp">Sho Kato</a>, <a href="https://openreview.net/profile?id=~Haifeng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haifeng_Chen1">Haifeng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vds4SNooOe-details-592" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vds4SNooOe-details-592"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep learning, represenation learning, generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on "coarse" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a "coarsely" pre-trained model to new tasks with a few "finer-grained" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a training framework characterized by a novel superclass conditional Gaussian mixture (SCGM) based generative model for learning fine-grained representations for cross-granularity adaptation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vds4SNooOe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-ApAkox5mp" data-number="427">
        <h4>
          <a href="https://openreview.net/forum?id=-ApAkox5mp">
              SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-ApAkox5mp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaccharie_Ramzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zaccharie_Ramzi1">Zaccharie Ramzi</a>, <a href="https://openreview.net/profile?id=~Florian_Mannel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Mannel1">Florian Mannel</a>, <a href="https://openreview.net/profile?id=~Shaojie_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaojie_Bai1">Shaojie Bai</a>, <a href="https://openreview.net/profile?id=~Jean-Luc_Starck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Luc_Starck1">Jean-Luc Starck</a>, <a href="https://openreview.net/profile?id=~Philippe_Ciuciu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Ciuciu1">Philippe Ciuciu</a>, <a href="https://openreview.net/profile?id=~Thomas_Moreau2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Moreau2">Thomas Moreau</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-ApAkox5mp-details-904" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-ApAkox5mp-details-904"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit models, bi-level optimization, quasi-newton methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models~(DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach in many settings, ranging from hyperparameter optimization to large Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the computational cost of the backward pass by up to two orders of magnitude. All this is achieved while retaining the excellent performance of the original models in hyperparameter optimization and on CIFAR, and giving encouraging and competitive results on ImageNet.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Use the approximate Jacobian matrix computed in quasi-Newton methods to perform the inversion needed in the training of implicit models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=-ApAkox5mp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ccWaPGl9Hq" data-number="411">
        <h4>
          <a href="https://openreview.net/forum?id=ccWaPGl9Hq">
              Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ccWaPGl9Hq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiawei_Huang3">Jiawei Huang</a>, <a href="https://openreview.net/profile?id=~Jinglin_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinglin_Chen2">Jinglin Chen</a>, <a href="https://openreview.net/profile?id=~Li_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Zhao1">Li Zhao</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?id=~Nan_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nan_Jiang2">Nan Jiang</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 03 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ccWaPGl9Hq-details-746" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ccWaPGl9Hq-details-746"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning theory, deployment efficiency, linear MDP</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an ''optimization with constraints'' perspective: we are interested in exploring an MDP and obtaining a near-optimal policy within minimal \emph{deployment complexity}, whereas in each deployment the policy can sample a large batch of data. Using finite-horizon linear MDPs as a concrete structural model, we reveal the fundamental limit in achieving deployment efficiency by establishing information-theoretic lower bounds, and provide algorithms that achieve the optimal deployment efficiency. Moreover, our formulation for DE-RL is flexible and can serve as a building block for other practically relevant settings; we give ''Safe DE-RL'' and ''Sample-Efficient DE-RL'' as two examples, which may be worth future investigation.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a formal theoretical formulation for depolyment-efficient reinforcement learning; establish lower bounds for deployment complexity and study near-optimal deployment-efficient algorithms in linear MDP setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ccWaPGl9Hq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="A3HHaEdqAJL" data-number="344">
        <h4>
          <a href="https://openreview.net/forum?id=A3HHaEdqAJL">
              Task Relatedness-Based Generalization Bounds for Meta Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=A3HHaEdqAJL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiechao_Guan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiechao_Guan2">Jiechao Guan</a>, <a href="https://openreview.net/profile?id=~Zhiwu_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwu_Lu1">Zhiwu Lu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 02 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#A3HHaEdqAJL-details-586" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="A3HHaEdqAJL-details-586"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Supposing the $n$ training tasks and the new task are sampled from the same environment, traditional meta learning theory derives an error bound on the expected loss over the new task in terms of the empirical training loss, uniformly over the set of all hypothesis spaces. However, there is still little research on how the relatedness of these tasks can affect the full utilization of all $mn$ training data (with $m$ examples per task). In this paper, we propose to address this problem by defining a new notion of task relatedness according to the existence of the bijective transformation between two tasks. A novel generalization bound of $\mathcal{O}(\frac{1}{\sqrt{mn}})$ for meta learning is thus derived by exploiting the proposed task relatedness. Moreover, when investigating a special branch of meta learning that involves representation learning with deep neural networks, we establish spectrally-normalized bounds for both classification and regression problems. Finally, we demonstrate that the relatedness requirement between two tasks is satisfied when the sample space possesses the completeness and separability properties, validating the rationality and applicability of our proposed task-relatedness measure.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="pFyXqxChZc" data-number="314">
        <h4>
          <a href="https://openreview.net/forum?id=pFyXqxChZc">
              IntSGD: Adaptive Floatless Compression of Stochastic Gradients
          </a>
        
          
            <a href="https://openreview.net/pdf?id=pFyXqxChZc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konstantin_Mishchenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantin_Mishchenko1">Konstantin Mishchenko</a>, <a href="https://openreview.net/profile?id=~Bokun_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bokun_Wang2">Bokun Wang</a>, <a href="https://openreview.net/profile?id=~Dmitry_Kovalev2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Kovalev2">Dmitry Kovalev</a>, <a href="https://openreview.net/profile?id=~Peter_Richt%C3%A1rik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Richtárik1">Peter Richtárik</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#pFyXqxChZc-details-634" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pFyXqxChZc-details-634"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">optimization, distributed optimization, compression, theory, parallel training, switchML</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose the provably convergent and computationally cheap IntSGD algorithm for efficient distributed machine learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=pFyXqxChZc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="iLHOIDsPv1P" data-number="313">
        <h4>
          <a href="https://openreview.net/forum?id=iLHOIDsPv1P">
              PAC-Bayes Information Bottleneck
          </a>
        
          
            <a href="https://openreview.net/pdf?id=iLHOIDsPv1P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zifeng_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zifeng_Wang3">Zifeng Wang</a>, <a href="https://openreview.net/profile?id=~Shao-Lun_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shao-Lun_Huang3">Shao-Lun Huang</a>, <a href="https://openreview.net/profile?id=~Ercan_Engin_Kuruoglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ercan_Engin_Kuruoglu1">Ercan Engin Kuruoglu</a>, <a href="https://openreview.net/profile?id=~Jimeng_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimeng_Sun3">Jimeng Sun</a>, <a href="https://openreview.net/profile?id=~Xi_Chen21" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_Chen21">Xi Chen</a>, <a href="https://openreview.net/profile?id=~Yefeng_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yefeng_Zheng2">Yefeng Zheng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#iLHOIDsPv1P-details-885" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iLHOIDsPv1P-details-885"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">information bottleneck, representation learning, generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, over-parameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel PAC-Bayes bound guided information bottleneck for understanding and enhancing deep representation learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=iLHOIDsPv1P&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="jXKKDEi5vJt" data-number="249">
        <h4>
          <a href="https://openreview.net/forum?id=jXKKDEi5vJt">
              Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing
          </a>
        
          
            <a href="https://openreview.net/pdf?id=jXKKDEi5vJt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sai_Praneeth_Karimireddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sai_Praneeth_Karimireddy1">Sai Praneeth Karimireddy</a>, <a href="https://openreview.net/profile?id=~Lie_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lie_He1">Lie He</a>, <a href="https://openreview.net/profile?id=~Martin_Jaggi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Jaggi1">Martin Jaggi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 May 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">25 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#jXKKDEi5vJt-details-923" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jXKKDEi5vJt-details-923"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated learning, Distributed learning, Byzantine robust optimization, Heterogeneity (Non-IID)</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Byzantine-robust distributed learning with heterogeneous data distribution</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=jXKKDEi5vJt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AvcfxqRy4Y" data-number="227">
        <h4>
          <a href="https://openreview.net/forum?id=AvcfxqRy4Y">
              Understanding the Role of Self Attention for Efficient Speech Recognition
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AvcfxqRy4Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kyuhong_Shim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyuhong_Shim1">Kyuhong Shim</a>, <a href="https://openreview.net/profile?id=~Jungwook_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jungwook_Choi1">Jungwook Choi</a>, <a href="https://openreview.net/profile?id=~Wonyong_Sung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonyong_Sung1">Wonyong Sung</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AvcfxqRy4Y-details-887" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AvcfxqRy4Y-details-887"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformer, self attention, speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct roles: phonetic and linguistic localization. Especially, we show by experiments that phonetic localization in the lower layers extracts phonologically meaningful features from speech and reduces the phonetic variance in the utterance for proper linguistic localization in the upper layers. From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved. To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze the role of self attention in Transformer-based speech recognition and present a practical technique to design a model that accelerates the inference and improve the performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=AvcfxqRy4Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="8WawVDdKqlL" data-number="215">
        <h4>
          <a href="https://openreview.net/forum?id=8WawVDdKqlL">
              Label Encoding for Regression Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=8WawVDdKqlL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Deval_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deval_Shah1">Deval Shah</a>, <a href="https://openreview.net/profile?id=~Zi_Yu_Xue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zi_Yu_Xue1">Zi Yu Xue</a>, <a href="https://openreview.net/profile?id=~Tor_Aamodt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tor_Aamodt1">Tor Aamodt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#8WawVDdKqlL-details-471" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8WawVDdKqlL-details-471"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Regression, Label encoding, Output codes</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can improve accuracy by utilizing well-studied binary classification algorithms. We introduce binary-encoded labels (BEL), which generalizes the application of binary classification to regression by providing a framework for considering arbitrary multi-bit values when encoding target values. We identify desirable properties of suitable encoding and decoding functions used for the conversion between real-valued and binary-encoded labels based on theoretical and empirical study. These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings. BEL can be combined with off-the-shelf task-specific feature extractors and trained end-to-end. We propose a series of sample encoding, decoding, and training loss functions for BEL and demonstrate they result in lower error than direct regression and specialized approaches while being suitable for a diverse set of regression problems, network architectures, and evaluation metrics. BEL achieves state-of-the-art accuracies for several regression benchmarks. Code is available at https://github.com/ubc-aamodt-group/BEL_regression.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose binary-encoded labels (BEL) which improves regression by generalizing the application of binary classification. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=8WawVDdKqlL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zNHzqZ9wrRB" data-number="202">
        <h4>
          <a href="https://openreview.net/forum?id=zNHzqZ9wrRB">
              Equivariant Transformers for Neural Network based Molecular Potentials
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zNHzqZ9wrRB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Philipp_Th%C3%B6lke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Thölke1">Philipp Thölke</a>, <a href="https://openreview.net/profile?id=~Gianni_De_Fabritiis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gianni_De_Fabritiis1">Gianni De Fabritiis</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 10 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zNHzqZ9wrRB-details-224" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zNHzqZ9wrRB-details-224"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Molecular Modeling, Quantum Chemistry, Attention, Transformers</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields. In this work we propose TorchMD-NET, a novel equivariant Transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1, and many QM9 targets in both accuracy and computational efficiency. Through an extensive attention weight analysis, we gain valuable insights into the black box predictor and show differences in the learned representation of conformers versus conformations sampled from molecular dynamics or normal modes. Furthermore, we highlight the importance of datasets including off-equilibrium conformations for the evaluation of molecular potentials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel equivariant Transformer architecture for the prediction of molecular potentials and provide insights into the molecular representation through extensive analysis of the model's attention weights.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="9XhPLAjjRB" data-number="172">
        <h4>
          <a href="https://openreview.net/forum?id=9XhPLAjjRB">
              SGD Can Converge to Local Maxima
          </a>
        
          
            <a href="https://openreview.net/pdf?id=9XhPLAjjRB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Ziyin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liu_Ziyin1">Liu Ziyin</a>, <a href="https://openreview.net/profile?email=botao.li95%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="botao.li95@gmail.com">Botao Li</a>, <a href="https://openreview.net/profile?id=~James_B_Simon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_B_Simon1">James B Simon</a>, <a href="https://openreview.net/profile?id=~Masahito_Ueda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masahito_Ueda1">Masahito Ueda</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#9XhPLAjjRB-details-715" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9XhPLAjjRB-details-715"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stochastic gradient descent, saddle points, convergence, amsgrad, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that it can be common for SGD to converge to saddle points and maxima.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="H0oaWl6THa" data-number="158">
        <h4>
          <a href="https://openreview.net/forum?id=H0oaWl6THa">
              Hybrid Local SGD for Federated Learning with Heterogeneous Communications
          </a>
        
          
            <a href="https://openreview.net/pdf?id=H0oaWl6THa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuanxiong_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanxiong_Guo1">Yuanxiong Guo</a>, <a href="https://openreview.net/profile?id=~Ying_Sun5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Sun5">Ying Sun</a>, <a href="https://openreview.net/profile?email=ruihu2017%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruihu2017@gmail.com">Rui Hu</a>, <a href="https://openreview.net/profile?id=~Yanmin_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanmin_Gong1">Yanmin Gong</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 02 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#H0oaWl6THa-details-807" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H0oaWl6THa-details-807"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated Learning, Communication Efficiency, Heterogeneity, Local SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive when each device needs to communicate with the remote server repeatedly for many times over bandwidth-limited networks. In light of both device-to-device (D2D) and device-to-server (D2S) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best balance between model accuracy and runtime. We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings. We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2S communications in HL-SGD can largely speed up the training time of federated learning. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EAy7C1cgE1L" data-number="64">
        <h4>
          <a href="https://openreview.net/forum?id=EAy7C1cgE1L">
              Increasing the Cost of Model Extraction with Calibrated Proof of Work
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EAy7C1cgE1L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Adam_Dziedzic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Dziedzic1">Adam Dziedzic</a>, <a href="https://openreview.net/profile?id=~Muhammad_Ahmad_Kaleem1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhammad_Ahmad_Kaleem1">Muhammad Ahmad Kaleem</a>, <a href="https://openreview.net/profile?id=~Yu_Shen_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Shen_Lu1">Yu Shen Lu</a>, <a href="https://openreview.net/profile?id=~Nicolas_Papernot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Papernot1">Nicolas Papernot</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">27 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EAy7C1cgE1L-details-726" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EAy7C1cgE1L-details-726"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model extraction, model stealing, model functionality stealing, proof-of-work, adversarial machine learning, trustworthy machine learning, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose to make model extraction more difficult by requiring users to complete a callibrated proof-of-work before they can read predictions from a machine learning model exposed via a public API.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=EAy7C1cgE1L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HFmAukZ-k-2" data-number="34">
        <h4>
          <a href="https://openreview.net/forum?id=HFmAukZ-k-2">
              Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HFmAukZ-k-2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marten_Lienen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marten_Lienen1">Marten Lienen</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HFmAukZ-k-2-details-248" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HFmAukZ-k-2-details-248"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spatio-temporal, finite, elements, forecasting, continuous, partial, differential, equation, PDE, graph, gnn, time-series</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A continuous-time graph neural network model for spatio-temporal forecasting that can structurally incorporate prior knowledge</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="BnQhMqDfcKG" data-number="4">
        <h4>
          <a href="https://openreview.net/forum?id=BnQhMqDfcKG">
              Probabilistic Implicit Scene Completion
          </a>
        
          
            <a href="https://openreview.net/pdf?id=BnQhMqDfcKG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsu_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongsu_Zhang1">Dongsu Zhang</a>, <a href="https://openreview.net/profile?id=~Changwoon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changwoon_Choi1">Changwoon Choi</a>, <a href="https://openreview.net/profile?id=~Inbum_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Inbum_Park1">Inbum Park</a>, <a href="https://openreview.net/profile?id=~Young_Min_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Young_Min_Kim1">Young Min Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#BnQhMqDfcKG-details-723" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BnQhMqDfcKG-details-723"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">3D shape completion, 3D generative model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a scalable generative model for multi-modal completion of 3D scenes in implicit representation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="7l1IjZVddDW" data-number="1">
        <h4>
          <a href="https://openreview.net/forum?id=7l1IjZVddDW">
              Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=7l1IjZVddDW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Qiang_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Meng1">Qiang Meng</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feng_Zhou8">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~Hainan_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hainan_Ren1">Hainan Ren</a>, <a href="https://openreview.net/profile?id=~Tianshu_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianshu_Feng1">Tianshu Feng</a>, <a href="https://openreview.net/profile?id=~Guochao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guochao_Liu1">Guochao Liu</a>, <a href="https://openreview.net/profile?id=~Yuanqing_Lin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanqing_Lin3">Yuanqing Lin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 09 Feb 2022)</span>
              <span class="item">ICLR 2022 Spotlight</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#7l1IjZVddDW-details-296" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7l1IjZVddDW-details-296"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>



<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class=" active " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="5">
          <span>›</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="4">
          <span>»</span>
      </li>
  </ul>
</nav>



</div>
    <div role="tabpanel" class="tab-pane fade active in" id="poster-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="afoV8W3-IYp" data-number="68">
        <h4>
          <a href="https://openreview.net/forum?id=afoV8W3-IYp">
              RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=afoV8W3-IYp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaojian_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojian_Ma1">Xiaojian Ma</a>, <a href="https://openreview.net/profile?id=~Weili_Nie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weili_Nie1">Weili Nie</a>, <a href="https://openreview.net/profile?id=~Zhiding_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiding_Yu1">Zhiding Yu</a>, <a href="https://openreview.net/profile?id=~Huaizu_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huaizu_Jiang1">Huaizu Jiang</a>, <a href="https://openreview.net/profile?id=~Chaowei_Xiao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaowei_Xiao2">Chaowei Xiao</a>, <a href="https://openreview.net/profile?id=~Yuke_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuke_Zhu1">Yuke Zhu</a>, <a href="https://openreview.net/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>, <a href="https://openreview.net/profile?id=~Anima_Anandkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anima_Anandkumar1">Anima Anandkumar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#afoV8W3-IYp-details-991" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="afoV8W3-IYp-details-991"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">visual relational reasoning, representation learning, systematic generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel concept-feature dictionary to enable two new concept-guided auxiliary tasks, which largely improve the model performances on visual relational reasoning, especially for systematic generalization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="anbBFlX1tJ1" data-number="56">
        <h4>
          <a href="https://openreview.net/forum?id=anbBFlX1tJ1">
              Boosted Curriculum Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=anbBFlX1tJ1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pascal_Klink2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pascal_Klink2">Pascal Klink</a>, <a href="https://openreview.net/profile?id=~Carlo_D%26%23x27%3BEramo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlo_D&#39;Eramo2">Carlo D'Eramo</a>, <a href="https://openreview.net/profile?id=~Jan_Peters3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Peters3">Jan Peters</a>, <a href="https://openreview.net/profile?id=~Joni_Pajarinen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joni_Pajarinen2">Joni Pajarinen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 12 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">25 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#anbBFlX1tJ1-details-660" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="anbBFlX1tJ1-details-660"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, curriculum learning, boosting, residual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Curriculum value-based reinforcement learning (RL) solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. However, finding an exact way of reusing action-values in this setting is still a poorly understood problem. In this paper, we introduce the concept of boosting to curriculum value-based RL, by approximating the action-value function as a sum of residuals trained on each task. This approach, which we refer to as boosted curriculum reinforcement learning (BCRL), has the benefit of naturally increasing the representativeness of the functional space by adding a new residual each time a new task is presented. This procedure allows reusing previous action-values while promoting expressiveness of the action-value function. We theoretically study BCRL as an approximate value iteration algorithm, discussing advantages over regular curriculum RL in terms of approximation accuracy and convergence to the optimal action-value function. Finally, we provide detailed empirical evidence of the benefits of BCRL in problems requiring curricula for accurate action-value estimation and targeted exploration.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel approach for curriculum RL that increases the representativeness of the functional space as new, increasingly complex, tasks from the curriculum are presented to the agent.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="w4cXZDDib1H" data-number="45">
        <h4>
          <a href="https://openreview.net/forum?id=w4cXZDDib1H">
              ViDT: An Efficient and Effective Fully Transformer-based Object Detector
          </a>
        
          
            <a href="https://openreview.net/pdf?id=w4cXZDDib1H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hwanjun_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hwanjun_Song2">Hwanjun Song</a>, <a href="https://openreview.net/profile?id=~Deqing_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deqing_Sun2">Deqing Sun</a>, <a href="https://openreview.net/profile?id=~Sanghyuk_Chun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanghyuk_Chun1">Sanghyuk Chun</a>, <a href="https://openreview.net/profile?id=~Varun_Jampani2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Varun_Jampani2">Varun Jampani</a>, <a href="https://openreview.net/profile?id=~Dongyoon_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongyoon_Han1">Dongyoon Han</a>, <a href="https://openreview.net/profile?id=~Byeongho_Heo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byeongho_Heo1">Byeongho Heo</a>, <a href="https://openreview.net/profile?id=~Wonjae_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonjae_Kim1">Wonjae Kim</a>, <a href="https://openreview.net/profile?id=~Ming-Hsuan_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming-Hsuan_Yang1">Ming-Hsuan Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#w4cXZDDib1H-details-775" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w4cXZDDib1H-details-775"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">object detection, vision transformer, detection transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We release the code and trained models at https://github.com/naver-ai/vidt.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We integrate vision and detection transformers to build an efficient  and effective fully transformer-based object detector. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5xEgrl_5FAJ" data-number="42">
        <h4>
          <a href="https://openreview.net/forum?id=5xEgrl_5FAJ">
              BiBERT: Accurate Fully Binarized BERT
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5xEgrl_5FAJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Haotong_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haotong_Qin1">Haotong Qin</a>, <a href="https://openreview.net/profile?id=~Yifu_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yifu_Ding2">Yifu Ding</a>, <a href="https://openreview.net/profile?id=~Mingyuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyuan_Zhang1">Mingyuan Zhang</a>, <a href="https://openreview.net/profile?id=~Qinghua_YAN1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qinghua_YAN1">Qinghua YAN</a>, <a href="https://openreview.net/profile?id=~Aishan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aishan_Liu1">Aishan Liu</a>, <a href="https://openreview.net/profile?email=dangqingqing%40baidu.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dangqingqing@baidu.com">Qingqing Dang</a>, <a href="https://openreview.net/profile?id=~Ziwei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Liu1">Ziwei Liu</a>, <a href="https://openreview.net/profile?id=~Xianglong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xianglong_Liu2">Xianglong Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 11 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">30 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5xEgrl_5FAJ-details-931" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5xEgrl_5FAJ-details-931"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Network Binarization, Model Compression, BERT, NLP</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3 times and 31.2 times saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Our BiBERT, for the first time, presents a promising route towards the accurate fully binarized BERT (with 1-bit weight, embedding, and activation) and gives impressive 56.3 times and 31.2 times saving on FLOPs and model size, respectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5xEgrl_5FAJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="tBIQEvApZK5" data-number="40">
        <h4>
          <a href="https://openreview.net/forum?id=tBIQEvApZK5">
              Feature Kernel Distillation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=tBIQEvApZK5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Bobby_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bobby_He1">Bobby He</a>, <a href="https://openreview.net/profile?id=~Mete_Ozay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mete_Ozay1">Mete Ozay</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 May 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#tBIQEvApZK5-details-951" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tBIQEvApZK5-details-951"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Knowledge distillation, Neural Network (NN) Feature learning, ensembling NNs, Deep learning fundamentals, Image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel. We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN’s optimisation process. We extend the theoretical analysis of Allen-Zhu &amp; Li (2020) to show that a trained NN’s feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting. This enables us to prove that KD using only pairwise feature kernel comparisons can improve NN test accuracy in such settings, with both single &amp; ensemble teacher models, whereas standard training without KD fails to generalise. We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimentally corroborate our theory in the image classification setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD &amp; other feature kernel based KD baselines across a range of standard architectures &amp; datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A feature-learning perspective of (ensemble) Knowledge Distillation (KD) in Neural Networks to propose a new method (FKD), with both theoretical &amp; experimental results demonstrating FKD's advantages over standard KD baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-ngwPqanCEZ" data-number="39">
        <h4>
          <a href="https://openreview.net/forum?id=-ngwPqanCEZ">
              Representation-Agnostic Shape Fields
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-ngwPqanCEZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaoyang_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyang_Huang1">Xiaoyang Huang</a>, <a href="https://openreview.net/profile?id=~Jiancheng_Yang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiancheng_Yang3">Jiancheng Yang</a>, <a href="https://openreview.net/profile?id=~Yanjun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanjun_Wang1">Yanjun Wang</a>, <a href="https://openreview.net/profile?id=~Ziyu_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyu_Chen2">Ziyu Chen</a>, <a href="https://openreview.net/profile?id=~Linguo_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linguo_Li1">Linguo Li</a>, <a href="https://openreview.net/profile?id=~Teng_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Teng_Li2">Teng Li</a>, <a href="https://openreview.net/profile?id=~Bingbing_Ni3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bingbing_Ni3">Bingbing Ni</a>, <a href="https://openreview.net/profile?id=~Wenjun_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjun_Zhang3">Wenjun Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-ngwPqanCEZ-details-160" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-ngwPqanCEZ-details-160"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">shape embedding, 3D deep learning, shape classification and segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D deep learning. RASF is implemented with a learnable 3D grid with multiple channels to store local geometry. Based on RASF, shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinate indexing. While there are multiple ways to optimize the learnable parameters of RASF, we provide two effective schemes among all in this paper for RASF pre-training: shape reconstruction and normal estimation. Once trained, RASF becomes a plug-and-play performance booster with negligible cost. Extensive experiments on diverse 3D representation formats, networks and applications, validate the universal effectiveness of the proposed RASF. Code and pre-trained models are publicly available\footnote{\url{https://github.com/seanywang0408/RASF}}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a  generalizable and computation-efficient shape embedding layer for 3D deep learning, named Representation-Agnostic Shape Fields (RASF), to improve performance across different representations, backbones and down-stream tasks</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=-ngwPqanCEZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="C1_esHN6AVn" data-number="31">
        <h4>
          <a href="https://openreview.net/forum?id=C1_esHN6AVn">
              Learning Synthetic Environments and Reward Networks for Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=C1_esHN6AVn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fabio_Ferreira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabio_Ferreira1">Fabio Ferreira</a>, <a href="https://openreview.net/profile?id=~Thomas_Nierhoff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Nierhoff1">Thomas Nierhoff</a>, <a href="https://openreview.net/profile?id=~Andreas_S%C3%A4linger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Sälinger1">Andreas Sälinger</a>, <a href="https://openreview.net/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 17 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">26 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#C1_esHN6AVn-details-745" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="C1_esHN6AVn-details-745"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Synthetic Environments, Synthetic Data, Meta-Learning, Reinforcement Learning, Evolution Strategies, Reward Shaping</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an evolution-based approach to meta-learn synthetic neural environments and reward neural networks for reinforcement learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NH29920YEmj" data-number="30">
        <h4>
          <a href="https://openreview.net/forum?id=NH29920YEmj">
              Who Is Your Right Mixup Partner in Positive and Unlabeled Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NH29920YEmj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Changchun_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changchun_Li1">Changchun Li</a>, <a href="https://openreview.net/profile?id=~Ximing_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ximing_Li1">Ximing Li</a>, <a href="https://openreview.net/profile?id=~Lei_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Feng1">Lei Feng</a>, <a href="https://openreview.net/profile?id=~Jihong_Ouyang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihong_Ouyang2">Jihong Ouyang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NH29920YEmj-details-430" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NH29920YEmj-details-430"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Positive and Unlabeled Learning, Mixup, Heuristic</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Positive and Unlabeled (PU) learning targets inducing a binary classifier from weak training datasets of positive and unlabeled instances, which arise in many real-world applications. In this paper, we propose a novel PU learning method, namely Positive and unlabeled learning with Partially Positive Mixup (P3Mix), which simultaneously benefits from data augmentation and supervision correction with a heuristic mixup technique. To be specific, we take inspiration from the directional boundary deviation phenomenon observed in our preliminary experiments, where the learned PU boundary tends to deviate from the fully supervised boundary towards the positive side. For the unlabeled instances with ambiguous predictive results, we select their mixup partners from the positive instances around the learned PU boundary, so as to transform them into augmented instances near to the boundary yet with more precise supervision. Accordingly, those augmented instances may push the learned PU boundary towards the fully supervised boundary, thereby improving the classification performance. Comprehensive experimental results demonstrate the effectiveness of the heuristic mixup technique in PU learning and show that P3Mix can consistently outperform the state-of-the-art PU learning methods.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel PU learning method named P3Mix which simultaneously benefits from instance augmentation and supervision correction with a heuristic mixup technique.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NH29920YEmj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dDjSKKA5TP1" data-number="25">
        <h4>
          <a href="https://openreview.net/forum?id=dDjSKKA5TP1">
              Incremental False Negative Detection for Contrastive Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dDjSKKA5TP1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tsai-Shien_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsai-Shien_Chen1">Tsai-Shien Chen</a>, <a href="https://openreview.net/profile?id=~Wei-Chih_Hung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei-Chih_Hung1">Wei-Chih Hung</a>, <a href="https://openreview.net/profile?id=~Hung-Yu_Tseng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hung-Yu_Tseng2">Hung-Yu Tseng</a>, <a href="https://openreview.net/profile?id=~Shao-Yi_Chien1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shao-Yi_Chien1">Shao-Yi Chien</a>, <a href="https://openreview.net/profile?id=~Ming-Hsuan_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming-Hsuan_Yang1">Ming-Hsuan Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 16 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dDjSKKA5TP1-details-580" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dDjSKKA5TP1-details-580"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Self-supervised learning, Contrastive learning, Representation learning, Clustering-based learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as "false negatives". In this work, we show that the unfavorable effect from false negatives is more significant for the large-scale datasets with more semantic concepts. To address the issue, we propose a novel self-supervised contrastive learning framework that incrementally detects and explicitly removes the false negative samples. Specifically, following the training process, our method dynamically detects increasing high-quality false negatives considering that the encoder gradually improves and the embedding space becomes more semantically structural. Next, we discuss two strategies to explicitly remove the detected false negatives during contrastive learning. Extensive experiments show that our framework outperforms other self-supervised contrastive learning methods on multiple benchmarks in a limited resource setup.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper explores the effect of false negative samples in self-supervised contrastive learning and introduce a framework to incrementally detect and explicitly remove the false negatives.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rJvY_5OzoI" data-number="14">
        <h4>
          <a href="https://openreview.net/forum?id=rJvY_5OzoI">
              Multi-Critic Actor Learning: Teaching RL Policies to Act with Style
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rJvY_5OzoI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siddharth_Mysore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_Mysore1">Siddharth Mysore</a>, <a href="https://openreview.net/profile?email=gecheng%40ea.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gecheng@ea.com">George Cheng</a>, <a href="https://openreview.net/profile?email=yuzhao%40ea.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuzhao@ea.com">Yunqi Zhao</a>, <a href="https://openreview.net/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kate_Saenko1">Kate Saenko</a>, <a href="https://openreview.net/profile?email=febmeng%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="febmeng@gmail.com">Meng Wu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">23 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rJvY_5OzoI-details-356" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rJvY_5OzoI-details-356"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Multi-Style Learning, Multi-Task Learning, Actor-Critic</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Using a single value function (critic) shared over multiple tasks in Actor-Critic multi-task reinforcement learning (MTRL) can result in negative interference between tasks, which can compromise learning performance. Multi-Critic Actor Learning (MultiCriticAL) proposes instead maintaining separate critics for each task being trained while training a single multi-task actor. Explicitly distinguishing between tasks also eliminates the need for critics to learn to do so and mitigates interference between task-value estimates. MultiCriticAL is tested in the context of multi-style learning, a special case of MTRL where agents are trained to behave with different distinct behavior styles, and yields up to 56% performance gains over the single-critic baselines and even successfully learns behavior styles in cases where single-critic approaches may simply fail to learn. In a simulated real-world use case, MultiCriticAL enables learning policies that smoothly transition between multiple fighting styles on an experimental build of EA’s UFC game.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">MultiCriticAL is a single-actor, multi-critic framework for multi-task reinforcement learning, where task-based critic separation provides explicit per-task value-function approximation and enables improved performance over single-critic frameworks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=rJvY_5OzoI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="kezNJydWvE" data-number="12">
        <h4>
          <a href="https://openreview.net/forum?id=kezNJydWvE">
              Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring
          </a>
        
          
            <a href="https://openreview.net/pdf?id=kezNJydWvE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seungjun_Nah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungjun_Nah1">Seungjun Nah</a>, <a href="https://openreview.net/profile?id=~Sanghyun_Son1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanghyun_Son1">Sanghyun Son</a>, <a href="https://openreview.net/profile?id=~Jaerin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaerin_Lee1">Jaerin Lee</a>, <a href="https://openreview.net/profile?id=~Kyoung_Mu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyoung_Mu_Lee2">Kyoung Mu Lee</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 14 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#kezNJydWvE-details-841" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kezNJydWvE-details-841"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deblur, Reblur, Loss, Test-time adaptation, Self-supervised</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily designed to capture high-level contexts rather than low-level structures such as blurriness. Instead, we propose a more direct way to make images sharper by exploiting the inverse task of deblurring, namely, reblurring. Reblurring amplifies the remaining blur to rebuild the original blur, however, a well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we design two types of reblurring loss functions for better deblurring. The supervised reblurring loss at training stage compares the amplified blur between the deblurred and the sharp images. The self-supervised reblurring loss at inference stage inspects if noticeable blur remains in the deblurred. Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Reblurring, the inverse task of deblurring, is used for supervised/self-supervised learning of deblurring and improves the image sharpness.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=kezNJydWvE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="j-63FSNcO5a" data-number="11">
        <h4>
          <a href="https://openreview.net/forum?id=j-63FSNcO5a">
              Learning Disentangled Representation by Exploiting Pretrained Generative Models:  A Contrastive Learning View
          </a>
        
          
            <a href="https://openreview.net/pdf?id=j-63FSNcO5a" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xuanchi_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanchi_Ren1">Xuanchi Ren</a>, <a href="https://openreview.net/profile?id=~Tao_Yang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Yang9">Tao Yang</a>, <a href="https://openreview.net/profile?id=~Yuwang_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuwang_Wang3">Yuwang Wang</a>, <a href="https://openreview.net/profile?id=~Wenjun_Zeng3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjun_Zeng3">Wenjun Zeng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#j-63FSNcO5a-details-676" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="j-63FSNcO5a-details-676"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Latent space discovery, Disentangled representation learning, Generative models, Contrastive learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">From the intuitive notion of disentanglement, the image variations corresponding to different generative factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the generative factors and learn disentangled representation, previous methods typically leverage an extra regularization term when learning to generate realistic images. However, the term usually results in a trade-off between disentanglement and generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by (i) leveraging the pretrained generative models with high generation quality, (ii) focusing on discovering the traversal directions as generative factors for disentangled representation learning. To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations, and contrast the variations to jointly discover disentangled directions and learn disentangled representations. DisCo achieves the state-of-the-art disentangled representation learning and distinct direction discovering, given pretrained non-disentangled generative models including GAN, VAE, and Flow. Source code is at https://github.com/xrenaa/DisCo.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">DisCo is a new contrastive learning framework to leverage pretrained generative models to jointly learn disentangled representation and discover disentangled directions in the latent space.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="YgPqNctmyd" data-number="10">
        <h4>
          <a href="https://openreview.net/forum?id=YgPqNctmyd">
              Towards Building A Group-based Unsupervised Representation Disentanglement Framework
          </a>
        
          
            <a href="https://openreview.net/pdf?id=YgPqNctmyd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tao_Yang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Yang9">Tao Yang</a>, <a href="https://openreview.net/profile?id=~Xuanchi_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanchi_Ren1">Xuanchi Ren</a>, <a href="https://openreview.net/profile?id=~Yuwang_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuwang_Wang3">Yuwang Wang</a>, <a href="https://openreview.net/profile?id=~Wenjun_Zeng3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjun_Zeng3">Wenjun Zeng</a>, <a href="https://openreview.net/profile?id=~Nanning_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanning_Zheng1">Nanning Zheng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">32 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#YgPqNctmyd-details-642" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YgPqNctmyd-details-642"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Disentangled representation learning, Group theory, VAE</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that their goal can not be achieved without introducing other inductive biases. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the \emph{n-th dihedral group}, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model based on existing VAE-based methods to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With these conditions, we offer an option, from the perspective of the group-based definition, for the inductive bias that existing VAE-based models lack. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, built on the group-based definition and inspired by the n-th dihedral group, we first propose a theoretical framework towards achieving unsupervised representation disentanglement.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="AjGC97Aofee" data-number="9">
        <h4>
          <a href="https://openreview.net/forum?id=AjGC97Aofee">
              Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=AjGC97Aofee" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yulun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulun_Zhang1">Yulun Zhang</a>, <a href="https://openreview.net/profile?id=~Huan_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Wang3">Huan Wang</a>, <a href="https://openreview.net/profile?id=~Can_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Can_Qin1">Can Qin</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun_Fu1">Yun Fu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Mar 2022)</span>
              <span class="item">ICLR 2022 Poster</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#AjGC97Aofee-details-582" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AjGC97Aofee-details-582"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">image super-resolution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Several image super-resolution (SR) networks have been proposed of late for efficient SR, achieving promising results. However, they are still not lightweight enough and neglect to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to ensure the locations of pruned filters are aligned across different layers. Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters. To transfer the expressive power in the unimportant filters to the rest of the network, we employ $L_2$ regularization to drive the weights towards zero so that eventually, their absence will cause minimal performance degradation. We apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-Lite and a very deep one SRPN. We conduct extensive comparisons with both lightweight and larger networks. SRPN-Lite and SRPN perform favorably against other recent efficient SR approaches quantitatively and visually.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning efficient compressed models for bother lightweight and large image super-resolution networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>

















<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="  left-arrow" data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">«</a>
      </li>
      <li class="  left-arrow" data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">‹</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  " data-page-number="11">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">11</a>
      </li>
      <li class="  " data-page-number="12">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">12</a>
      </li>
      <li class="  " data-page-number="13">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">13</a>
      </li>
      <li class="  " data-page-number="14">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">14</a>
      </li>
      <li class="  " data-page-number="15">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">15</a>
      </li>
      <li class="  " data-page-number="16">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">16</a>
      </li>
      <li class="  " data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">17</a>
      </li>
      <li class=" active " data-page-number="18">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">18</a>
      </li>
      <li class="disabled  right-arrow" data-page-number="19">
          <span>›</span>
      </li>
      <li class="disabled  right-arrow" data-page-number="18">
          <span>»</span>
      </li>
  </ul>
</nav>

















</div>
    <div role="tabpanel" class="tab-pane fade  " id="submitted-submissions">
  <form class="form-inline notes-search-form " role="search">

    <div class="form-group search-content has-feedback">
      <input id="paper-search-input" type="text" class="form-control" placeholder="Search by paper title and metadata" autocomplete="off">
      <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
    </div>



    <input type="submit" style="display: none;">
  </form>

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="youe3QQepVB" data-number="4714">
        <h4>
          <a href="https://openreview.net/forum?id=youe3QQepVB">
              Generative Modeling for Multitask Visual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=youe3QQepVB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhipeng_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhipeng_Bao1">Zhipeng Bao</a>, <a href="https://openreview.net/profile?id=~Yu-Xiong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Xiong_Wang1">Yu-Xiong Wang</a>, <a href="https://openreview.net/profile?id=~Martial_Hebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Martial_Hebert1">Martial Hebert</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#youe3QQepVB-details-679" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="youe3QQepVB-details-679"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, especially in the low-data regimes, and our model consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a general multi-task oriented generative modeling (MGM) framework that introduces generative models to facilitate multi-task learning and it consistently outperforms state-of-the-art multi-task approaches.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2aC0_RxkBL_" data-number="4707">
        <h4>
          <a href="https://openreview.net/forum?id=2aC0_RxkBL_">
              Where is the bottleneck in long-tailed classification?
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2aC0_RxkBL_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zaid_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zaid_Khan1">Zaid Khan</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yun_Fu1">Yun Fu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2aC0_RxkBL_-details-584" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2aC0_RxkBL_-details-584"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, bias, long tailed learning, imbalanced learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A commonly held belief in deep-learning based long-tailed classiﬁcation is that the representations learned from long-tailed data are ”good enough” and the performance bottleneck is the classiﬁcation head atop the representation learner. We design experiments to investigate this folk wisdom, and ﬁnd that representations learned from long-tailed data distributions substantially differ from the representations learned from ”normal” data distributions. We show that the long-tailed representations are volatile and brittle with respect to the true data distribution. Compared to the representations learned from the true, balanced distributions, long-tailed representations fail to localize tail classes and display vastly worse inter-class separation and intra-class compactness when unseen samples from the true data distribution are embedded into the feature space. We provide an explanation for why data augmentation helps long-tailed classiﬁcation despite leaving the dataset imbalance unchanged — it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We investigate how learning from long-tailed distributions harms representations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="voEpzgY8gsT" data-number="4704">
        <h4>
          <a href="https://openreview.net/forum?id=voEpzgY8gsT">
              Additive Poisson Process: Learning Intensity of Higher-Order Interaction in Poisson Processes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=voEpzgY8gsT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Simon_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Luo1">Simon Luo</a>, <a href="https://openreview.net/profile?id=~Feng_Zhou9" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Zhou9">Feng Zhou</a>, <a href="https://openreview.net/profile?id=~lamiae_azizi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~lamiae_azizi1">lamiae azizi</a>, <a href="https://openreview.net/profile?id=~Mahito_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mahito_Sugiyama1">Mahito Sugiyama</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#voEpzgY8gsT-details-110" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="voEpzgY8gsT-details-110"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Poisson Process, Log-Linear Model, Energy-Based Model, Generalized Additive Models, Information Geometry</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Additive Poisson Process (APP), a novel framework that can model the higher-order interaction effects of the intensity functions in Poisson processes using projections into lower-dimensional space. Our model combines the techniques in information geometry to model higher-order interactions on a statistical manifold and in generalized additive models to use lower-dimensional projections to overcome the effects from the curse of dimensionality. Our approach solves a convex optimization problem by minimizing the KL divergence from a sample distribution in lower-dimensional projections to the distribution modeled by an intensity function in the Poisson process. Our empirical results show that our model is able to use samples observed in the lower dimensional space to estimate the higher-order intensity function with extremely sparse observations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An efficient technique that uses a log-linear model on a partial order structure to approximate a high-dimensional intensity functions in a Poisson Process.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=voEpzgY8gsT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="qfLJBJf_DnH" data-number="4701">
        <h4>
          <a href="https://openreview.net/forum?id=qfLJBJf_DnH">
              Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=qfLJBJf_DnH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Laureline_Logiaco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Laureline_Logiaco1">Laureline Logiaco</a>, <a href="https://openreview.net/profile?id=~G_Sean_Escola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~G_Sean_Escola1">G Sean Escola</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#qfLJBJf_DnH-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qfLJBJf_DnH-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuroscience, dynamical systems, thalamocortical architecture, motor preparation, continual learning, hierarchical continuous motor control, out-of-distribution generalization, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of learning dynamics that can produce hierarchically organized continuous outputs consisting of the flexible chaining of re-usable motor ‘motifs’ from which complex behavior is generated. Can a motif library be efficiently and extendably learned without interference between motifs, and can these motifs be chained in arbitrary orders without first learning the corresponding motif transitions during training? This requires (i) parameter updates while learning a new motif that do not interfere with the parameters used for the previously acquired ones; and (ii) successful motif generation when starting from the network states reached at the end of any of the other motifs, even if these states were not present during training (a case of out-of-distribution generalization). We meet the first requirement by designing recurrent neural networks (RNNs) with specific architectures that segregate motif-dependent parameters (as customary in continual learning works), and try a standard method to address the second by training with random initial states. We find that these standard RNNs are very unreliable during zero-shot transfer to motif chaining. We then use insights from the motor thalamocortical circuit, featuring a specific module that shapes motif transitions. We develop a method to constrain the RNNs to function similarly to the thalamocortical circuit during motif transitions, while preserving the large expressivity afforded by gradient-based training of non-analytically tractable RNNs. We then show that this thalamocortical inductive bias not only acts in synergy with gradient-descent RNN training to improve accuracy during in-training-distribution motif production, but also leads to zero-shot transfer to new motif chains with no performance cost. Besides proposing an efficient, robust and flexible RNN architecture, our results shed new light on the function of motor preparation in the brain.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Motor preparation in nonlinear RNNs supports robust chaining of accurate continuous motor motifs in never-experienced orders.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rF5UoZFrsF4" data-number="4697">
        <h4>
          <a href="https://openreview.net/forum?id=rF5UoZFrsF4">
              VUT: Versatile UI Transformer for Multimodal Multi-Task User Interface Modeling 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rF5UoZFrsF4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Li2">Yang Li</a>, <a href="https://openreview.net/profile?id=~Gang_Li13" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gang_Li13">Gang Li</a>, <a href="https://openreview.net/profile?id=~Xin_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Zhou3">Xin Zhou</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">9 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rF5UoZFrsF4-details-15" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rF5UoZFrsF4-details-15"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">User Interface Modeling, Multimodal input, Multi-task learning, Transformer, Layout Detection, Language Grounding, Image Captioning, Screen Summarization, Tappability Prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT has achieved accuracy either on par with or exceeding the accuracy when the model is trained for individual tasks separately.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The work addresses unique challenges of multimodal multi-task learning of distinct tasks for user interface modeling.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="3M3t3tUbA2Y" data-number="4686">
        <h4>
          <a href="https://openreview.net/forum?id=3M3t3tUbA2Y">
              DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=3M3t3tUbA2Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fei_Deng1">Fei Deng</a>, <a href="https://openreview.net/profile?id=~Ingook_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingook_Jang1">Ingook Jang</a>, <a href="https://openreview.net/profile?id=~Sungjin_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungjin_Ahn1">Sungjin Ahn</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#3M3t3tUbA2Y-details-757" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3M3t3tUbA2Y-details-757"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based reinforcement learning, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction
        often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions. To address this issue, previous work has proposed to contrastively learn the latent representations and its temporal dynamics, but showed inconsistent performance, often worse than Dreamer. Although, in computer vision, an alternative prototypical approach has often shown to be more accurate and robust, it is elusive how this approach can be combined best with the temporal dynamics learning in MBRL. In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal. Similar to SwAV, by encouraging uniform cluster assignment across the batch, we implicitly push apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details. We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model is consistently better than the previous models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Dy8gq-LuckD" data-number="4680">
        <h4>
          <a href="https://openreview.net/forum?id=Dy8gq-LuckD">
              Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Dy8gq-LuckD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Wu1">Nan Wu</a>, <a href="https://openreview.net/profile?id=~Stanislaw_Kamil_Jastrzebski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stanislaw_Kamil_Jastrzebski1">Stanislaw Kamil Jastrzebski</a>, <a href="https://openreview.net/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kyunghyun_Cho1">Kyunghyun Cho</a>, <a href="https://openreview.net/profile?id=~Krzysztof_J._Geras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krzysztof_J._Geras1">Krzysztof J. Geras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 18 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Dy8gq-LuckD-details-620" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Dy8gq-LuckD-details-620"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-modal learning, deep neural networks, multi-view learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities. We observe empirically that such behavior hurts its overall generalization. We validate our hypothesis by estimating the gain on the accuracy when the model has access to an additional modality. We refer to this gain as the conditional utilization rate of the modality. In the experiments, we consistently observe an imbalance in conditional utilization rate between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. We thus propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the model’s generalization on three datasets: Colored MNIST (Kim et al., 2019), Princeton ModelNet40 (Wu et al., 2015), and NVIDIA Dynamic Hand Gesture Dataset (Molchanov et al., 2016).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UeE41VsK1KJ" data-number="4677">
        <h4>
          <a href="https://openreview.net/forum?id=UeE41VsK1KJ">
              Subjective Learning for Open-Ended Data
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UeE41VsK1KJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tianren_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianren_Zhang1">Tianren Zhang</a>, <a href="https://openreview.net/profile?id=~Yizhou_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yizhou_Jiang1">Yizhou Jiang</a>, <a href="https://openreview.net/profile?id=~Xin_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Su1">Xin Su</a>, <a href="https://openreview.net/profile?id=~Shangqi_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shangqi_Guo2">Shangqi Guo</a>, <a href="https://openreview.net/profile?id=~Feng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Chen1">Feng Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UeE41VsK1KJ-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UeE41VsK1KJ-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Open-ended data, machine learning, supervised learning, data conflict</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional supervised learning typically assumes that the learning task can be solved by learning a single function since the data is sampled from a fixed distribution. However, this assumption is invalid in open-ended environments where no task-level data partitioning is available. In this paper, we present a novel supervised learning framework of learning from open-ended data, which is modeled as data implicitly sampled from multiple domains with the data in each domain obeying a domain-specific target function. Since different domains may possess distinct target functions, open-ended data inherently requires multiple functions to capture all its input-output relations, rendering training a single global model problematic. To address this issue, we devise an Open-ended Supervised Learning (OSL) framework, of which the key component is a subjective function that allocates the data among multiple candidate models to resolve the "conflict'' between the data from different domains, exhibiting a natural hierarchy. We theoretically analyze the learnability and the generalization error of OSL, and empirically validate its efficacy in both open-ended regression and classification tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We formalize the problem of learning from open-ended data that implicitly comes from multiple domains and inherently requires multiple functions to fully capture its input-output relations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UeE41VsK1KJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="2RNpZ8S4alJ" data-number="4675">
        <h4>
          <a href="https://openreview.net/forum?id=2RNpZ8S4alJ">
              KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=2RNpZ8S4alJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alireza_Rezazadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alireza_Rezazadeh1">Alireza Rezazadeh</a>, <a href="https://openreview.net/profile?email=cchoi%40umn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="cchoi@umn.edu">Changhyun Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#2RNpZ8S4alJ-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2RNpZ8S4alJ-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object-centric representation is an essential abstraction for physical reasoning and forward prediction. Most existing approaches learn this representation through extensive supervision (e.g, object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network)---an end-to-end unsupervised framework to reason about object interactions in complex systems based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, and novel object geometries. Experiments demonstrate the effectiveness of our model to accurately perform forward prediction and learn plannable object-centric representations which can also be used in downstream model-based control tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="an_ndI09oVZ" data-number="4667">
        <h4>
          <a href="https://openreview.net/forum?id=an_ndI09oVZ">
              Deep banach space kernels
          </a>
        
          
            <a href="https://openreview.net/pdf?id=an_ndI09oVZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mrityunjay_Bhardwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mrityunjay_Bhardwaj1">Mrityunjay Bhardwaj</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#an_ndI09oVZ-details-653" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="an_ndI09oVZ-details-653"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RKBS, RKHS, concatenated kernel learning, representation learning, deep learning, MLMKL, Deep Gaussian Processes, gaussian processes, kernel machines</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recent success of deep learning has encouraged many researchers to explore the deep/concatenated variants of classical kernel methods. Some of which includes MLMKL, DGP and DKL. Although, These methods have proven to be quite useful in various real-world settings. They still suffer from the limitations of only utilizing kernels from Hilbert spaces. In this paper, we address these shortcomings by introducing a new class of concatenated kernel learning methods that use the kernels from the reproducing kernel Banach spaces(RKBSs). These spaces turned out to be one of the most general spaces where a reproducing Kernel exists. We propose a framework of construction for these Deep RKBS models and then provide a representer theorem for regularized learning problems. We also describe the relationship with its deep RKHS variant as well as standard Deep Gaussian Processes. In the end, we construct and implement a two-layer deep RKBS model and demonstrate it on a range of machine learning tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">a new class of deep kernel methods which uses kernels from reproducing kernel banach spaces (RKBS).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="0Kj5mhn6sw" data-number="4650">
        <h4>
          <a href="https://openreview.net/forum?id=0Kj5mhn6sw">
              Gesture2Vec: Clustering Gestures using  Representation Learning Methods for Co-speech Gesture Generation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=0Kj5mhn6sw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Payam_Jome_Yazdian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_Jome_Yazdian1">Payam Jome Yazdian</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?email=angelica%40sfu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="angelica@sfu.ca">Angelica Lim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#0Kj5mhn6sw-details-898" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0Kj5mhn6sw-details-898"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">representation learning, gesture generation, vector quantization, machine translation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans. Similarly, the co-speech gesture is a key ingredient in human-agent interaction including both virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. Furthermore, we use a machine translation model that takes input text and translates it into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated quantized gestures from the input text as an input to the autoencoder’s decoder to produce gesture sequences. The resulting gestures can be applied to both virtual agents and humanoid robots. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=0Kj5mhn6sw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="WXy4C-RjET" data-number="4644">
        <h4>
          <a href="https://openreview.net/forum?id=WXy4C-RjET">
              Logit Attenuating Weight Normalization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=WXy4C-RjET" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aman_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aman_Gupta1">Aman Gupta</a>, <a href="https://openreview.net/profile?id=~Rohan_Ramanath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Ramanath1">Rohan Ramanath</a>, <a href="https://openreview.net/profile?id=~Jun_Shi4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jun_Shi4">Jun Shi</a>, <a href="https://openreview.net/profile?id=~Anika_Ramachandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anika_Ramachandran1">Anika Ramachandran</a>, <a href="https://openreview.net/profile?id=~SIROU_ZHU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SIROU_ZHU1">SIROU ZHU</a>, <a href="https://openreview.net/profile?id=~Mingzhou_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mingzhou_Zhou1">Mingzhou Zhou</a>, <a href="https://openreview.net/profile?id=~Sathiya_Keerthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sathiya_Keerthi1">Sathiya Keerthi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#WXy4C-RjET-details-185" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WXy4C-RjET-details-185"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning, gradient methods, stochastic optimization, generalization gap, imagenet, adam, large batch training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over-parameterized deep networks trained using gradient-based optimizers is a popular way of solving classification and ranking problems. Without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that, even with weight decay (WD) and normal hyper-parameter tuning, adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain.
        
        An alternative to WD for improving a network's adaptivity is to directly control the magnitude of the weights and hence the logits. We propose a method called Logit Attenuating Weight Normalization (LAWN), that can be stacked onto any gradient-based optimizer.  LAWN initially starts off training in a free (unregularized) mode and, after some initial epochs, it constrains the weight norms of layers, thereby controlling the logits and improving adaptivity. This is a new regularization approach that does not use WD anywhere; instead, the number of initial free epochs becomes the new hyper-parameter. The resulting LAWN variant of adaptive optimizers gives a solid lift to generalization performance, making their performance equal or even exceed SGD's performance on benchmark image classification and recommender datasets. Another important feature is that LAWN also greatly improves the adaptive optimizers when used with large batch sizes.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">An optimizer for deep learning called Logit Attenuating Weight Normalization (LAWN) for superior generalization performance and scaling to large batches</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=WXy4C-RjET&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxBH9j8IE_H" data-number="4643">
        <h4>
          <a href="https://openreview.net/forum?id=UxBH9j8IE_H">
              Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxBH9j8IE_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~BITHIKA_PAL1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BITHIKA_PAL1">BITHIKA PAL</a>, <a href="https://openreview.net/profile?id=~Arindam_Biswas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arindam_Biswas1">Arindam Biswas</a>, <a href="https://openreview.net/profile?id=~Pabitra_Mitra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pabitra_Mitra1">Pabitra Mitra</a>, <a href="https://openreview.net/profile?id=~BISWAJIT_BASU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~BISWAJIT_BASU1">BISWAJIT BASU</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxBH9j8IE_H-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxBH9j8IE_H-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Neural Networks, Network Pruning, Ramanujan Graphs, Eigenvalue bounds, Spectral Gap</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A Ramanujan graph perspective to explain the lottery ticket hypothesis</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a0SRWViFYW" data-number="4642">
        <h4>
          <a href="https://openreview.net/forum?id=a0SRWViFYW">
              Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a0SRWViFYW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Patrick_R._Johnstone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_R._Johnstone1">Patrick R. Johnstone</a>, <a href="https://openreview.net/profile?id=~Jonathan_Eckstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Eckstein1">Jonathan Eckstein</a>, <a href="https://openreview.net/profile?id=~Thomas_Flynn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Flynn1">Thomas Flynn</a>, <a href="https://openreview.net/profile?id=~Shinjae_Yoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinjae_Yoo1">Shinjae Yoo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a0SRWViFYW-details-897" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a0SRWViFYW-details-897"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">convex optimization, min-max games, saddle-point problems, first-order stochastic methods, proximal methods, operator splitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new, stochastic variant of the projective splitting (PS) family of algorithms for monotone inclusion problems.  It can solve min-max and noncooperative game formulations arising in applications such as robust ML without the convergence issues associated with gradient descent-ascent, the current de facto standard approach in ML applications.  Our proposal is the first version of PS able to use stochastic gradient oracles. It can solve min-max games while handling multiple constraints and nonsmooth regularizers via projection and proximal operators. Unlike other stochastic splitting methods that can solve such problems, our method does not rely on a product-space reformulation of the original problem. We prove almost-sure convergence of the iterates to the solution and a convergence rate for the expected residual.  By working with monotone inclusions rather than variational inequalities, our analysis avoids the drawbacks of measuring convergence through the restricted gap function. We close with numerical experiments on a distributionally robust sparse logistic regression problem.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop a stochastic splitting method that can easily handle min-max problems with multiple regularizers and constraints</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=a0SRWViFYW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_K6rwRjW9WO" data-number="4637">
        <h4>
          <a href="https://openreview.net/forum?id=_K6rwRjW9WO">
              RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_K6rwRjW9WO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Victor_Quintas-Martinez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Quintas-Martinez1">Victor Quintas-Martinez</a>, <a href="https://openreview.net/profile?id=~Victor_Chernozhukov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Victor_Chernozhukov1">Victor Chernozhukov</a>, <a href="https://openreview.net/profile?id=~Vasilis_Syrgkanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vasilis_Syrgkanis1">Vasilis Syrgkanis</a>, <a href="https://openreview.net/profile?id=~Whitney_Newey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Whitney_Newey1">Whitney Newey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_K6rwRjW9WO-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_K6rwRjW9WO-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="75" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.281em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></mjx-assistive-mml></mjx-container>-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, that is derived based on a functional-specific theoretical derivation of what is known as the influence function and which leads to properties such as double robustness and Neyman orthogonality. We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method solely requires value query oracle access to the linear functional. We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Reisz representer and regression loss, while sharing representation layers for the two functions. We also propose a random forest method which learns a locally linear representation of the Reisz function. Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We implement an automatic debiasing procedure for causal and policy effects based on automatically learning their corresponding Riesz representation, using Neural Nets and Random Forests.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_K6rwRjW9WO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Mo9R9oqzPo" data-number="4636">
        <h4>
          <a href="https://openreview.net/forum?id=Mo9R9oqzPo">
              New Definitions and Evaluations for Saliency Methods: Staying Intrinsic and Sound
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Mo9R9oqzPo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arushi_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arushi_Gupta1">Arushi Gupta</a>, <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Dingli_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dingli_Yu1">Dingli Yu</a>, <a href="https://openreview.net/profile?id=~Kaifeng_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaifeng_Lyu2">Kaifeng Lyu</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sanjeev_Arora1">Sanjeev Arora</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Mo9R9oqzPo-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mo9R9oqzPo-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">saliency, masking based methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Saliency methods seek to provide human-interpretable explanations for the output of machine learning model on a given input. A plethora of saliency methods exist, as well as an extensive literature on their justifications/criticisms/evaluations. This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans. It tries to introduce methods and evaluations for masked-based saliency methods that are {\em intrinsic} --- use just the training dataset and the trained net, and do not use separately trained nets, distractor distributions, human evaluations or annotations. Since a mask can be seen as a "certificate" justifying the net's answer, we introduce notions of {\em completeness} and {\em soundness} (the latter being the new contribution) motivated by logical proof systems. These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.V. regularization, upscaling). </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mo9R9oqzPo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="GthNKCqdDg" data-number="4623">
        <h4>
          <a href="https://openreview.net/forum?id=GthNKCqdDg">
              Selective Token Generation for Few-shot Language Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=GthNKCqdDg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Daejin_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daejin_Jo1">Daejin Jo</a>, <a href="https://openreview.net/profile?id=~Taehwan_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Taehwan_Kwon1">Taehwan Kwon</a>, <a href="https://openreview.net/profile?id=~Sungwoong_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sungwoong_Kim2">Sungwoong Kim</a>, <a href="https://openreview.net/profile?id=~Eun-Sol_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eun-Sol_Kim1">Eun-Sol Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#GthNKCqdDg-details-571" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GthNKCqdDg-details-571"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Generation, Reinforcement Learning, Few-shot Learning, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among these transfer learning algorithms from PLMs, additive learning that incorporates a task-specific adapter on top of the fixed PLM has been popularly used to alleviate the severe overfitting problem in the few-shot setting. However, this added task-specific adapter is generally trained by maximum likelihood estimation that can easily suffer from the so-called exposure bias problem, especially in sequential text generation. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) for few-shot natural language generation (NLG) tasks. In particular, we propose to use a selective token generation between the transformer-based PLM and the task-specific adapter during both training and inference. This output token selection between the two generators allows the adapter to take into account only on the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, in order to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="INO8hGXD2M" data-number="4614">
        <h4>
          <a href="https://openreview.net/forum?id=INO8hGXD2M">
              Adversarial Distributions Against Out-of-Distribution Detectors
          </a>
        
          
            <a href="https://openreview.net/pdf?id=INO8hGXD2M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sangwoong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sangwoong_Yoon1">Sangwoong Yoon</a>, <a href="https://openreview.net/profile?id=~Jinwon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinwon_Choi1">Jinwon Choi</a>, <a href="https://openreview.net/profile?id=~Yonghyeon_LEE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yonghyeon_LEE1">Yonghyeon LEE</a>, <a href="https://openreview.net/profile?id=~Yung-Kyun_Noh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yung-Kyun_Noh1">Yung-Kyun Noh</a>, <a href="https://openreview.net/profile?id=~Frank_C._Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_C._Park1">Frank C. Park</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#INO8hGXD2M-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="INO8hGXD2M-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">out-of-distribution detection, outlier detection, adversarial attack, model evaluation, markov chain monte carlo</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution. As an outlier may deviate from the training distribution in unexpected ways, an ideal OOD detector should be able to detect all types of outliers. However, current evaluation protocols test a detector over OOD datasets that cover only a small fraction of all possible outliers, leading to overly optimistic views of OOD detector performance.  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers.  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector. Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a novel evaluation method for out-of-distribution detectors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eOdSD0B5TE" data-number="4606">
        <h4>
          <a href="https://openreview.net/forum?id=eOdSD0B5TE">
              On the Implicit Biases of Architecture &amp; Gradient Descent
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eOdSD0B5TE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jeremy_Bernstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeremy_Bernstein1">Jeremy Bernstein</a>, <a href="https://openreview.net/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisong_Yue1">Yisong Yue</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">20 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eOdSD0B5TE-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eOdSD0B5TE-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalisation, function space, PAC-Bayes, NNGP, orthants, margin</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Do neural networks generalise because of bias in the functions returned by gradient descent, or bias already present in the network architecture? <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="76" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">¿</mjx-utext><mjx-c class="mjx-c1D443 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45E TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">é</mjx-utext><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c3F TEX-MI"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">¿Por qué no los dos?</mtext></math></mjx-assistive-mml></mjx-container> This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent. To measure the implicit bias of architecture, new technical tools are developed to both <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="77" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">analytically bound</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="78" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">consistently estimate</mtext></math></mjx-assistive-mml></mjx-container> the average test error of the neural network--Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-Pérez et al. (2019) and underscoring the importance of architecture. Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior. This highlights a curious fact: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="79" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">minimum a posteriori</mtext></math></mjx-assistive-mml></mjx-container> functions can generalise best, and gradient descent can select for those functions. In summary, new technical tools suggest a nuanced portrait of generalisation involving both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">New technical tools suggest a nuanced portrait of generalisation that involves both the implicit biases of architecture and gradient descent.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=eOdSD0B5TE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vPK-G5HbnWg" data-number="4603">
        <h4>
          <a href="https://openreview.net/forum?id=vPK-G5HbnWg">
              PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vPK-G5HbnWg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zehao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zehao_Dong1">Zehao Dong</a>, <a href="https://openreview.net/profile?id=~Muhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhan_Zhang1">Muhan Zhang</a>, <a href="https://openreview.net/profile?id=~Fuhai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fuhai_Li1">Fuhai Li</a>, <a href="https://openreview.net/profile?id=~Yixin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yixin_Chen1">Yixin Chen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vPK-G5HbnWg-details-659" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vPK-G5HbnWg-details-659"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DAG encoder, graph neural network, Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optimization of directed acyclic graph (DAG) structures has many applications, such as neural architecture search (NAS) and probabilistic graphical model learning. Encoding DAGs into real vectors is a dominant component in most neural-network-based DAG optimization frameworks. Currently, most popular DAG encoders use an asynchronous message passing scheme which sequentially processes nodes according to the dependency between nodes in a DAG. That is, a node must not be processed until all its predecessors are processed. As a result, they are inherently not parallelizable. In this work, we propose a Parallelizable Attention-based Computation structure Encoder (PACE) that processes nodes simultaneously and encodes DAGs in parallel. We demonstrate the superiority of PACE through  encoder-dependent optimization subroutines that search the optimal DAG structure based on the learned DAG embeddings. Experiments show that PACE not only improves the effectiveness over previous sequential DAG encoders with a significantly boosted training and inference speed, but also generates smooth latent (DAG encoding) spaces that are beneficial to downstream optimization subroutines.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper introduces a novel DAG encoder based on Transformer to encode the computation structure defined by DAGs in a fully parallelizable manner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=vPK-G5HbnWg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="hEiwVblq4P" data-number="4602">
        <h4>
          <a href="https://openreview.net/forum?id=hEiwVblq4P">
              Proper Straight-Through Estimator: Breaking symmetry promotes convergence to true minimum
          </a>
        
          
            <a href="https://openreview.net/pdf?id=hEiwVblq4P" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shinya_Gongyo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shinya_Gongyo1">Shinya Gongyo</a>, <a href="https://openreview.net/profile?id=~Kohta_Ishikawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kohta_Ishikawa1">Kohta Ishikawa</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#hEiwVblq4P-details-792" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hEiwVblq4P-details-792"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">quantization, binary network, low bit network, Straight through estimator, STE</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the quantized network, its gradient shows either vanishing or diverging. The network thus cannot be learned by the standard back-propagation, so that an alternative approach called Straight Through Estimator (STE), which replaces the part of the gradient with a simple differentiable function, is used. While STE is known to work well for learning the quantized network empirically, it has not been established theoretically. A recent study by Yin et. al. (2019) has provided theoretical support for STE. However, its justification is still limited to the model in the one-hidden layer network with the binary activation where  Gaussian generates the input data, and the true labels are output from the teacher network with the same binary network architecture. In this paper, we discuss the effectiveness of STEs in more general situations without assuming the shape of the input distribution and the labels. By considering the scale symmetry of the network and specific properties of the STEs, we find that STE with clipped Relu is superior to STEs with identity function and vanilla Relu. The clipped Relu STE, which breaks the scale symmetry, may pick up one of the local minima degenerated in scales, while the identity STE and vanilla Relu STE, which keep the scale symmetry, may not pick it up. To confirm this observation, we further present an analysis of a simple misspecified model as an example. We find that all the stationary points are identical with the vanishing points of the cRelu STE gradient, while some of them are not identical with the vanishing points of the identity and Relu STE.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We discuss breaking symmetry embedded in the network by Straight through estimators enhances the possibility of convergence to the true minimum.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="W6BpshgRi0q" data-number="4599">
        <h4>
          <a href="https://openreview.net/forum?id=W6BpshgRi0q">
              Ask2Mask: Guided Data Selection for Masked Speech Modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=W6BpshgRi0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Murali_Karthick_Baskar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Murali_Karthick_Baskar1">Murali Karthick Baskar</a>, <a href="https://openreview.net/profile?id=~Andrew_Rosenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrew_Rosenberg1">Andrew Rosenberg</a>, <a href="https://openreview.net/profile?id=~Bhuvana_Ramabhadran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhuvana_Ramabhadran2">Bhuvana Ramabhadran</a>, <a href="https://openreview.net/profile?id=~Yu_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zhang2">Yu Zhang</a>, <a href="https://openreview.net/profile?email=pedro%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="pedro@google.com">Pedro Moreno</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#W6BpshgRi0q-details-97" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W6BpshgRi0q-details-97"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM), Data selection, Self-supervision, ASR, Speech recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work,  we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training.  ATM employs an external ASR model or \textit{scorer} to weight unsupervised input samples in two different ways: 1) A fine-grained data selection is performed by masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. 2) ATM is further extended to focus at utterance-level by weighting the final MSM loss with the utterance-level confidence score.  We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and AMI (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6\% relative) while still yielding modest improvements under matched conditions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Data selection Approach for masked speech model to focus on relevant samples to learn meaningful speech representations</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZWykq5n4zx" data-number="4598">
        <h4>
          <a href="https://openreview.net/forum?id=ZWykq5n4zx">
              Boosting the Confidence of Near-Tight Generalization Bounds for Uniformly Stable Randomized Algorithms
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZWykq5n4zx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaotong_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaotong_Yuan1">Xiaotong Yuan</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 15 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">11 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZWykq5n4zx-details-544" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZWykq5n4zx-details-544"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uniform stability, Randomized learning algorithms, Bagging, Generalization bounds, Stochastic gradient methods</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">High probability generalization bounds of uniformly stable learning algorithms have recently been actively studied with a series of near-tight results established by~\citet{feldman2019high,bousquet2020sharper}. However, for randomized algorithms with on-average uniform stability, such as stochastic gradient descent (SGD) with time decaying learning rates, it still remains less well understood if these deviation bounds still hold with high confidence over the internal randomness of algorithm. This paper addresses this open question and makes progress towards answering it inside a classic framework of confidence-boosting. To this end, we first establish an in-expectation first moment generalization error bound for randomized learning algorithm with on-average uniform stability, based on which we then show that a properly designed subbagging process leads to near-tight high probability generalization bounds over the randomness of data and algorithm. We further substantialize these generic results to SGD to derive improved high probability generalization bounds for convex or non-convex optimization with natural time decaying learning rates, which have not been possible to prove with the existing uniform stability results. Specially for deterministic uniformly stable algorithms, our confidence-boosting results improve upon the best known generalization bounds in terms of a logarithmic factor on sample size, which moves a step forward towards resolving an open question raised by~\citet{bousquet2020sharper}.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A confidence-boosting method for deriving near-tight generalization bounds with high probability for uniformly stable randomized learning algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="EhwEUb2ynIa" data-number="4591">
        <h4>
          <a href="https://openreview.net/forum?id=EhwEUb2ynIa">
              How to Adapt Your Large-Scale Vision-and-Language Model
          </a>
        
          
            <a href="https://openreview.net/pdf?id=EhwEUb2ynIa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Konwoo_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Konwoo_Kim1">Konwoo Kim</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepak_Pathak1">Deepak Pathak</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#EhwEUb2ynIa-details-550" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EhwEUb2ynIa-details-550"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transfer learning, fine-tuning, layernorm, CLIP, prompt-tuning, adaptation, zero-shot, pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training large-scale vision and language models (e.g. CLIP) has shown promising results in representation and transfer learning. We investigate the question of how to efficiently adapt these models to downstream tasks. For image classification, linear probes have been the standard for ease of use and efficiency, while for language, other approaches like prompt tuning have emerged. We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one. We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board. We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks. Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models. Website at https://sites.google.com/view/adapt-large-scale-models</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a thorough analysis of different methods on how to adapt large-scale pretrained vision-and-language models to several downstream classification tasks, and find that just tuning LayerNorm is an effective fine-tuning baseline.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zLb9oSWy933" data-number="4583">
        <h4>
          <a href="https://openreview.net/forum?id=zLb9oSWy933">
              Fast Finite Width Neural Tangent Kernel
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zLb9oSWy933" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Roman_Novak2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roman_Novak2">Roman Novak</a>, <a href="https://openreview.net/profile?id=~Jascha_Sohl-Dickstein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jascha_Sohl-Dickstein2">Jascha Sohl-Dickstein</a>, <a href="https://openreview.net/profile?id=~Samuel_Stern_Schoenholz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samuel_Stern_Schoenholz1">Samuel Stern Schoenholz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">29 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zLb9oSWy933-details-277" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zLb9oSWy933-details-277"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Tangent Kernel, NTK, Finite Width, Fast, Algorithm, JAX, Jacobian, Software</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="80" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-msup><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2F TEX-S1"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D715"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.577em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="normal">Θ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>∂</mi><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>∂</mi><mi>θ</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mi>∂</mi><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em" fence="true" stretchy="true" symmetric="true">/</mo></mrow><mi>∂</mi><mi>θ</mi><mo data-mjx-texclass="CLOSE">]</mo></mrow><mi>T</mi></msup></math></mjx-assistive-mml></mjx-container>, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite-width NTK is notoriously expensive to compute, which severely limits its practical utility. 
        
        We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. 
        Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency.
        
        We open-source (https://github.com/iclr2022anon/fast_finite_width_ntk) our two algorithms as general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc.) and introduce no new hyper-parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We develop and open-source a new algorithm for fast computation of the finite width Neural Tangent Kernel, the outer product of Jacobians of a neural network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zLb9oSWy933&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zaALYtvbRlH" data-number="4574">
        <h4>
          <a href="https://openreview.net/forum?id=zaALYtvbRlH">
              SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zaALYtvbRlH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Peng_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peng_Qi1">Peng Qi</a>, <a href="https://openreview.net/profile?id=~Guangtao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guangtao_Wang1">Guangtao Wang</a>, <a href="https://openreview.net/profile?id=~Jing_Huang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jing_Huang3">Jing Huang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zaALYtvbRlH-details-272" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zaALYtvbRlH-details-272"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sequential data, sample efficiency, data augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=zaALYtvbRlH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PC8u74o7xc2" data-number="4571">
        <h4>
          <a href="https://openreview.net/forum?id=PC8u74o7xc2">
              Embedding models through the lens of Stable Coloring
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PC8u74o7xc2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Aditya_Desai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aditya_Desai1">Aditya Desai</a>, <a href="https://openreview.net/profile?id=~Shashank_Sonkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shashank_Sonkar1">Shashank Sonkar</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Baraniuk1">Richard Baraniuk</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 19 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PC8u74o7xc2-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PC8u74o7xc2-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Embedding-based approaches find the semantic meaning of tokens in structured data such as natural language, graphs, and even images. To a great degree, these approaches have developed independently in different domains. However, we find a common principle underlying these formulations, and it is rooted in solutions to the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For instance, we find links between stable coloring, distribution hypothesis in natural language processing, and non-local-means denoising algorithm in image signal processing. We even find that stable coloring has strong connections to a broad class of unsupervised embedding models which is surprising at first since stable coloring is generally applied for combinatorial problems. To establish this connection concretely we define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them. Grounded on this framework, we show that many algorithms ranging across different domains are, in fact, searching for continuous stable coloring solutions of an underlying graph corresponding to the domain.  We show that popular and widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and Vis-Transformer can be understood  as instantiations of our general algorithm that solves the problem of continuous stable coloring. These instantiations offer useful insights into the workings of state-of-the-art models like BERT stimulating new research directions.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose unified theoretical framework underlying the state-of-the art embedding models</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XIZaWGCPl0b" data-number="4564">
        <h4>
          <a href="https://openreview.net/forum?id=XIZaWGCPl0b">
              Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XIZaWGCPl0b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Atul_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Atul_Sharma1">Atul Sharma</a>, <a href="https://openreview.net/profile?id=~Wei_Chen26" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Chen26">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Joshua_Christian_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_Christian_Zhao1">Joshua Christian Zhao</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qiang_Qiu1">Qiang Qiu</a>, <a href="https://openreview.net/profile?id=~Somali_Chaterji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Somali_Chaterji1">Somali Chaterji</a>, <a href="https://openreview.net/profile?id=~Saurabh_Bagchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Bagchi1">Saurabh Bagchi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 24 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">33 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XIZaWGCPl0b-details-574" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XIZaWGCPl0b-details-574"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, aggregation, security, untargeted model poisoning attack</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learning—multi-party, distributed learning in a decentralized environment—is vulnerable to model poisoning attacks, even more so than centralized learning approaches.  This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Trimmed mean, and FoolsGold.  However, a recently developed targeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be pushed away from the optima to increase the test error rate. In this work, we develop tesseract—a defense against this directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT is based on a simple intuition that in a federated learning setting, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that TESSERACT provides robustness against even an adaptive white-box version of the attack.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">How to defend federated learning against local model poisoning attack, the most effective attack known to date, using the pattern of progression of gradients as each client learns.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XIZaWGCPl0b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="rX3rZYP8zZF" data-number="4561">
        <h4>
          <a href="https://openreview.net/forum?id=rX3rZYP8zZF">
              CareGraph: A Graph-based Recommender System for Diabetes Self-Care
          </a>
        
          
            <a href="https://openreview.net/pdf?id=rX3rZYP8zZF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sirinart_Tangruamsub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sirinart_Tangruamsub1">Sirinart Tangruamsub</a>, <a href="https://openreview.net/profile?id=~Karthik_Kappaganthu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Kappaganthu1">Karthik Kappaganthu</a>, <a href="https://openreview.net/profile?email=jodonovan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="jodonovan@teladochealth.com">John O'Donovan</a>, <a href="https://openreview.net/profile?email=anmol.madan%40teladochealth.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="anmol.madan@teladochealth.com">Anmol Madan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#rX3rZYP8zZF-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rX3rZYP8zZF-details-370"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graph, knowledge graph embedding, recommendation system</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management.  We propose a Deep Neural Network-based recommender that uses the knowledge graph embeddings to recommend health nudges for maximizing engagement by combating the cold-start and sparsity problems. We use a leave-one-out approach to evaluate the model. We compare the proposed model performance with a text similarity and Deep-and-Cross Network-based approach as the baseline. The overall improvement in Click-Through-Rate prediction AUC for the Knowledge-Graph-based model was 11%. We also observe that our model improved the average AUC by 5% in cold-start situations. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nct9j3BVswZ" data-number="4558">
        <h4>
          <a href="https://openreview.net/forum?id=Nct9j3BVswZ">
              Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nct9j3BVswZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Sercan_O_Arik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sercan_O_Arik1">Sercan O Arik</a>, <a href="https://openreview.net/profile?id=~Chen-Yu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chen-Yu_Lee2">Chen-Yu Lee</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tomas_Pfister1">Tomas Pfister</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nct9j3BVswZ-details-45" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nct9j3BVswZ-details-45"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Anomaly detection, Data refinement, Iterative training</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classification method by 6.3 AUC and 12.5 average precision / 22.9 F1-score.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QKEkEFpKBBv" data-number="4557">
        <h4>
          <a href="https://openreview.net/forum?id=QKEkEFpKBBv">
              DNBP: Differentiable Nonparametric Belief Propagation
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QKEkEFpKBBv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anthony_Opipari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anthony_Opipari1">Anthony Opipari</a>, <a href="https://openreview.net/profile?id=~Jana_Pavlasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jana_Pavlasek1">Jana Pavlasek</a>, <a href="https://openreview.net/profile?email=joecc%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="joecc@umich.edu">Chao Chen</a>, <a href="https://openreview.net/profile?email=shoutian%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shoutian@umich.edu">Shoutian Wang</a>, <a href="https://openreview.net/profile?id=~Karthik_Desingh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karthik_Desingh1">Karthik Desingh</a>, <a href="https://openreview.net/profile?id=~Odest_Jenkins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Odest_Jenkins1">Odest Jenkins</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QKEkEFpKBBv-details-265" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QKEkEFpKBBv-details-265"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Belief Propagation, Bayesian Inference, Nonparametric Inference</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. Existing nonparametric belief propagation methods rely on domain-specific features encoded in the probabilistic factors of a graphical model. In this work, we replace each crafted factor with a differentiable neural network enabling the factors to be learned using an efficient optimization routine from labeled data. By combining differentiable neural networks with an efficient belief propagation algorithm, our method learns to maintain a set of marginal posterior samples using end-to-end training. We evaluate our differentiable nonparametric belief propagation (DNBP) method on a set of articulated pose tracking tasks and compare performance with learned baselines. Results from these experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches. The project webpage is available at: https://sites.google.com/view/diff-nbp</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=QKEkEFpKBBv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="TEt7PsVZux6" data-number="4540">
        <h4>
          <a href="https://openreview.net/forum?id=TEt7PsVZux6">
              I-PGD-AT: Efficient Adversarial Training via Imitating Iterative PGD Attack 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=TEt7PsVZux6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xiaosen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaosen_Wang1">Xiaosen Wang</a>, <a href="https://openreview.net/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>, <a href="https://openreview.net/profile?id=~Krishnaram_Kenthapadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Krishnaram_Kenthapadi1">Krishnaram Kenthapadi</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#TEt7PsVZux6-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TEt7PsVZux6-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Single-step Adversarial Training, Catastrophic Overfitting, Adversarial Robustness, Adversarial Example</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial training has been widely used in various machine learning paradigms to improve the robustness; while it would increase the training cost due to the perturbation optimization process. To improve the efficiency, recent studies leverage Fast Gradient Sign Method with Random Start (FGSM-RS) for adversarial training. However, such methods would lead to relatively low robustness and catastrophic overfitting, which means the robustness against iterative attacks (e.g. Projected Gradient Descent (PGD)) would suddenly drop to 0%. Different approaches have been proposed to address this problem, while later studies show that catastrophic overfitting still remains. In this paper, motivated by the fact that expensive iterative adversarial training methods achieve high robustness without catastrophic overfitting, we aim to ask: Can we perform iterative adversarial training in an efficient way? To this end, we first analyze the difference of perturbation generated by FGSM-RS and PGD and find that PGD tends to craft diverse discrete values instead of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="81" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>±</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> in FGSM-RS. Based on this observation, we propose an efficient single-step adversarial training method I-PGD-AT by adopting I-PGD attack for training, in which I-PGD imitates PGD virtually. Unlike FGSM that crafts the perturbation directly using the sign of gradient, I-PGD imitates the perturbation of PGD based on the magnitude of gradient. Extensive empirical evaluations on CIFAR-10 and Tiny ImageNet demonstrate that our I-PGD-AT can improve the robustness compared with the baselines and significantly delay catastrophic overfitting. Moreover, we explore and discuss the factors that affect catastrophic overfitting. Finally, to demonstrate the generality of I-PGD-AT, we integrate it into PGD adversarial training and show that it can even further improve the robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an efficient adversarial training approach I-PGD-AT by imitating PGD virtually to improve single-step adversarial training effectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=TEt7PsVZux6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="wQ7RCayXUSl" data-number="4539">
        <h4>
          <a href="https://openreview.net/forum?id=wQ7RCayXUSl">
              Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters.
          </a>
        
          
            <a href="https://openreview.net/pdf?id=wQ7RCayXUSl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Seyed_Kamyar_Seyed_Ghasemipour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seyed_Kamyar_Seyed_Ghasemipour1">Seyed Kamyar Seyed Ghasemipour</a>, <a href="https://openreview.net/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ofir_Nachum1">Ofir Nachum</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">21 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#wQ7RCayXUSl-details-874" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQ7RCayXUSl-details-874"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline reinforcement learning, batch reinforcement learning, ensembles, uncertainty estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order to achieve strong performance in offline reinforcement learning (RL),  it is necessary to act conservatively with respect to confident lower-bounds on anticipated values of actions. Thus, a valuable approach would be to obtain high quality uncertainty estimates on action values. In current supervised learning literature, state-of-the-art approaches to uncertainty estimation and calibration rely on ensembling methods. In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL. We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble. Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods. Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains. In the more challenging domains of the D4RL offline RL benchmark, MSG significantly surpasses highly well-tuned state-of-the-art methods in batch RL. Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective. We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles. We hope our work engenders increased focus into deep network uncertainty estimation techniques directed for reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We demonstrate how significantly beneficial uncertainty estimation through ensembles can be for offline RL and demonstrate much work is still needed for efficient ensembles to be as effective as deep ensembles.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQ7RCayXUSl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="E9e18Ms5TeV" data-number="4537">
        <h4>
          <a href="https://openreview.net/forum?id=E9e18Ms5TeV">
              A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes
          </a>
        
          
            <a href="https://openreview.net/pdf?id=E9e18Ms5TeV" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zachary_Nado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zachary_Nado1">Zachary Nado</a>, <a href="https://openreview.net/profile?id=~Justin_Gilmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Justin_Gilmer1">Justin Gilmer</a>, <a href="https://openreview.net/profile?id=~Christopher_J_Shallue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christopher_J_Shallue1">Christopher J Shallue</a>, <a href="https://openreview.net/profile?id=~Rohan_Anil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohan_Anil1">Rohan Anil</a>, <a href="https://openreview.net/profile?id=~George_Edward_Dahl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~George_Edward_Dahl1">George Edward Dahl</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#E9e18Ms5TeV-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E9e18Ms5TeV-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural networks, deep learning, neural network optimization, hyperparameter tuning, optimizer comparison</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We retune the Nesterov/Adam optimizers on pipelines where LARS/LAMB are commonly used and achieve similar or better performance, providing competitive baselines for the large batch training setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=E9e18Ms5TeV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="5ueTHF0yAlZ" data-number="4533">
        <h4>
          <a href="https://openreview.net/forum?id=5ueTHF0yAlZ">
              Improving greedy core-set configurations for active learning with uncertainty-scaled distances
          </a>
        
          
            <a href="https://openreview.net/pdf?id=5ueTHF0yAlZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yuchen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuchen_Li1">Yuchen Li</a>, <a href="https://openreview.net/profile?id=~Frank_Rudzicz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frank_Rudzicz2">Frank Rudzicz</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#5ueTHF0yAlZ-details-478" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5ueTHF0yAlZ-details-478"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Active learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We scale perceived distances of the core-set algorithm by a factor of uncertainty and search for low-confidence configurations, finding significant improvements in sample efficiency across CIFAR10/100 and SVHN image classification, especially in larger acquisition sizes. We show the necessity of our modifications and explain how the improvement is due to a probabilistic quadratic speed-up in the convergence of core-set loss, under assumptions about the relationship of model uncertainty and misclassification.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Improved core-set for active learning using confidence-scaled distances.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=5ueTHF0yAlZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Nus6fOfh1HW" data-number="4531">
        <h4>
          <a href="https://openreview.net/forum?id=Nus6fOfh1HW">
              On the Relationship between Heterophily and Robustness of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Nus6fOfh1HW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiong_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiong_Zhu1">Jiong Zhu</a>, <a href="https://openreview.net/profile?id=~Junchen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junchen_Jin1">Junchen Jin</a>, <a href="https://openreview.net/profile?id=~Donald_Loveland2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Donald_Loveland2">Donald Loveland</a>, <a href="https://openreview.net/profile?id=~Michael_T_Schaub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_T_Schaub1">Michael T Schaub</a>, <a href="https://openreview.net/profile?id=~Danai_Koutra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danai_Koutra1">Danai Koutra</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">24 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Nus6fOfh1HW-details-825" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nus6fOfh1HW-details-825"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph neural networks, adversarial attacks, heterophily, structural perturbation, robustness, relation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels). In this work, we formalize the relation between heterophily and robustness, bridging two topics previously investigated by separate lines of research. We theoretically and empirically show that for graphs exhibiting homophily (low heterophily), impactful structural attacks always lead to increased levels of heterophily, while for graph with heterophily the change in the homophily level depends on the node degrees. By leveraging these insights, we deduce that a design principle identified to significantly improve predictive performance under heterophily—separate aggregators for ego- and neighbor-embeddings—can also inherently offer increased robustness to GNNs. Our extensive empirical analysis shows that GNNs adopting this design alone can achieve significantly improved empirical and certifiable robustness compared to the best-performing unvaccinated model. Furthermore, models with this design can be readily combined with explicit defense mechanisms to yield improved robustness with up to 18.33% increase in performance under attacks compared to the best-performing vaccinated model.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We explore the interplay between heterophily &amp; robustness in GNNs, and show that 1) effective structural attacks on homophilous graphs increase heterophily, 2) heterophilous GNN designs can be combined with defense mechanisms for improved robustness.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Nus6fOfh1HW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="-29uFS4FiDZ" data-number="4524">
        <h4>
          <a href="https://openreview.net/forum?id=-29uFS4FiDZ">
              Word Sense Induction with Knowledge Distillation from BERT
          </a>
        
          
            <a href="https://openreview.net/pdf?id=-29uFS4FiDZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Anik_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anik_Saha1">Anik Saha</a>, <a href="https://openreview.net/profile?id=~Alex_Gittens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alex_Gittens1">Alex Gittens</a>, <a href="https://openreview.net/profile?id=~Bulent_Yener2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bulent_Yener2">Bulent Yener</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#-29uFS4FiDZ-details-338" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-29uFS4FiDZ-details-338"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embeddings, sense embeddings, word sense induction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems.  Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Effective approach to distil word meaning from contextual embeddings to word sense embeddings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PGGjnBiQ84G" data-number="4520">
        <h4>
          <a href="https://openreview.net/forum?id=PGGjnBiQ84G">
              Learning Surface Parameterization for Document Image Unwarping
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PGGjnBiQ84G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sagnik_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sagnik_Das1">Sagnik Das</a>, <a href="https://openreview.net/profile?id=~Ke_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ke_Ma3">Ke Ma</a>, <a href="https://openreview.net/profile?id=~Zhixin_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhixin_Shu1">Zhixin Shu</a>, <a href="https://openreview.net/profile?id=~Dimitris_Samaras3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dimitris_Samaras3">Dimitris Samaras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">15 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PGGjnBiQ84G-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PGGjnBiQ84G-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit functions, texture mapping, surface parameterization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping. We propose an efficient method to learn surface parameterization by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates. Our surface parameterization network can be conveniently plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Recent work on differentiable rendering techniques for implicit surfaces has shown high-quality 3D scene reconstruction and view synthesis results. However, these methods typically learn the appearance color as a function of the surface points and lack explicit surface parameterization. Thus they do not allow texture map extraction or texture editing. By introducing explicit surface parameterization and learning with a recent differentiable renderer for implicit surfaces, we demonstrate state-of-the-art document-unwarping via texture extraction. We show that our approach can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. We also demonstrate the usefulness of our system by applying it to document texture editing.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning surface parameterization using rendering loss and multiview images</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PGGjnBiQ84G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="4JlwgTbmzXQ" data-number="4519">
        <h4>
          <a href="https://openreview.net/forum?id=4JlwgTbmzXQ">
              EqR: Equivariant Representations for Data-Efficient Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=4JlwgTbmzXQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnab_Kumar_Mondal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnab_Kumar_Mondal1">Arnab Kumar Mondal</a>, <a href="https://openreview.net/profile?id=~Vineet_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vineet_Jain1">Vineet Jain</a>, <a href="https://openreview.net/profile?id=~Kaleem_Siddiqi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kaleem_Siddiqi1">Kaleem Siddiqi</a>, <a href="https://openreview.net/profile?id=~Siamak_Ravanbakhsh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siamak_Ravanbakhsh1">Siamak Ravanbakhsh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#4JlwgTbmzXQ-details-529" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4JlwgTbmzXQ-details-529"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Equivariance, Invariance, Representation learning, Reinforcement learning, Symmetric MDPs, MDP homomorphism, Lie parameterization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent’s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Equivariant representation learning for data-efficient reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=4JlwgTbmzXQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JHXjK94yH-y" data-number="4518">
        <h4>
          <a href="https://openreview.net/forum?id=JHXjK94yH-y">
              Explore and Control with Adversarial Surprise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JHXjK94yH-y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arnaud_Fickinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arnaud_Fickinger1">Arnaud Fickinger</a>, <a href="https://openreview.net/profile?id=~Natasha_Jaques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Natasha_Jaques1">Natasha Jaques</a>, <a href="https://openreview.net/profile?id=~Samyak_Parajuli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Samyak_Parajuli1">Samyak Parajuli</a>, <a href="https://openreview.net/profile?id=~Michael_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Chang1">Michael Chang</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stuart_Russell1">Stuart Russell</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Levine1">Sergey Levine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JHXjK94yH-y-details-658" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JHXjK94yH-y-details-658"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, intrinsic motivation, exploration, multi-agent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering. However, a central challenge in unsupervised RL is to extract behaviors that meaningfully affect the world and cover the range of possible outcomes, without getting distracted by inherently unpredictable, uncontrollable, and stochastic elements in the environment. To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences. The Explore agent seeks out states that maximally surprise the Control agent, which in turn aims to minimize surprise, and thereby manipulate the environment to return to familiar and predictable states. The competition between these two policies drives them to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions. Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Two policies play a multi-agent adversarial game over the amount of surprise or observation entropy an agent experiences, leading the agent to fully explore the underlying state space and learn meaningful behaviors.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ck_iw4jMC4l" data-number="4508">
        <h4>
          <a href="https://openreview.net/forum?id=Ck_iw4jMC4l">
              Logical Activation Functions: Logit-space equivalents of Boolean Operators
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ck_iw4jMC4l" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Scott_C_Lowe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Scott_C_Lowe1">Scott C Lowe</a>, <a href="https://openreview.net/profile?email=robearle11%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="robearle11@gmail.com">Robert Earle</a>, <a href="https://openreview.net/profile?id=~Jason_d%26%23x27%3BEon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jason_d&#39;Eon1">Jason d'Eon</a>, <a href="https://openreview.net/profile?id=~Thomas_Trappenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Trappenberg1">Thomas Trappenberg</a>, <a href="https://openreview.net/profile?id=~Sageev_Oore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sageev_Oore1">Sageev Oore</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">16 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ck_iw4jMC4l-details-257" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ck_iw4jMC4l-details-257"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">activation functions, logits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence (versus absence) of features within the stimulus. Under this interpretation, we can derive the probability <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="82" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>∩</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> that a pair of independent features are both present in the stimulus from their logits. By converting the resulting probability back into a logit, we obtain a logit-space equivalent of the AND operation. However, since this function involves taking multiple exponents and logarithms, it is not well suited to be directly used within neural networks. We thus constructed an efficient approximation named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="83" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> (the AND operator Approximate for Independent Logits) utilizing only comparison and addition operations, which can be deployed as an activation function in neural networks. Like MaxOut, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="84" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> is a generalization of ReLU to two-dimensions. Additionally, we constructed efficient approximations of the logit-space equivalents to the OR and XNOR operators. We deployed these new activation functions, both in isolation and in conjunction, and demonstrated their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We motivate novel activation functions which are logit-space equivalents to boolean operations, and find they work well on a wide variety of tasks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ofLwshMBL_H" data-number="4507">
        <h4>
          <a href="https://openreview.net/forum?id=ofLwshMBL_H">
              Continual Learning Using Task Conditional Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ofLwshMBL_H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Honglin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Honglin_Li1">Honglin Li</a>, <a href="https://openreview.net/profile?id=~Frieder_Ganz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Frieder_Ganz1">Frieder Ganz</a>, <a href="https://openreview.net/profile?id=~David_J._Sharp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_J._Sharp1">David J. Sharp</a>, <a href="https://openreview.net/profile?id=~Payam_M._Barnaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Payam_M._Barnaghi1">Payam M. Barnaghi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 21 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ofLwshMBL_H-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ofLwshMBL_H-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">catastrophic forgetting, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning changes, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. Dynamic approaches, which assign new neuron resources to the upcoming tasks, are introduced to address this issue. However, most of the dynamic methods need task information about the upcoming tasks during the inference phase to activate the corresponding neurons. To address this issue, we introduce Task Conditional Neural Network which allows the model to identify the task information automatically. The proposed model can continually learn and embed new tasks into the model without losing the information about previously learned tasks. We evaluate the proposed model combined with the mixture of experts approach on the MNIST and CIFAR100 datasets and show how it significantly improves the continual learning process without requiring task information in advance.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A dynamic approach for continual learning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xspalMXAB0M" data-number="4483">
        <h4>
          <a href="https://openreview.net/forum?id=xspalMXAB0M">
              A Boosting Approach to Reinforcement Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xspalMXAB0M" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nataly_Brukhim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nataly_Brukhim1">Nataly Brukhim</a>, <a href="https://openreview.net/profile?id=~Elad_Hazan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elad_Hazan1">Elad Hazan</a>, <a href="https://openreview.net/profile?id=~Karan_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karan_Singh1">Karan Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">12 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xspalMXAB0M-details-921" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xspalMXAB0M-details-921"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form.
            Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function, or by relaxing the solution guarantee to a local optimality condition.
        
            We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an effective policy. The notion of weak learning we study is that of sampled-based approximate optimization of linear functions over policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods iteratively. We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular, our bound does not explicitly depend on the number of states.
        
            A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global optimality guarantees.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=xspalMXAB0M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HUeyM2qVey2" data-number="4482">
        <h4>
          <a href="https://openreview.net/forum?id=HUeyM2qVey2">
              Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HUeyM2qVey2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Michael_Anthony_Puthawala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Anthony_Puthawala1">Michael Anthony Puthawala</a>, <a href="https://openreview.net/profile?id=~Matti_Lassas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matti_Lassas1">Matti Lassas</a>, <a href="https://openreview.net/profile?id=~Ivan_Dokmani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_Dokmanić1">Ivan Dokmanić</a>, <a href="https://openreview.net/profile?id=~Maarten_V._de_Hoop2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maarten_V._de_Hoop2">Maarten V. de Hoop</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">22 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HUeyM2qVey2-details-492" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HUeyM2qVey2-details-492"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Universality, Flow Networks, Manifold Learning, Density Estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements. We find that such networks universally approximate a large class of manifolds simultaneously with densities supported on them. Among others, our results apply to the well-known coupling and autoregressive flows. We build on the work of Teshima et al. 2020 on bijective flows and study injective architectures proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a new theoretical device called the \emph{embedding gap}, which measures how far one continuous manifold is from embedding another. We relate the embedding gap to a relaxation of universally we call the \emph{manifold embedding property}, capturing the geometric part of universality. Our proof also establishes that optimality of a network can be established ``in reverse,''  resolving a conjecture made in Brehmer et al. 2020 and opening the door for simple layer-wise training schemes. Finally, we show that the studied networks admit an exact layer-wise projection result, Bayesian uncertainty quantification, and black-box recovery of network weights.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We analyze neural networks composed of bijective flows and injective expansive elements and find that such networks universally approximate a large class of manifolds and densities there on.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="zrdUVVAvcP2" data-number="4477">
        <h4>
          <a href="https://openreview.net/forum?id=zrdUVVAvcP2">
              GrASP: Gradient-Based Affordance Selection for Planning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=zrdUVVAvcP2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Vivek_Veeriah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vivek_Veeriah2">Vivek Veeriah</a>, <a href="https://openreview.net/profile?id=~Zeyu_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zeyu_Zheng1">Zeyu Zheng</a>, <a href="https://openreview.net/profile?id=~Richard_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Lewis1">Richard Lewis</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satinder_Singh2">Satinder Singh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#zrdUVVAvcP2-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zrdUVVAvcP2-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, affordances</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for \emph{selecting} affordances useful for planning---for learning which small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option  affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Learning to select affordances in the form of options and primitive actions for lookahead planning</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="14kbUbOaZUc" data-number="4476">
        <h4>
          <a href="https://openreview.net/forum?id=14kbUbOaZUc">
              Metric Learning on Temporal Graphs via Few-Shot Examples
          </a>
        
          
            <a href="https://openreview.net/pdf?id=14kbUbOaZUc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongqi_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongqi_Fu1">Dongqi Fu</a>, <a href="https://openreview.net/profile?id=~Liri_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liri_Fang1">Liri Fang</a>, <a href="https://openreview.net/profile?id=~Ross_Maciejewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ross_Maciejewski1">Ross Maciejewski</a>, <a href="https://openreview.net/profile?id=~Vetle_I_Torvik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vetle_I_Torvik1">Vetle I Torvik</a>, <a href="https://openreview.net/profile?id=~Jingrui_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingrui_He1">Jingrui He</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#14kbUbOaZUc-details-155" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="14kbUbOaZUc-details-155"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Metric Learning, Few-Shot Learning, Temporal Graph</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs; Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales. Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The first attempt to learn temporal graph representations, on the graph-level, covering the whole lifetime, and only consuming a few labeled samples. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=14kbUbOaZUc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a61qArWbjw_" data-number="4475">
        <h4>
          <a href="https://openreview.net/forum?id=a61qArWbjw_">
              Scalable multimodal variational autoencoders with surrogate joint posterior
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a61qArWbjw_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Masahiro_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Masahiro_Suzuki1">Masahiro Suzuki</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yutaka_Matsuo1">Yutaka Matsuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">13 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a61qArWbjw_-details-465" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a61qArWbjw_-details-465"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep generative models, multimodal learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To obtain a joint representation from multimodal data in variational autoencoders (VAEs), it is important to infer the representation from arbitrary subsets of modalities after learning.  A scalable way to achieve this is to aggregate the inferences of each modality as experts. A state-of-the-art approach to learning this aggregation of experts is to encourage all modalities to be reconstructed and cross-generated from arbitrary subsets. However, this learning may be insufficient if cross-generation is difficult. Furthermore, to evaluate its objective function, exponential generation paths concerning the number of modalities are required. To alleviate these problems, we propose to explicitly minimize the divergence between inferences from arbitrary subsets and the surrogate joint posterior that approximates the true joint posterior. We also proposed using a gradient origin network, a deep generative model that learns inferences without using an inference network, thereby reducing the need for additional parameters by introducing the surrogate posterior. We demonstrate that our method performs better than existing scalable multimodal VAEs in inference and generation.    
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We proposed a scalable and high performance multimodal VAE in the framework of approximating inferences from arbitrary subsets of modalities to a surrogate joint posterior.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fwJWhOxuzV9" data-number="4471">
        <h4>
          <a href="https://openreview.net/forum?id=fwJWhOxuzV9">
              Semi-supervised Offline Reinforcement Learning with Pre-trained Decision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fwJWhOxuzV9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Catherine_Cang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Catherine_Cang1">Catherine Cang</a>, <a href="https://openreview.net/profile?id=~Kourosh_Hakhamaneshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Hakhamaneshi1">Kourosh Hakhamaneshi</a>, <a href="https://openreview.net/profile?id=~Ryan_Rudes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryan_Rudes1">Ryan Rudes</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Igor_Mordatch4">Igor Mordatch</a>, <a href="https://openreview.net/profile?id=~Aravind_Rajeswaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aravind_Rajeswaran1">Aravind Rajeswaran</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michael_Laskin1">Michael Laskin</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fwJWhOxuzV9-details-870" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fwJWhOxuzV9-details-870"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-task RL, Decision Transformer, self-supervised RL, Pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Pre-training deep neural network models using large unlabelled datasets followed by fine-tuning them on small task-specific datasets has emerged as a dominant paradigm in natural language processing (NLP) and computer vision (CV). Despite the widespread success, such a paradigm has remained atypical in reinforcement learning (RL).
        In this paper, we investigate how we can leverage large reward-free (i.e. task-agnostic) offline datasets of prior interactions to pre-train agents that can then be fine-tuned using a small reward-annotated dataset. To this end, we present Pre-trained Decision Transformer (PDT), a simple yet powerful algorithm for semi-supervised Offline RL. By masking reward tokens during pre-training, the transformer learns to autoregressivley predict actions based on previous state and action context and effectively extracts behaviors present in the dataset. During fine-tuning, rewards are un-masked and the agent learns the set of skills that should be invoked for the desired behavior as per the reward function. We demonstrate the efficacy of this simple and flexible approach on tasks from the D4RL benchmark with limited reward annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce Pre-trained Decision Transformers, a simple and flexible architecture that can be pre-trained on unlabeled environment interactions and can quickly adapt to several downstream tasks with just a small reward-annotated fine-tuning dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="_Ko4kT3ckWy" data-number="4470">
        <h4>
          <a href="https://openreview.net/forum?id=_Ko4kT3ckWy">
              Increase and Conquer: Training Graph Neural Networks on Growing Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=_Ko4kT3ckWy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Juan_Cervino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_Cervino1">Juan Cervino</a>, <a href="https://openreview.net/profile?id=~Luana_Ruiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luana_Ruiz1">Luana Ruiz</a>, <a href="https://openreview.net/profile?id=~Alejandro_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alejandro_Ribeiro1">Alejandro Ribeiro</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#_Ko4kT3ckWy-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_Ko4kT3ckWy-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Graph Neural Networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNNs) use graph convolutions to exploit network invariances and learn meaningful features from network data. However, on large-scale graphs convolutions incur in high computational cost, leading to scalability limitations. Leveraging the graphon --- the limit object of a graph --- in this paper we consider the problem of learning a graphon neural network (WNN) --- the limit object of a GNN --- by training GNNs on graphs sampled Bernoulli from the graphon. Under smoothness conditions, we show that: (i) the expected distance between the learning steps on the GNN and on the WNN decreases asymptotically with the size of the graph, and (ii) when training on a sequence of growing graphs, gradient descent follows the learning direction of the WNN. Inspired by these results, we propose a novel algorithm to learn GNNs on large-scale graphs that, starting from a moderate number of nodes, successively increases the size of the graph during training. This algorithm is benchmarked on both a recommendation system and a decentralized control problem where it is shown to retain comparable performance, to its large-scale counterpart, at a reduced computational cost.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper describes a way to train GNNs on a sequence of growing graphs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=_Ko4kT3ckWy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="MQuxKr2F1Xw" data-number="4468">
        <h4>
          <a href="https://openreview.net/forum?id=MQuxKr2F1Xw">
              Multi-Trigger-Key: Towards Multi-Task Privacy-Preserving In Deep Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=MQuxKr2F1Xw" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ren_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ren_Wang1">Ren Wang</a>, <a href="https://openreview.net/profile?id=~Zhe_Xu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhe_Xu7">Zhe Xu</a>, <a href="https://openreview.net/profile?id=~Alfred_Hero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alfred_Hero1">Alfred Hero</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Submitted</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#MQuxKr2F1Xw-details-953" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MQuxKr2F1Xw-details-953"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning-based Multi-Task Classification (MTC) is widely used in applications like facial attribute and healthcare that warrant strong privacy guarantees. In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective. MTK associates each secured task in the multi-task dataset with a specifically designed trigger-key. The true information can be revealed by adding the trigger-key if the user is authorized. We obtain such an MTK model by training it with a newly generated training set. To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance. Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=MQuxKr2F1Xw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>‹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">›</a>
      </li>
      <li class="  right-arrow" data-page-number="31">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="desk-rejected-withdrawn-submissions">

<ul class="list-unstyled submissions-list">
    <li class="note " data-id="XNYOJD0QdBD" data-number="4713">
        <h4>
          <a href="https://openreview.net/forum?id=XNYOJD0QdBD">
              Personalized PageRank meets Graph Attention Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XNYOJD0QdBD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Julie_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julie_Choi1">Julie Choi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XNYOJD0QdBD-details-676" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XNYOJD0QdBD-details-676"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GNN, Personalized PageRank, Graph Attention Network, Graph Neural Network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rising interest in graph neural networks (GNNs) for representation learning over the past few years. GNNs provide a general and efficient framework to learn from graph-structured data. However, GNNs typically only use the information of a very limited neighborhood for each node. A larger neighborhood would be desirable to provide the model with more information. However, increasing the size of the neighborhood is not trivial since neighborhood aggregation over many layers leads to over-smoothing. In this work, we incorporate the limit distribution of Personalized PageRank (PPR) into graph attention networks (GATs) to address this issue. Intuitively, message aggregation based on Personalized PageRank corresponds to infinitely many neighborhood aggregation layers. We show that our models outperform a variety ofbaseline models across all datasets used for our experiments. Our implementation is publicly available online.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Personalized PageRank meets Graph Attention Networks.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="HLTLhiBtUcW" data-number="4712">
        <h4>
          <a href="https://openreview.net/forum?id=HLTLhiBtUcW">
              Enhanced neural network regularization with macro-block dropout
          </a>
        
          
            <a href="https://openreview.net/pdf?id=HLTLhiBtUcW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#HLTLhiBtUcW-details-188" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HLTLhiBtUcW-details-188"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">macro block dropout, regularization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> This paper proposes a new regularization algorithm referred to as macro-block dropout. The overfitting issue has been a difficult problem in training large network models. The dropout technique has proven to be simple yet very effective for regularization by preventing complex co-adaptations on training data.  In this work, we observe that in the hidden outputs, the correlations between geometrically close elements are usually stronger than those between distant elements. Motivated by this observation, we define a macro-block that contains multiple elements of the hidden output layer in order to reduce co-adaptations more effectively. Rather than applying dropout to each element, we apply random dropout to each macro-block. In our experiments with  image classification tasks on the MNIST and the ImageNet datasets as well as a speech recognition task on the LibriSpeech set, this simple algorithm has shown a quite significant improvement over the conventional dropout approach</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a macro-block dropout for better regularization.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XY1DWeh58WR" data-number="4709">
        <h4>
          <a href="https://openreview.net/forum?id=XY1DWeh58WR">
              Deep Recurrent Neural Network Layers with Layerwise Loss
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XY1DWeh58WR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chanwoo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chanwoo_Kim2">Chanwoo Kim</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XY1DWeh58WR-details-82" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XY1DWeh58WR-details-82"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning techniques have brought significant performance improvement to various areas of machine learning. Especially in the computer vision area, very deep networks such as ResNet have shown notable performance improvement. However, in speech recognition or language processing, such kinds of a very deep network have not been extensively employed. In this paper, we propose a very deep LSTM structure and their training strategy. In our training strategy, we first start training a conventional model with several LSTM layers. One notable difference is that for the top LSTM layer of the initial model, the Connectionist Temporal Classification (CTC) loss is applied both to the input and output of this top LSTM layer. Once this initial model is sufficiently layered, this top layer is copied to construct a very deep LSTM stack. For this newly constructed stack, the CTC loss is applied to every output of the LSTM layer as well as the top of the stack. Experimental results show that this deep LSTM structure shows significantly better results than the conventional model with 5 ~ 6 layers with a comparable number of parameters.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we propose a very deep neural network with layerwise loss</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="np5BgCFSsbm" data-number="4433">
        <h4>
          <a href="https://openreview.net/forum?id=np5BgCFSsbm">
              Neocortical cell type classification from electrophysiology recordings using deep neural networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=np5BgCFSsbm" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Raymond_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Raymond_Wang1">Raymond Wang</a>, <a href="https://openreview.net/profile?id=~Sang_Min_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sang_Min_Han1">Sang Min Han</a>, <a href="https://openreview.net/profile?id=~Marta_Agnieszka_Gajowa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marta_Agnieszka_Gajowa1">Marta Agnieszka Gajowa</a>, <a href="https://openreview.net/profile?id=~Chunlei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chunlei_Liu1">Chunlei Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#np5BgCFSsbm-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="np5BgCFSsbm-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neuron type classification, convolutional neural network, electrophysiology</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the neural code requires identifying different functional units involved in the neural circuits. One way to identify these functional units is to solve a neuron type classification problem. For decades, current clamp electrophysiology recordings have provided the means to classify the neurons based on subtle differences in action potential shapes and spiking patterns. However, significant variations in neuronal type definitions, classification pipelines, and variability in the neuronal activities make unambiguous determination of neuron type challenging. Previous solutions to this electrophysiology-based cell type classification problem consisted of dimensionality reduction juxtaposed with clustering using hand-crafted action potential features. Recent discoveries have allowed genetic-based cell-type classifications, which have fewer ambiguities, but they are less practical in vivo and have even lower throughput. Leveraging the unprecedented ground truth data published in the Allen Institute Cell Types Database, which contains anatomical, genetic, and electrophysiology characterizations of neurons in the mouse neocortex, we construct a robust and efficient convolutional neural network (CNN) that successfully classifies neurons according to their genetic label or broad type (excitatory or inhibitory) solely using current-clamp electrophysiology recordings. The CNN is configured as a multiple-input single-output network consisting of three subnetworks that take in the raw time series electrophysiology recording as well as the real and imaginary components of its Fourier coefficients. Our single pipeline method is fast and streamlined while simultaneously outperforming previous methods and achieving more classification classes using only single current-clamp trace as the input. This end-to-end convolutional neural network-based classification method removes the need for hand-crafted features, specific knowledge, or human intervention for quick identification of the cell type with high accuracy, enabling interpretation of the experimental data in a bias-free manner and a much broader scientific context.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A robust and efficient convolutional neural network successfully classifies neurons according to their genetic label and broad type using only current-clamp electrophysiology recordings.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="NMSugaVzIT" data-number="4294">
        <h4>
          <a href="https://openreview.net/forum?id=NMSugaVzIT">
              Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm
          </a>
        
          
            <a href="https://openreview.net/pdf?id=NMSugaVzIT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Meena_Jagadeesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Meena_Jagadeesan1">Meena Jagadeesan</a>, <a href="https://openreview.net/profile?id=~Ilya_Razenshteyn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ilya_Razenshteyn1">Ilya Razenshteyn</a>, <a href="https://openreview.net/profile?id=~Suriya_Gunasekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Suriya_Gunasekar1">Suriya Gunasekar</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#NMSugaVzIT-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NMSugaVzIT-details-264"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">minimizing parameter l2 norm, representation cost, implicit bias</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We provide a function space characterization of the inductive bias resulting from minimizing the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="85" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of the weights in multi-channel linear convolutional networks. We define an \textit{induced regularizer} in the function space as the minimum <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="86" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm of weights of a network required to realize a function.  For two layer linear convolutional networks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="87" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> output channels and kernel size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="88" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, we show the following: (a) If the inputs to the network have a single channel, the induced regularizer for any <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="89" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is \textit{independent} of the number of output channels <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="90" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for networks with multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias \emph{does} depend on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="91" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. However, for sufficiently large <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="92" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>, the induced regularizer is again given by an SDP that is independent of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="93" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container>. In particular, the induced regularizer for  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="94" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="95" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>=</mo><mi>D</mi></math></mjx-assistive-mml></mjx-container> are given in closed form as the nuclear norm and the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="96" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> group-sparse norm, respectively, of the Fourier coefficients.
        We investigate the applicability of our theoretical results to a broader scope of ReLU convolutional networks through experiments on MNIST and CIFAR-10 datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We study the function space view of minimizing l2 norm of weights in multi-channel linear convolutional networks, uncovering an invariance to the number of output channels. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=NMSugaVzIT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="DVSN9nJB1_" data-number="3792">
        <h4>
          <a href="https://openreview.net/forum?id=DVSN9nJB1_">
              E-LANG: Energy-based Joint Inferencing of Super and Swift Language Models
          </a>
        
          
            <a href="https://openreview.net/pdf?id=DVSN9nJB1_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Mohammad_Akbari3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohammad_Akbari3">Mohammad Akbari</a>, <a href="https://openreview.net/profile?id=~Amin_Banitalebi-Dehkordi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amin_Banitalebi-Dehkordi1">Amin Banitalebi-Dehkordi</a>, <a href="https://openreview.net/profile?id=~Yong_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yong_Zhang2">Yong Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#DVSN9nJB1_-details-385" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DVSN9nJB1_-details-385"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based models, dynamic inference, joint language models, super model optimization, NLP, BERT, T5</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building very large and highly capable language models has been a trend in the past several years. Despite their great performance, they incur a high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the incoming samples to one of the two models based on the energy characteristics of the representations in the latent space. The proposed approach is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. Unlike existing methods that are for the most part only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The performance of the proposed Energy-based joint inferencing of LANGuage models, E-LANG, is verified through an extensive set of experiments with T5 and BERT architectures on GLUE, SuperGLUE, and WMT benchmarks. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA (state-of-the-art) on GLUE with 3.2X less computations. Code is available in the supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">In this paper, we present E-LANG, an energy-based joint inference approach, which combines Super and Swift language models for achieving efficient inference without sacrificing the accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=DVSN9nJB1_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="k_Zy6glYaqc" data-number="3557">
        <h4>
          <a href="https://openreview.net/forum?id=k_Zy6glYaqc">
              Quantum Alphatron
          </a>
        
          
            <a href="https://openreview.net/pdf?id=k_Zy6glYaqc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Siyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siyi_Yang1">Siyi Yang</a>, <a href="https://openreview.net/profile?id=~Patrick_Rebentrost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_Rebentrost1">Patrick Rebentrost</a>, <a href="https://openreview.net/profile?id=~Miklos_Santha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miklos_Santha1">Miklos Santha</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#k_Zy6glYaqc-details-310" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="k_Zy6glYaqc-details-310"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finding provably efficient algorithms for learning neural networks is a fundamental challenge in the theory of machine learning. The Alphatron of Goel and Klivans is the first provably efficient algorithm for learning neural networks with more than one nonlinear layer. The algorithm succeeds with any distribution on the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="97" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-dimensional unit ball and without any assumption on the structure of the network. In this work, we refine the original Alphatron by a pre-computing phase for its most time-consuming part, the evaluation of the kernel function. This refined algorithm improves the run time of the original Alphatron, while retaining the same learning guarantee. Based on the refined algorithm, we quantize the pre-computing phase with provable learning guarantee in the fault-tolerant quantum computing model. In a well-defined learning model, this quantum algorithm is able to provide a quadratic speedup in the data dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="98" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>. In addition, we discuss the second type of speedup, quantizing the evaluation of the gradient in the stochastic gradient descent procedure. Our work contributes to the study of quantum learning with kernels and from samples.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="QWc35QxXPzZ" data-number="3480">
        <h4>
          <a href="https://openreview.net/forum?id=QWc35QxXPzZ">
              The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces
          </a>
        
          
            <a href="https://openreview.net/pdf?id=QWc35QxXPzZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Chi_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chi_Jin1">Chi Jin</a>, <a href="https://openreview.net/profile?id=~Qinghua_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qinghua_Liu1">Qinghua Liu</a>, <a href="https://openreview.net/profile?id=~Tiancheng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tiancheng_Yu1">Tiancheng Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#QWc35QxXPzZ-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QWc35QxXPzZ-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">theoretical reinforcement learning, Markov games with general function approximation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern reinforcement learning (RL) commonly engages practical problems with large state spaces, where function approximation must be deployed to approximate either the value function or the policy. While recent progresses in RL theory address a rich set of RL problems with general function approximation, such successes are mostly restricted to the single-agent setting. It remains elusive how to extend these results to multi-agent RL, especially due to the new challenges arising from its game-theoretical nature. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algorithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension -- a new complexity measure adapted from its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter, which facilitates the learning of the main player by deliberately exploiting her weakness. Our theoretical framework is generic, which applies to a wide range of models including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper studies sample-efficient learning of Markov Games with general function approximation.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="6Q5RdltG3L" data-number="3243">
        <h4>
          <a href="https://openreview.net/forum?id=6Q5RdltG3L">
              Human imperceptible attacks and applications to improve fairness
          </a>
        
          
            <a href="https://openreview.net/pdf?id=6Q5RdltG3L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xinru_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xinru_Hua1">Xinru Hua</a>, <a href="https://openreview.net/profile?id=~Huanzhong_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Huanzhong_Xu1">Huanzhong Xu</a>, <a href="https://openreview.net/profile?id=~Jose_Blanchet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jose_Blanchet1">Jose Blanchet</a>, <a href="https://openreview.net/profile?id=~Viet_Anh_Nguyen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Viet_Anh_Nguyen2">Viet Anh Nguyen</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#6Q5RdltG3L-details-494" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Q5RdltG3L-details-494"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural networks are able to perform at least as well as humans in numerous tasks involving object classification and image generation. However, there is also evidence that perturbations which are imperceptible to humans may significantly degrade the performance of well-trained deep neural networks. We provide a Distributionally Robust Optimization (DRO) framework which integrates human-based image quality assessment methods to design optimal attacks that are imperceptible to humans but significantly damaging to deep neural networks. Our attack algorithm can generate better-quality (less perceptible to humans) attacks than other state-of-the-art human imperceptible attack methods. We provide an algorithmic implementation of independent interest which can speed up DRO training significantly. Finally, we demonstrate how the use of optimally designed human imperceptible attacks can improve group fairness in image classification while maintaining a similar accuracy.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=6Q5RdltG3L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="vNDHZZa-Q92" data-number="3220">
        <h4>
          <a href="https://openreview.net/forum?id=vNDHZZa-Q92">
              Neural Extensions: Training Neural Networks with Set Functions
          </a>
        
          
            <a href="https://openreview.net/pdf?id=vNDHZZa-Q92" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Nikolaos_Karalias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikolaos_Karalias1">Nikolaos Karalias</a>, <a href="https://openreview.net/profile?id=~Joshua_David_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joshua_David_Robinson1">Joshua David Robinson</a>, <a href="https://openreview.net/profile?id=~Andreas_Loukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andreas_Loukas1">Andreas Loukas</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#vNDHZZa-Q92-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vNDHZZa-Q92-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">continuous extensions, algorithmic reasoning, set functions, machine learning, combinatorial optimization, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integrating discrete computational steps into deep learning architectures is an important consideration when learning to reason over discrete items. However, many tasks that involve discrete choices are defined via (combinatorial) set functions, and thereby pose challenges for end-to-end training. In this work, we explore a general framework to construct continuous extensions of such discrete functions that enables training via gradient methods. Our framework includes well-known extensions such as the Lovasz extension of submodular set functions and facilitates the design of novel continuous extensions based on problem-specific considerations, including constraints. We demonstrate the versatility of our framework on tasks ranging from combinatorial optimization to image classification. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A principled framework for continuous extensions of set functions in machine learning.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="oykI6Kmq3Xi" data-number="3194">
        <h4>
          <a href="https://openreview.net/forum?id=oykI6Kmq3Xi">
              Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games
          </a>
        
          
            <a href="https://openreview.net/pdf?id=oykI6Kmq3Xi" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ryann_Sim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryann_Sim1">Ryann Sim</a>, <a href="https://openreview.net/profile?id=~EFSTRATIOS_PANTELEIMON_SKOULAKIS2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~EFSTRATIOS_PANTELEIMON_SKOULAKIS2">EFSTRATIOS PANTELEIMON SKOULAKIS</a>, <a href="https://openreview.net/profile?id=~Lillian_J_Ratliff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lillian_J_Ratliff1">Lillian J Ratliff</a>, <a href="https://openreview.net/profile?id=~Georgios_Piliouras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Georgios_Piliouras1">Georgios Piliouras</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#oykI6Kmq3Xi-details-940" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oykI6Kmq3Xi-details-940"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">extensive form games, network extensive form games, online learning, optimistic gradient descent ascent</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFG) and particularly in EFGs with many agents lags far behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="99" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> rate of convergence to the set of Nash equilibria. Moreover, we show that the day-to-day behavior also converges to Nash with rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="100" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mrow data-mjx-texclass="ORD"><mo>−</mo><mi>t</mi></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for some game-dependent constant <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="101" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mo>&gt;</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We provide a formulation of network zero-sum extensive form games and show that optimistic gradient ascent admits fast convergence to Nash, both in time average and in the day-to-day sense.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=oykI6Kmq3Xi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JvVFSmFV8G" data-number="3163">
        <h4>
          <a href="https://openreview.net/forum?id=JvVFSmFV8G">
              Which model to trust: assessing the influence of models on the performance of reinforcement learning algorithms for continuous control tasks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JvVFSmFV8G" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Giacomo_Arcieri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Giacomo_Arcieri1">Giacomo Arcieri</a>, <a href="https://openreview.net/profile?email=woelfle%40fzi.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="woelfle@fzi.de">David Wölfle</a>, <a href="https://openreview.net/profile?id=~Eleni_Chatzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eleni_Chatzi1">Eleni Chatzi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JvVFSmFV8G-details-602" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JvVFSmFV8G-details-602"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, model-based reinforcement learning, deep learning, bayesian deep learning, gaussian processes, continuous control, model uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=JvVFSmFV8G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ulj0tR-k7q" data-number="3070">
        <h4>
          <a href="https://openreview.net/forum?id=Ulj0tR-k7q">
              On strong convergence of the two-tower model for recommender system
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ulj0tR-k7q" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~SHIRONG_XU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~SHIRONG_XU1">SHIRONG XU</a>, <a href="https://openreview.net/profile?id=~Junhui_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junhui_Wang3">Junhui Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ulj0tR-k7q-details-836" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ulj0tR-k7q-details-836"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Artificial neural networks, Collaborative filtering, Empirical process, Recommender system, Two-tower model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recommender system is capable of predicting preferred items for a user by integrating information from similar users or items. A popular model in recommender system is the so-called two-tower model, which employs two deep neural networks to embed users and items into a low-dimensional space, and predicts ratings via the geometrical relationship of the embeddings of user and item in the embedded space. Even though it is popularly used for recommendations, its theoretical properties remain largely unknown. In this paper, we establish some asymptotic results of the two-tower model in terms of its strong convergence to the optimal recommender system, showing that it achieves a fast convergence rate depending on the intrinsic dimensions of inputs features. To the best of our knowledge, this is among the first attempts to establish the statistical guarantee of the two-tower model. Through numerical experiments, we also demonstrate that the two-tower model is capable of capturing the effects of users' and items' features on ratings, leading to higher prediction accuracy over its competitors in both simulated examples and a real application data set. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Establishing theoretical guarantee for the two-tower model in recommender system</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ulj0tR-k7q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UquMPXFTpgp" data-number="2976">
        <h4>
          <a href="https://openreview.net/forum?id=UquMPXFTpgp">
              Cluster Tree for Nearest Neighbor Search
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UquMPXFTpgp" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dan_Kushnir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Kushnir1">Dan Kushnir</a>, <a href="https://openreview.net/profile?id=~Sandeep_Silwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandeep_Silwal1">Sandeep Silwal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UquMPXFTpgp-details-391" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UquMPXFTpgp-details-391"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nearest neighbor search, tree algorithms, graph cuts, random projections</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Tree-based algorithms are an important and widely used class of algorithms for  Nearest Neighbor Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in spite of possessing theoretical guarantees and strong practical performance, a major drawback of the RP tree is its lack of adaptability to the input dataset.
        
        Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing ClusterTree, a new tree based algorithm. Our approach utilizes randomness as in RP trees while adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees and other tree based methods for NNS while maintaining efficient construction time. In addition, we show theoretically and empirically that ClusterTree finds partitions which are superior to those found by RP trees in preserving the cluster structure of the input dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel tree-based algorithm for nearest neighbor search which adapts to the cluster structure of the input dataset.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xGZcxaYbJBF" data-number="2193">
        <h4>
          <a href="https://openreview.net/forum?id=xGZcxaYbJBF">
              A Multi-Task Learning Algorithm for Non-personalized Recommendations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xGZcxaYbJBF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jiawei_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiawei_Zhang8">Jiawei Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xGZcxaYbJBF-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xGZcxaYbJBF-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Recommendation and Ranking, Non-personalized Recommendations, Multitask Learning, collaborative filtering, Two-tower DNN</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce a multi-task learning (MTL) algorithm for recommending non-personalized videos to watch next on industrial video sharing platforms. Personalized recommendations have been studied for decades, while researches on non-personalized solutions are very rare to be seen, which still remain a huge portion in industry. As an indispensable part in recommender system, non-personalized video recommender system also faces several real-world challenges, including maintaining high relevance between source item and target items, as well as achieving multiple competing ranking objectives. To solve these, we largely extended model-based collaborative filtering algorithm by adding related candidate generation stage, Two-tower DNN structure and a multi-task learning mechanism. Compared with typical baseline solutions, our proposed algorithm can capture both linear and non-linear relationships from user-item interactions, and live experiments demonstrate that it can significantly advance the state of the art on recommendation quality.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A multi-task learning (MTL) algorithm is introduced for recommending non-personalized videos to watch next on industrial video sharing platforms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="XTzAhbVbKgq" data-number="2123">
        <h4>
          <a href="https://openreview.net/forum?id=XTzAhbVbKgq">
              Batched Lipschitz Bandits
          </a>
        
          
            <a href="https://openreview.net/pdf?id=XTzAhbVbKgq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yasong_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yasong_Feng1">Yasong Feng</a>, <a href="https://openreview.net/profile?id=~Zengfeng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zengfeng_Huang1">Zengfeng Huang</a>, <a href="https://openreview.net/profile?id=~Tianyu_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tianyu_Wang4">Tianyu Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#XTzAhbVbKgq-details-375" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XTzAhbVbKgq-details-375"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-armed bandits, online learning, batched bandits, Lipschitz bandits</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we study the batched Lipschitz bandit problem, where the expected reward is Lipschitz and the reward observations are collected in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback setting. In particular, we show that for a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="102" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>-step problem with Lipschitz reward of zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="103" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, our algorithm achieves theoretically optimal regret rate of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="104" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.509em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.501em; margin-left: 0.056em;"><mjx-texatom size="s" texclass="ORD"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mi>T</mi><mrow data-mjx-texclass="ORD"><mfrac><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>d</mi><mi>z</mi></msub><mo>+</mo><mn>2</mn></mrow></mfrac></mrow></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> using only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="105" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" style="font-size: 83.3%;"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mrow><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>T</mi></mrow><msub><mi>d</mi><mi>z</mi></msub></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> batches. For the lower bound, we show that in an environment with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="106" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>-batches, for any policy <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="107" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></mjx-assistive-mml></mjx-container>, there exists a problem instance such that the expected regret is lower bounded by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="108" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.361em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c28" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 1.233em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow style="font-size: 83.3%;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-msup><mjx-mrow><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.763em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-stretchy-v class="mjx-c29" style="height: 3.307em; vertical-align: -1.404em;"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi mathvariant="normal">Ω</mi><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><msup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow><mi>B</mi></msup></mrow></mfrac></msup><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="109" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>z</mi></msub><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="110" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>z</mi></msub></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="111" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the dimension of the arm space. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present a novel landscape-aware algorithm to solve the batched Lipschitz bandit problem, and show that our algorithm matches the optimal regret upper bound using less than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="112" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>T</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> batches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=XTzAhbVbKgq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="CdqsSPLNx-" data-number="2094">
        <h4>
          <a href="https://openreview.net/forum?id=CdqsSPLNx-">
              Deep Dynamic Attention Model with Gate Mechanism for Solving Time-dependent Vehicle Routing Problems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=CdqsSPLNx-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Feng_Guo7" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Feng_Guo7">Feng Guo</a>, <a href="https://openreview.net/profile?id=~Qu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qu_Wei1">Qu Wei</a>, <a href="https://openreview.net/profile?id=~Miao_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Miao_Wang2">Miao Wang</a>, <a href="https://openreview.net/profile?id=~Zhaoxia_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhaoxia_Guo1">Zhaoxia Guo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#CdqsSPLNx--details-819" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CdqsSPLNx--details-819"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vehicle routing problems (VRPs) are a type of classical combinatorial optimization problems widely existing in logistics and transportation operations. There has been an increasing interest to use deep reinforcement learning (DRL) techniques to tackle VRPs, and previous DRL-based studies assumed time-independent travel times between customers. However, travel times in real-world road networks are time-varying, which need to be considered in practical VRPs. We thus propose a Deep Dynamic Attention Models with Gate Mechanisms (DDAM-GM) to learn heuristics for time-dependent VRPs (TDVRPs) in real-world road networks. It extracts the information of node location, node demand, and time-varying travel times between nodes to obtain enhanced node embeddings through a dimension-reducing MHA layer and a synchronous encoder. In addition, we use a gate mechanism to obtain better context embedding. On the basis of a 110-day travel time dataset with 240 time periods per day from an urban road network with 408 nodes and 1250 directed links, we conduct a series of experiments to validate the effectiveness of the proposed model on TDVRPs without and with consideration of time windows, respectively. Experimental results show that our model outperforms significantly two state-of-the-art DRL-based models.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lDvJM5XUyrx" data-number="1436">
        <h4>
          <a href="https://openreview.net/forum?id=lDvJM5XUyrx">
              Towards Understanding Catastrophic Overfitting in Fast Adversarial Training
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lDvJM5XUyrx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Renjie_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Renjie_Chen2">Renjie Chen</a>, <a href="https://openreview.net/profile?id=~Yuan_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan_Luo1">Yuan Luo</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yisen_Wang1">Yisen Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lDvJM5XUyrx-details-600" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lDvJM5XUyrx-details-600"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robustness, Fast Adversarial Training, Catastrophic Overfitting</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">After adversarial training was proposed, a series of works focus on improving the compunational efficiency of adversarial training for deep neural networks (DNNs). Recently, FGSM based single-step adversarial training has been found to be able to train a robust model with the robustness comparable to the one trained by multi-step PGD, but it is an order of magnitude faster. However, there exists a failure mode called Catastrophic Overfitting (CO) where the network loses its robustness against PGD attack suddenly and can be hardly recovered by itself during the training process. In this paper, we identify that CO is closely related to the high-order terms in Taylor expansion after rethinking and decomposing the min-max problem in adversarial training. The negative high-order terms lead to a phenomenon called Perturbation Loss Distortion, which is the underlying cause of CO. Based on the observations, we propose a simple but effective regularization method named Fast Linear Adversarial Training (FLAT) to avoid CO in the single-step adversarial training by making the loss surface flat.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Fast Linear Adversarial Training (FLAT) can help prevent the Catastrophic Overfitting in single-step adversarial training.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=lDvJM5XUyrx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZVqsBl2HapR" data-number="1434">
        <h4>
          <a href="https://openreview.net/forum?id=ZVqsBl2HapR">
              Error-based or target-based? A unifying framework for learning in recurrent spiking networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZVqsBl2HapR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Cristiano_Capone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cristiano_Capone1">Cristiano Capone</a>, <a href="https://openreview.net/profile?id=~Paolo_Muratore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Paolo_Muratore1">Paolo Muratore</a>, <a href="https://openreview.net/profile?id=~Pier_Stanislao_Paolucci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pier_Stanislao_Paolucci1">Pier Stanislao Paolucci</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZVqsBl2HapR-details-915" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZVqsBl2HapR-details-915"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">target-based, error-based, recurrent neural network, spiking neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning in biological or artificial networks means changing the laws governing the network dynamics in order to better behave in a specific situation. In the field of supervised learning, two complementary approaches stand out: error-based and target-based learning. However, there exists no consensus on which is better suited for which task, and what is the most biologically plausible. Here we propose a comprehensive theoretical framework that includes these two frameworks as special cases. This novel theoretical formulation offers major insights into the differences between the two approaches. In particular, we show how target-based naturally emerges from error-based when the number of constraints on the target dynamics, and as a consequence on the internal network dynamics, is comparable to the degrees of freedom of the network. Moreover, given the experimental evidences on the relevance that spikes have in biological networks, we investigate the role of coding with specific patterns of spikes by introducing a parameter that defines the tolerance to precise spike timing during learning. Our approach naturally lends itself to Imitation Learning (and Behavioral Cloning in particular) and we apply it to solve relevant closed-loop tasks such as the button-and-food task, and the 2D Bipedal Walker. We show that a high dimensionality feedback structure is extremely important when it is necessary to solve a task that requires retaining memory for a long time (button-and-food). On the other hand, we find that coding with specific patterns of spikes enables optimal performances in a motor task (the 2D Bipedal Walker). Finally, we show that our theoretical formulation suggests protocols to deduce the structure of learning feedback in biological networks.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce a new learning framework that includes target- and error-based approches as special cases. It allows to understand their relationship and to explore what learning rule is optimal in the different tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZVqsBl2HapR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="x8l2miKNqPb" data-number="1409">
        <h4>
          <a href="https://openreview.net/forum?id=x8l2miKNqPb">
              Generate Triggers  in Neural Relation Extraction
          </a>
        
          
            <a href="https://openreview.net/pdf?id=x8l2miKNqPb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Liu_Yujiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Yujiang1">Liu Yujiang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#x8l2miKNqPb-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x8l2miKNqPb-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">relation triggers，evolutive mask， pointer network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the relation extraction task, the relationship between two entities is determined by some specific words in their source text. These words are called relation triggers, which are the evidence to explain the relationship; other words are called ir-relevant words. The current relationship extraction neural network model aims at identifying the relation type between two entities mentioned in source text by encoding the text and entities. However, these models cannot output the relation triggers, but only gives the result of relation classification. Although models can generate weights for every single word through the improvement of attention mechanism, the weights will be affected by irrelevant words essentially, which are not required by the relation extraction task. In order to output re-lation triggers accurately, we propose a novel training frame-work for Relation Extraction (RE) that reduces the negative effect of irrelevant words on them in the encoding stage. In specific, we leverage Evolutive Mask based Point Network (EMPN) as a decoder to generate relation triggers and encode these words again. For an ordered output in relation triggers, we utilize order loss to constrain the output order in them. Ex-tensive experiment results demonstrate that the effectiveness of our proposed model achieves state-of-the-art performance on three RE benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Predict relation type and generate trigger words to make the results reasonable with Evolutive Mask based Point Network.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=x8l2miKNqPb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="IPwwNwMvHFW" data-number="1384">
        <h4>
          <a href="https://openreview.net/forum?id=IPwwNwMvHFW">
              Multi-Agent Decentralized Belief Propagation on Graphs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=IPwwNwMvHFW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Yitao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yitao_Chen1">Yitao Chen</a>, <a href="https://openreview.net/profile?id=~Deepanshu_Vasal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deepanshu_Vasal1">Deepanshu Vasal</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#IPwwNwMvHFW-details-428" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IPwwNwMvHFW-details-428"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">I-pomdps, Belief propagation, Multi-agent control</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the problem of interactive partially observable Markov decision processes (I-POMDPs),where the agents are located at the nodes of a communication network.  Specifically, we assume a certain message type for all messages.  Moreover, each agent makes individual decisions based on the interactive belief states, the information observed locally and the messages received from its neighbors over the network.Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors.  We propose a decentralized belief propagation algorithm for the problem,  and prove the convergence of our algorithm.Finally we show multiple applications of our framework. Our work appears to be the first study of decentralized belief propagation algorithm for networked multi-agent I-POMDPs.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a methodology to do multi agent belief propagation on grahps</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="PU3VGS93gxD" data-number="1202">
        <h4>
          <a href="https://openreview.net/forum?id=PU3VGS93gxD">
              Sample Complexity of Deep Active Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=PU3VGS93gxD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhao_Song6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Song6">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Baocheng_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Baocheng_Sun1">Baocheng Sun</a>, <a href="https://openreview.net/profile?id=~Danyang_Zhuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Danyang_Zhuo1">Danyang Zhuo</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#PU3VGS93gxD-details-988" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PU3VGS93gxD-details-988"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning algorithms require large numbers of labeled training data to deliver state-of-the-art results. However, in many domains of AI, there are abundant unlabeled data but it is costly to get data labeled by experts, such as medical diagnosis and fraud detection. In these domains, active learning, where an algorithm maximizes model accuracy while requiring the least number of labeled data, is appealing.
        Active learning uses both labeled and unlabeled data to train models, and the learning algorithm decides which subset of data should acquire labels.
        Due to the costly label acquisition, it is interesting to know whether it is possible from a theoretical perspective to understand how many labeled data are actually needed to train a machine learning model. This question is known as the sample complexity problem, and it has been extensively explored for training linear machine learning models (e.g., linear regression). Today, deep learning has become the de facto method for machine learning, but the sample complexity problem for deep active learning remains unsolved. This problem is challenging due to the non-linear nature of neural networks.
        In this paper, we present the first deep active learning algorithm which has a provable sample complexity. Using this algorithm, we have derived the first upper bound on the number of required labeled data for training neural networks. 
        Our upper bound shows that the minimum number of labeled data a neural net needs does not depend on the data distribution or the width of the neural network but is determined by the smoothness of non-linear activation and the dimension of the input data.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We prove the first upper bound on the sample complexity of active learning for training one-hidden layer neural networks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=PU3VGS93gxD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xO4xryFQltO" data-number="943">
        <h4>
          <a href="https://openreview.net/forum?id=xO4xryFQltO">
              A new perspective on probabilistic image modeling
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xO4xryFQltO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Alexander_Gepperth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Gepperth1">Alexander Gepperth</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xO4xryFQltO-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xO4xryFQltO-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep mixture models, sum-product networks, probabilistic circuits, image modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new probabilistic approach for image modeling capable of density estimation, sampling and tractable inference. DCGMM instances exhibit a CNN-like layered structure, in which the principal building  blocks are convolutional Gaussian Mixture (cGMM) layers. A key innovation w.r.t. related models lile sum-produdct networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer optimizes an independent loss function and therefore has an independent probabilistic interpretation. This modular approach permits intervening transformation layers to harness the full spectrum of 
        (potentially non-invertible) mappings available to CNNs, e.g., max-pooling or (half-)convolutions. DCGMM sampling and inference are realized by a deep chain of hierarchical priors, where samples generated by each cGMM layer parameterize sampling in the next-lower cGMM layer. For sampling through non-invertible transformation layers, we introduce a new gradient-based sharpening technique that exploits redundancy (overlap) in, e.g., half-convolutions. The basic quantities forward-transported through a DCGMM instance are the posterior probabilities of cGMM layers, which ensures numerical stability and facilitates the selection of learning rates.
        DCGMMs can be trained end-to-end by SGD from random initial conditions, much like CNNs. We experimentally show that DCGMMs compare favorably to several recent PC and SPN models in terms of inference, classification and sampling, the latter particularly for challenging datasets such as SVHN. A public TF2 implementation is provided as well.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A conceptually new approach for probabilistic image modeling based on multiple linked GMMs, which can generate samples of excellent quality w.r.t. related approaches, particularly for SVHN.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UajXTGRjuKB" data-number="840">
        <h4>
          <a href="https://openreview.net/forum?id=UajXTGRjuKB">
              Sampling Before Training: Rethinking the Effect of Edges in the Process of Training Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UajXTGRjuKB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Hengyuan_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hengyuan_Ma1">Hengyuan Ma</a>, <a href="https://openreview.net/profile?email=xianmu.yq%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="xianmu.yq@antgroup.com">Qi Yang</a>, <a href="https://openreview.net/profile?email=wenxi.sbw%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="wenxi.sbw@antgroup.com">Bowen Sun</a>, <a href="https://openreview.net/profile?email=shunlong.wxd%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="shunlong.wxd@antgroup.com">Long Shun</a>, <a href="https://openreview.net/profile?email=kui.lijk%40antgroup.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="kui.lijk@antgroup.com">Junkui Li</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Feng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianfeng_Feng2">Jianfeng Feng</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UajXTGRjuKB-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UajXTGRjuKB-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks (GNN) demonstrate excellent performance on many graph-based tasks; however, they also impose a heavy computational burden when trained on a large-scale graph. Although various sampling methods have been proposed to speed up training GNN by shrinking the scale of the graph during training, they become unavailable if we need to perform sampling before training. In this paper, we quantify the importance of every edge for training in the graph with the extra information they convey in addition to the node features, as inspired by a manifold learning algorithm called diffusion map. Based on this calculation, we propose Graph Diffusion Sampling (GDS), a simple but effective sampling method for shrinking the size of the edge set before training. GDS prefers to sample edges with high importance, and edges dropped by GDS will never be used in the training procedure. We empirically show that GDS preserves the edges crucial for training in a variety of models (GCN, GraphSAGE, GAT, and JKNet). Compared to training on the full graph, GDS can guarantee the performance of the model while only samples a small fraction of the edges.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UajXTGRjuKB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="xREjEGUoY4c" data-number="761">
        <h4>
          <a href="https://openreview.net/forum?id=xREjEGUoY4c">
              Robot Intent Recognition Method Based on State Grid Business Office
          </a>
        
          
            <a href="https://openreview.net/pdf?id=xREjEGUoY4c" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Lanfang_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lanfang_Dong1">Lanfang Dong</a>, <a href="https://openreview.net/profile?id=~Zhao_Pu_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhao_Pu_Hu1">Zhao Pu Hu</a>, <a href="https://openreview.net/profile?id=~Hanchao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanchao_Liu1">Hanchao Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#xREjEGUoY4c-details-785" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xREjEGUoY4c-details-785"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial intelligence is currently in an era of change, not only changing the artificial intelligence technology itself, but also changing human society. It has become more and more common to use artificial intelligence as the core human-computer interaction technology to replace manpower. Intention recognition is an important part of the human-machine dialogue system, and deep learning technology is gradually being applied to the task of intent recognition. However, intent recognition based on deep learning often has problems such as low recognition accuracy and slow recognition speed. In response to these problems, this paper designs a BERT fine-tuning to improve the network structure based on the pre-training model and proposes new continuous pre-training goals. To improve the accuracy of intent recognition, a method based on multi-teacher model compression is proposed to compress the pre-training model, which reduces the time consumption of model inference.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="gTdmGt48ht1" data-number="691">
        <h4>
          <a href="https://openreview.net/forum?id=gTdmGt48ht1">
              On the Double Descent of Random Features Models Trained with SGD
          </a>
        
          
            <a href="https://openreview.net/pdf?id=gTdmGt48ht1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Fanghui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Fanghui_Liu1">Fanghui Liu</a>, <a href="https://openreview.net/profile?id=~Johan_Suykens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Johan_Suykens1">Johan Suykens</a>, <a href="https://openreview.net/profile?id=~Volkan_Cevher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Volkan_Cevher1">Volkan Cevher</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 04 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Desk Rejected Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#gTdmGt48ht1-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gTdmGt48ht1-details-805"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">random features, over-parameterized model, double descent, SGD</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study generalization properties of random features (RF) regression in high dimensions  optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds of RF regression under both constant and adaptive step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well in the interpolation setting, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm interpolator, as a theoretical justification of using SGD in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We show that, random features models trained with SGD in high dimensions still generalizes well for interpolation learning, recovers double descent, and incurs no loss in the excess risk when compared to the exact closed-form solution.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="fG9WttDhAaa" data-number="4723">
        <h4>
          <a href="https://openreview.net/forum?id=fG9WttDhAaa">
              Rethinking Positional Encoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=fG9WttDhAaa" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Jianqiao_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianqiao_Zheng1">Jianqiao Zheng</a>, <a href="https://openreview.net/profile?id=~Sameera_Ramasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sameera_Ramasinghe1">Sameera Ramasinghe</a>, <a href="https://openreview.net/profile?email=simon.lucey%40adelaide.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="simon.lucey@adelaide.edu.au">Simon Lucey</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#fG9WttDhAaa-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fG9WttDhAaa-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions.  Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=fG9WttDhAaa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="LZVXOnSrD0Y" data-number="4720">
        <h4>
          <a href="https://openreview.net/forum?id=LZVXOnSrD0Y">
              Pareto Frontier Approximation Network (PA-Net) Applied to Multi-objective TSP
          </a>
        
          
            <a href="https://openreview.net/pdf?id=LZVXOnSrD0Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ishaan_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ishaan_Mehta1">Ishaan Mehta</a>, <a href="https://openreview.net/profile?email=s.saeedi%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="s.saeedi@ryerson.ca">Sajad Saeedi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">10 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#LZVXOnSrD0Y-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LZVXOnSrD0Y-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotics, planning, TSP, RL, Multi Objective Optimization, Pareto Optimality</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-objective optimization is used in various areas of robotics like control, planning etc. Their solutions are dependent on multiple objective functions, which can be conflicting in nature. In such cases, the optimality is defined in terms of Pareto optimality. A set of these Pareto Optimal solutions in the objective space form a Pareto front (or frontier). Each solution has its own trade off. For instance, the travelling salesman problem (TSP) is used in robotics for task/resource allocation. Often this allocation is influenced by multiple objective functions and is solved using Multi-objective travelling salesman problem (MOTSP). In this work, we present PA-Net, a network that generates good approximations of the Pareto front for the multi-objective optimization problems. Our training framework is applicable to other multi-objective optimization problems; however, in this work, we focus on solving MOTSP.  Firstly, MOTSP is converted into a constrained optimization problem. We then train our network to solve this constrained problem using the Lagrangian relaxation and policy gradient. With PA-Net we are able to generate better quality Pareto fronts with fast inference times as compared to other learning based and classical methods. Finally, we present the application of PA-Net to find optimal visiting order in coverage planning.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">PA-Net: a network that approximates Pareto Frontier for Multi Objective TSP problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=LZVXOnSrD0Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="aJORhCrlYqu" data-number="4718">
        <h4>
          <a href="https://openreview.net/forum?id=aJORhCrlYqu">
              ARMCMC:  Online Bayesian Density Estimation of Model Parameters
          </a>
        
          
            <a href="https://openreview.net/pdf?id=aJORhCrlYqu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedram_Agand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedram_Agand1">Pedram Agand</a>, <a href="https://openreview.net/profile?id=~Mo_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mo_Chen1">Mo Chen</a>, <a href="https://openreview.net/profile?id=~Hamid_Taghirad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hamid_Taghirad1">Hamid Taghirad</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">30 Sept 2021 (modified: 01 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#aJORhCrlYqu-details-622" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aJORhCrlYqu-details-622"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian, Probabilistic approaches, MCMC, Hunt Crossley, parameter identification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although the Bayesian paradigm provides a rigorous framework to estimate the full probability distribution over unknown parameters,  its online  implementation can be challenging due to heavy computational costs. This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches. These shortcomings include: being solely able to account for Gaussian noise, being applicable to systems with linear in the parameters (LIP) constraint, or having requirements on persistence excitation (PE). In ARMCMC, we propose a variable jump distribution, which depends on a temporal forgetting factor.  This allows one to adjust the trade-off between exploitation and exploration, depending on whether there is an abrupt change to the parameter being estimated. We prove that ARMCMC requires fewer samples to achieve the same precision and reliability compared to conventional MCMC approaches.  We demonstrate our approach on two challenging benchmarks:  the estimation of parameters in a soft bending actuator and the Hunt-Crossley dynamic model. Our method shows at-least 70\% improvement in parameter point estimation accuracy and approximately 55\% reduction in tracking error of the value of interest compared to recursive least squares and conventional MCMC.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This paper proposes Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) which estimates full probability density of model parameters while alleviating  shortcomings of conventional online approaches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=aJORhCrlYqu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Xd6T7cT7vwj" data-number="4673">
        <h4>
          <a href="https://openreview.net/forum?id=Xd6T7cT7vwj">
              Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning 
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Xd6T7cT7vwj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Marcus_L%C3%A5ng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marcus_Lång1">Marcus Lång</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Xd6T7cT7vwj-details-839" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xd6T7cT7vwj-details-839"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Strongly Self-Normalizing Neural Networks with Applications to Implicit Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent studies have show that wide neural networks with orthogonal linear layers and Gaussian Poincaré normalized activation functions avoid vanishing and exploding gradients for input vectors with the correct magnitude. This paper introduces a strengthening of the condition that the activation function must be Gaussian Poincaré normalized which creates robustness to deviations from standard normal distribution in the pre-activations, thereby reducing the dependence on the requirement that the network is wide and that the input vector has the correct magnitude. In implicit representation learning this allows the training of deep networks of this type where the linear layers are no longer constrained to be orthogonal linear transformations. Networks of this type can be fitted to a reference image to 1/10th the mean square error achievable with previous methods. Herein is also given an improved positional encoding for implicit representation learning of two-dimensional images and a small-batch training procedure for fitting of neural networks to images which allows fitting in fewer epochs, leading to substantial improvement in training time.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Similar to SIREN, but able to fit images to higher accuracy (PSNR=67 instead PSNR=50 for a specific reference image).</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="sHUFhv03qX_" data-number="4671">
        <h4>
          <a href="https://openreview.net/forum?id=sHUFhv03qX_">
              Q-Learning Scheduler for Multi-Task Learning through the use of Histogram of Task Uncertainty
          </a>
        
          
            <a href="https://openreview.net/pdf?id=sHUFhv03qX_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Kourosh_Meshgi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kourosh_Meshgi2">Kourosh Meshgi</a>, <a href="https://openreview.net/profile?id=~Maryam_Sadat_Mirzaei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maryam_Sadat_Mirzaei1">Maryam Sadat Mirzaei</a>, <a href="https://openreview.net/profile?id=~Satoshi_Sekine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Satoshi_Sekine1">Satoshi Sekine</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#sHUFhv03qX_-details-63" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sHUFhv03qX_-details-63"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Q-learning, Multi-Task Learning, MTL Scheduling, Histogram of Task Uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Simultaneous training of a multi-task learning network on different domains or tasks is not always straightforward. It could lead to inferior performance or generalization compared to the corresponding single-task networks. To maximally taking advantage of the benefits of multi-task learning, an effective training scheduling method is deemed necessary. Traditional schedulers follow a heuristic or prefixed strategy, ignoring the relation of the tasks, their sample complexities, and the state of the emergent shared features. We proposed a deep Q-Learning Scheduler (QLS) that monitors the state of the tasks and the shared features using a novel histogram of task uncertainty, and through trial-and-error, learns an optimal policy for task scheduling. Extensive experiments on multi-domain and multi-task settings with various task difficulty profiles have been conducted, the proposed method is benchmarked against other schedulers, its superior performance has been demonstrated, and results are discussed.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A deep Q-learning-based task scheduling method to improve multi-tasking learning based on a novel histogram of task uncertainty.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="lTiW8Jet8t" data-number="4664">
        <h4>
          <a href="https://openreview.net/forum?id=lTiW8Jet8t">
              Efficient Ensembles of Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=lTiW8Jet8t" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Amrit_Nagarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amrit_Nagarajan1">Amrit Nagarajan</a>, <a href="https://openreview.net/profile?id=~Jacob_R._Stevens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_R._Stevens1">Jacob R. Stevens</a>, <a href="https://openreview.net/profile?id=~Anand_Raghunathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anand_Raghunathan1">Anand Raghunathan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#lTiW8Jet8t-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lTiW8Jet8t-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) have enabled the power of deep learning to be applied to inputs beyond the Euclidean domain, with applications ranging from social networks and product recommendation engines to the life sciences. GNNs, like other classes of machine learning models, benefit from ensemble learning, wherein multiple models are combined to provide higher accuracy and robustness than single models. However, ensembles suffer from significantly higher inference processing and storage requirements, limiting their use in practical applications. In this work, we leverage the unique characteristics of GNNs to overcome these overheads, creating efficient ensemble GNNs that are faster than even single models at inference time. We observe that during message passing, nodes that are incorrectly classified (error nodes) also end up adversely affecting the representations of other nodes in their neighborhood. This error propagation also makes GNNs more difficult to approximate (e.g., through pruning) for efficient inference. We propose a technique to create ensembles of diverse models, and further propose Error Node Isolation (ENI), which prevents error nodes from sending messages to (and thereby influencing) other nodes. In addition to improving accuracy, ENI also leads to a significant reduction in the memory footprint and the number of arithmetic operations required to evaluate the computational graphs of all neighbors of error nodes. Remarkably, these savings outweigh even the overheads of using multiple models in the ensemble. A second key benefit of ENI is that it  enhances the resilience of GNNs to approximations. Consequently, we propose Edge Pruning and Network Pruning techniques that target both the input graph and the neural networks used to process the graph. Our experiments on GNNs for transductive and inductive node classification demonstrate that ensembles with ENI are simultaneously more accurate (by up to 4.6% and 3.8%) and  faster (by up to 2.8<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="113" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> and 5.7<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="114" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container>) when compared to the best-performing single models and ensembles without ENI, respectively. In addition, GNN ensembles with ENI are consistently more accurate than single models and ensembles without ENI when subject to pruning, leading to additional speedups of up to 5<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="115" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> with no loss in accuracy.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Ndffz5uo6H" data-number="4656">
        <h4>
          <a href="https://openreview.net/forum?id=Ndffz5uo6H">
              Updater-Extractor Architecture for Inductive World State Representations
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Ndffz5uo6H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Arsenii_Kirillovich_Moskvichev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Arsenii_Kirillovich_Moskvichev1">Arsenii Kirillovich Moskvichev</a>, <a href="https://openreview.net/profile?id=~James_A_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_A_Liu1">James A Liu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Ndffz5uo6H-details-122" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ndffz5uo6H-details-122"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">transformers, long-term-memory, sequential processing, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Developing sequential models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure. Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Proposing a theoretically and practically justified way to introduce persistent world state representations into transformer architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ndffz5uo6H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ys-bh0Eer_" data-number="4654">
        <h4>
          <a href="https://openreview.net/forum?id=ys-bh0Eer_">
              Block Contextual MDPs for Continual Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ys-bh0Eer_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Franziska_Meier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Franziska_Meier2">Franziska Meier</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joelle_Pineau1">Joelle Pineau</a>, <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amy_Zhang1">Amy Zhang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ys-bh0Eer_-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ys-bh0Eer_-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, MDP, Block Contextual MDP, Continual Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In reinforcement learning (RL), when defining a Markov Decision Process (MDP), the environment dynamics is implicitly assumed to be stationary. This assumption of stationarity, while simplifying, can be unrealistic in many scenarios. In the continual reinforcement learning scenario, the sequence of tasks is another source of nonstationarity. In this work, we propose to examine this continual reinforcement learning setting through the block contextual MDP (BC-MDP) framework, which enables us to relax the assumption of stationarity. This framework challenges RL algorithms to handle both nonstationarity and rich observation settings and, by additionally leveraging smoothness properties, enables us to study generalization bounds for this setting. Finally, we take inspiration from adaptive control to propose a novel algorithm that addresses the challenges introduced by this more realistic BC-MDP setting, allows for zero-shot adaptation at evaluation time, and achieves strong performance on several nonstationary environments.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We introduce the Lipschitz Block Contextual MDP framework for the continual RL setting and propose a representation learning algorithm that enables RL agents to generalize to non-stationary environments.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="FeaitX_a5Av" data-number="4645">
        <h4>
          <a href="https://openreview.net/forum?id=FeaitX_a5Av">
              GSD: Generalized Stochastic Decoding
          </a>
        
          
            <a href="https://openreview.net/pdf?id=FeaitX_a5Av" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ning_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ning_Gong1">Ning Gong</a>, <a href="https://openreview.net/profile?id=~Nianmin_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nianmin_Yao1">Nianmin Yao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 22 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">14 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#FeaitX_a5Av-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FeaitX_a5Av-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Processing, Decoding Algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although substantial progress has been made in various text generation tasks, there remains a vast gap between current generations and human languages. One reason is that virtually all decoding methods currently developed are pragmatic to address the text degeneration problem, which exists in both deterministic and stochastic decoding algorithms. So, why text generated from these algorithms are divergent? What is the critical difference between these algorithms? Moreover, is it possible to design a generalized framework where existing decoding algorithms can be naturally connected, uniformly described, and mutually inspired?
        In this paper, we try to explore answers to these intriguing questions. Correctly, we propose a generalized decoding framework that can be used to describe and connect existing popular decoding algorithms. Based on the framework, we propose a novel implementation with a distinctive core from existing decoding algorithms. As far as we know, this is the first work trying to propose a generalized framework to bridge these decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms. Experiments show that text produced by our method is closest to the characteristics of human languages. Source code and the generated text can be accessed from https://github.com/ginoailab/gsd.git.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A novel work proposing a generalized framework to connect existing decoding algorithms using formal theorems and concrete implementations. By setting up different conditions, our framework provides infinite space to develop new decoding algorithms.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="JgmY4TUgznC" data-number="4641">
        <h4>
          <a href="https://openreview.net/forum?id=JgmY4TUgznC">
              Few-Shot Multi-task Learning via Implicit regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=JgmY4TUgznC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dongsung_Huh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dongsung_Huh1">Dongsung Huh</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#JgmY4TUgznC-details-11" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JgmY4TUgznC-details-11"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Few Shot Learning, Learning Instability</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern machine learning is highly data-intensive. Few-shot learning (FSL) aims to resolve this sample efficiency problem by learning from multiple tasks and quickly adapt to new tasks containing only a few samples. However,  FSL problems proves to be significantly more challenging and require more compute expensive process to optimize. In this work, we consider multi-task linear regression (MTLR) as a canonical problem for few-shot learning, and investigate the source of challenge of FSL. We find that the MTLR exhibits local minimum problems that are not present in single-task problem, and thus making the learning much more challenging. We also show that the problem can be resolved by  overparameterizing the  model by increasing both the width and depth of the linear network and initializing the weights with small values, exploiting the implicit regularization bias of gradient descent-based learning.  </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UzOEYQM-xTg" data-number="4635">
        <h4>
          <a href="https://openreview.net/forum?id=UzOEYQM-xTg">
              Robust Long-Tailed Learning under Label Noise
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UzOEYQM-xTg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Tong_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tong_Wei1">Tong Wei</a>, <a href="https://openreview.net/profile?id=~Jiang-Xin_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiang-Xin_Shi1">Jiang-Xin Shi</a>, <a href="https://openreview.net/profile?id=~Wei-Wei_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei-Wei_Tu1">Wei-Wei Tu</a>, <a href="https://openreview.net/profile?id=~Yu-Feng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu-Feng_Li1">Yu-Feng Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UzOEYQM-xTg-details-779" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UzOEYQM-xTg-details-779"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weakly-supervised learning, long-tailed learning, learning with noisy labels, semi-supervised learning, multi-label learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework,~\algo, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on both benchmark and real-world datasets demonstrate substantial improvement over many existing methods. For example, \algo\ outperforms baselines by more than 5\% in test accuracy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=UzOEYQM-xTg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="a3NaSCJ20V" data-number="4634">
        <h4>
          <a href="https://openreview.net/forum?id=a3NaSCJ20V">
              Equivariant Grasp learning In Real Time
          </a>
        
          
            <a href="https://openreview.net/pdf?id=a3NaSCJ20V" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Xupeng_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xupeng_Zhu1">Xupeng Zhu</a>, <a href="https://openreview.net/profile?id=~Dian_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dian_Wang1">Dian Wang</a>, <a href="https://openreview.net/profile?id=~Ondrej_Biza1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ondrej_Biza1">Ondrej Biza</a>, <a href="https://openreview.net/profile?id=~Robert_Platt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_Platt1">Robert Platt</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#a3NaSCJ20V-details-55" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a3NaSCJ20V-details-55"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robotic Grasping, Equivariance, Reinforcement Leanring</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual grasp detection is a key problem in robotics where the agent must learn to model the grasp function, a mapping from an image of a scene onto a set of feasible grasp poses. In this paper, we recognize that the grasp function is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="116" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">SE</mi></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>-equivariant and that it can be modeled using an equivariant convolutional neural network. As a result, we are able to significantly improve the sample efficiency of grasp learning to the point where we can learn a good approximation of the grasp function within only 500 grasp experiences. This is fast enough that we can learn to grasp completely on a physical robot in about an hour. </span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="I13PP8-cdvz" data-number="4632">
        <h4>
          <a href="https://openreview.net/forum?id=I13PP8-cdvz">
              SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
          </a>
        
          
            <a href="https://openreview.net/pdf?id=I13PP8-cdvz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Sheng_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sheng_Cheng1">Sheng Cheng</a>, <a href="https://openreview.net/profile?id=~Yi_Ren3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Ren3">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yezhou_Yang1">Yezhou Yang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">5 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#I13PP8-cdvz-details-859" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I13PP8-cdvz-details-859"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stroke-based representation, Spatial robustness, Robust feature learning, Novel pattern generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing end-to-end visual recognition models do not possess innate spatial invariance and are thus vulnerable to out-of-training attacks. This suggests the need of a better representation design. This paper follows existing cognitive studies to investigate a sketch representation that specify stroke information on vertices and inter-stroke information on edges. The resultant representation, combined with a graph neural network, achieves both high classification accuracy and high robustness against translation, rotation, and stroke-wise parametric and topological attacks thanks to the use of spatially invariant stroke features and GNN architecture. While prior studies demonstrated similar sketch representations for classification and generation, these attempts heavily relied on run-time statistical inference rather than more efficient bottom-up computation via GNN. The presented sketch representation poses good structured expression capability as it enables generation of sketches semantically different from the training dataset.  Lastly, we show SSR-GNNs are able to accomplish all tasks (classification, robust feature learning, and novel pattern generation), which shows that the representation is task-agnostic. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">The paper presents a Stroke-based Sketch Representation with Graph Neural Networks which is spatially robust, with structured expression capability and is task-agnostic.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="RSd79AULOu" data-number="4625">
        <h4>
          <a href="https://openreview.net/forum?id=RSd79AULOu">
              Fairness-aware Federated Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=RSd79AULOu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhuozhuo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuozhuo_Tu1">Zhuozhuo Tu</a>, <a href="https://openreview.net/profile?id=~zhiqiang_xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~zhiqiang_xu1">zhiqiang xu</a>, <a href="https://openreview.net/profile?id=~Tairan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tairan_Huang1">Tairan Huang</a>, <a href="https://openreview.net/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dacheng_Tao1">Dacheng Tao</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ping_Li3">Ping Li</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">17 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#RSd79AULOu-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RSd79AULOu-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Learning Theory</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized. In such a setting, naively minimizing an aggregate loss may introduce bias and disadvantage model performance on certain clients. To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set. By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable. We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees. We further prove generalization bounds for learning with this objective. Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a new framework to address the fairness issues in federated learning and provide theoretical guarantees.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="edqz84cQ79T" data-number="4622">
        <h4>
          <a href="https://openreview.net/forum?id=edqz84cQ79T">
              Shaping latent representations using Self-Organizing Maps with Relevance Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=edqz84cQ79T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Pedro_Braga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pedro_Braga1">Pedro Braga</a>, <a href="https://openreview.net/profile?email=hrm%40cin.ufpe.br" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="hrm@cin.ufpe.br">Heitor Medeiros</a>, <a href="https://openreview.net/profile?id=~Hansenclever_Bassani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hansenclever_Bassani1">Hansenclever Bassani</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#edqz84cQ79T-details-367" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="edqz84cQ79T-details-367"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Clustering, Learning Prototypes, Topological Representations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work indicates that Deep Clustering (DC) methods are a viable option for unsupervised representations learning of visual features. By combining representation learning and clustering, traditional approaches have been shown to build latent representations that capture essential features of the data while preserving topological characteristics. In this sense, models based on Self-Organizing Maps models with relevance learning (SOMRL) were considered as they perform well in clustering besides being able to create a map that learns the relevance of each input dimension for each cluster, preserving the original relations and topology of the data. We hypothesize that this type of model can produce a more intuitive and disentangled representation in the latent space by promoting smoother transitions between cluster points over time. This work proposes a representation learning framework that combines a new gradient-based SOMRL model and autoencoders. The SOMRL learns the relevance weights for each input dimension of each cluster. It creates a tendency to separate the information into subspaces. To achieve this, we designed a new loss function term that weighs these learned relevances and provides an estimated unsupervised error to be used in combination with a reconstruction loss. The model is evaluated in terms of clustering performance and quality of the learned representations and then compared with start-of-the-art models, showing competitive results.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">This work proposes a representation learning framework that combines a new Self-Organizing Maps with autoencoders to shape their latent spaces into cluster prototypes living in separate subspaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=edqz84cQ79T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="eAEcdRkcMHh" data-number="4611">
        <h4>
          <a href="https://openreview.net/forum?id=eAEcdRkcMHh">
              HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N:M Sparsity and Integer Quantization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=eAEcdRkcMHh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Minjia_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minjia_Zhang1">Minjia Zhang</a>, <a href="https://openreview.net/profile?id=~Connor_Holmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Connor_Holmes1">Connor Holmes</a>, <a href="https://openreview.net/profile?id=~Yuxiong_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuxiong_He1">Yuxiong He</a>, <a href="https://openreview.net/profile?id=~Bo_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Wu1">Bo Wu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#eAEcdRkcMHh-details-651" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eAEcdRkcMHh-details-651"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Efficient Inference, N:M Sparsification, Quantization, Transformer networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks. However, the huge size of these models brings significant challenges to fine-tuning and online deployment due to latency and cost constraints. Recently, hardware manufacturers have released new architectures that support efficient N:M sparsity and low-precision integer computation for fast inferencing. In contrast to unstructured sparsity, N:M sparsity specifies that out of each chunk of N contiguous weight parameters, exactly M parameters are non-zero. Moreover, these architectures also support processing data with reduced precision, such as INT8. Prior work often considers inducing N:M sparsity and integer quantization in isolation or as independent pieces of a compression pipeline. However, there lacks a systematic investigation towards how N:M sparsity and integer quantization can be effectively combined to exploit the maximum degree of redundancy and enable even faster acceleration for pre-trained Transformer networks.
        
        In this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM). We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and
        solved in a unified manner. When evaluated across the GLUE suite of NLP benchmarks, our approach outperforms baselines that consider each of these problems independently, retaining 99.4\% accuracy of the dense baseline while being able to execute on newly released hardware effectively. </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">HoloFormer is a unified and systematic approach to learn N:M sparsity and integer quantization for compressing pre-trained Transformer networks</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ad_F_z27pCx" data-number="4595">
        <h4>
          <a href="https://openreview.net/forum?id=ad_F_z27pCx">
              A Discussion On the Validity of Manifold Learning
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ad_F_z27pCx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Dai_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dai_Shi1">Dai Shi</a>, <a href="https://openreview.net/profile?id=~Andi_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andi_Han1">Andi Han</a>, <a href="https://openreview.net/profile?id=~Yi_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yi_Guo3">Yi Guo</a>, <a href="https://openreview.net/profile?id=~Junbin_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Junbin_Gao1">Junbin Gao</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 30 Sept 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ad_F_z27pCx-details-132" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ad_F_z27pCx-details-132"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Manifold learning, Dimensionality Reduction, Computational Geometry, Simplicial Complex</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition (orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any -simplex decomposition result on a -manifold.  However,  constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="SK1nec-Ehd" data-number="4584">
        <h4>
          <a href="https://openreview.net/forum?id=SK1nec-Ehd">
              PulseImpute: A Novel Benchmark Task and Architecture for Imputation of Physiological Signals
          </a>
        
          
            <a href="https://openreview.net/pdf?id=SK1nec-Ehd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Maxwell_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maxwell_Xu1">Maxwell Xu</a>, <a href="https://openreview.net/profile?id=~Alexander_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alexander_Moreno1">Alexander Moreno</a>, <a href="https://openreview.net/profile?id=~James_Matthew_Rehg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Matthew_Rehg1">James Matthew Rehg</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#SK1nec-Ehd-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SK1nec-Ehd-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">missingness, imputation, mHealth, sensors, transformer, self-attention</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Providing care for patients with chronic diseases is one of the biggest drivers of the nation’s rising healthcare costs, but many of these diseases are linked to mutable health behaviors. Mobile health (mHealth) biophysical sensors that continuously measure our current conditions provide the framework for a personalized guidance system for the maintenance of healthy behaviors. However, this physiological sensor data is plagued with missingness due to insecure attachments, wireless dropout, battery, and adherence issues. These issues cripple their rich diagnostic utility as well as their ability to enable temporally-precise interventions. While there is a sizable amount of research focusing on imputation methods, surprisingly, no works have addressed the patterns of missingness, quasi-periodic signal structure, and the between subject heterogeneity that characterizes physiological signals in mHealth applications. We present the PulseImpute Challenge, the first challenge dataset for physiological signal imputation which includes a large set of baselines' performances on realistic missingness models and data. Next, we demonstrate the potential to address this quasi-periodic structure and heterogeneity with our Dilated Convolution Bottleneck (DCB) Transformer, a transformer architecture with a self-attention mechanism that is able to attend to corresponding waveform features in quasi-periodic signals. By utilizing stacked dilated convolutions with bottleneck layers for query and key transformations, we visually demonstrate that the kernel similarity in the attention model gives high similarity to similar temporal features across quasi-periodic periods. We hope the release of our challenge task definitions and baseline implementations will spur the community to address this challenging and important problem. 
         </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We present PulseImpute, a benchmarking challenge for the imputation of biophysical signals, and propose a novel self-attention module for attending over quasi-periodic signals.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="dhLChxJwgMR" data-number="4578">
        <h4>
          <a href="https://openreview.net/forum?id=dhLChxJwgMR">
              HFSP: A Hardware-friendly Soft Pruning Framework for Vision Transformers
          </a>
        
          
            <a href="https://openreview.net/pdf?id=dhLChxJwgMR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zhenglun_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhenglun_Kong1">Zhenglun Kong</a>, <a href="https://openreview.net/profile?id=~Peiyan_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peiyan_Dong1">Peiyan Dong</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaolong_Ma2">Xiaolong Ma</a>, <a href="https://openreview.net/profile?id=~Xin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xin_Meng1">Xin Meng</a>, <a href="https://openreview.net/profile?id=~Mengshu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mengshu_Sun1">Mengshu Sun</a>, <a href="https://openreview.net/profile?id=~Wei_Niu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wei_Niu3">Wei Niu</a>, <a href="https://openreview.net/profile?id=~Bin_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Ren1">Bin Ren</a>, <a href="https://openreview.net/profile?id=~Minghai_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minghai_Qin1">Minghai Qin</a>, <a href="https://openreview.net/profile?id=~Hao_Tang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Tang6">Hao Tang</a>, <a href="https://openreview.net/profile?id=~Yanzhi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yanzhi_Wang3">Yanzhi Wang</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">4 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#dhLChxJwgMR-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dhLChxJwgMR-details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision Transformers, Hardware-friendly, Soft Token Pruning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViT, and the related edge device deployment, we leverage the input token sparsity and propose a hardware-friendly soft pruning framework (HFSP), which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique to package the pruned tokens, which integrate the less informative tokens generated by the selector module into a package token, and participates in subsequent calculations rather than being discarded completely.  From a hardware standpoint, our framework is bound to the tradeoff between accuracy and specific hardware constraints through our proposed hardware-oriented progressive training, and all the operators embedded in the framework have been well-supported. Experimental results demonstrate that the proposed framework significantly reduces the computational costs of ViTs while maintaining comparable performance on image classification. For example, our method reduces the FLOPs of DeiT-S by over 42.6% while only sacrificing 0.46% top-1 accuracy. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. Code will be publicly released.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">A vision transformer pruning framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=dhLChxJwgMR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="znpOLJUYGcA" data-number="4568">
        <h4>
          <a href="https://openreview.net/forum?id=znpOLJUYGcA">
              Automatic Integration for Neural Temporal Point Process
          </a>
        
          
            <a href="https://openreview.net/pdf?id=znpOLJUYGcA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Zihao_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zihao_Zhou1">Zihao Zhou</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rose_Yu1">Rose Yu</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">1 Reply</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#znpOLJUYGcA-details-34" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="znpOLJUYGcA-details-34"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point process</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integration lies at the heart of the temporal point process. Due to the intrinsic mathematical difficulty of symbolic integration, neural temporal point process models either constrain the intensity function to an integrable functional form or apply certain numerical methods. However, the former type of model has limited expressive power, and the latter type of model suffers additional numerical errors and high computational costs. In this paper, we introduce automatic integration with neural point process models, a new paradigm for efficient, closed-form nonparametric inference of temporal point process characterized by any intensity function. We test the model against a variety of synthetic temporal point process datasets and show that the model can better capture inter-event intensity changes than state-of-the-art methods. We also identify certain model settings that would lead the MLE estimator for the temporal point process to be inconsistent.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">Leveraging automatic integration for more efficient and accurate recovery of temporal point process's intensity</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="ZV7MoEj44Et" data-number="4556">
        <h4>
          <a href="https://openreview.net/forum?id=ZV7MoEj44Et">
              Measuring the Effectiveness of Self-Supervised Learning using Calibrated Learning Curves
          </a>
        
          
            <a href="https://openreview.net/pdf?id=ZV7MoEj44Et" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Andrei_Atanov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrei_Atanov1">Andrei Atanov</a>, <a href="https://openreview.net/profile?id=~Shijian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shijian_Xu1">Shijian Xu</a>, <a href="https://openreview.net/profile?email=onur.beker%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="onur.beker@epfl.ch">Onur Beker</a>, <a href="https://openreview.net/profile?id=~Andrey_Filatov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrey_Filatov1">Andrey Filatov</a>, <a href="https://openreview.net/profile?id=~Amir_Zamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amir_Zamir1">Amir Zamir</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 20 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#ZV7MoEj44Et-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZV7MoEj44Et-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Self-Supervised Learning, Transfer Learning, Metric, Evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-supervised learning has witnessed remarkable progress in recent years, in particular with the introduction of augmentation-based contrastive methods. While a number of large-scale empirical studies on the performance of self-supervised pre-training have been conducted, there isn't yet an agreed upon set of control baselines, evaluation practices, and metrics to report. We identify this as an important angle of investigation and propose an evaluation standard that aims to quantify and communicate transfer learning performance in an informative yet accessible setup. This is done by baking in a number of key control baselines in the evaluation method, particularly the blind guess (quantifying the dataset bias), the scratch model (quantifying the architectural contribution), and the gold standard (quantifying the upper-bound). We further provide a number of experiments to demonstrate how the proposed evaluation can be employed in empirical studies of basic questions -- for example, whether the effectiveness of existing self-supervised learning methods is skewed towards image classification versus other tasks, such as dense pixel-wise predictions. 
        </span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose an evaluation standard for measuring the effectiveness of self-supervised learning based on incorporating important control baselines.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="UxTR9Z2DW8R" data-number="4552">
        <h4>
          <a href="https://openreview.net/forum?id=UxTR9Z2DW8R">
              Reinforcement Learning State Estimation for High-Dimensional Nonlinear Systems
          </a>
        
          
            <a href="https://openreview.net/pdf?id=UxTR9Z2DW8R" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Saviz_Mowlavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saviz_Mowlavi1">Saviz Mowlavi</a>, <a href="https://openreview.net/profile?id=~Mouhacine_Benosman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mouhacine_Benosman1">Mouhacine Benosman</a>, <a href="https://openreview.net/profile?id=~Saleh_Nabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saleh_Nabi1">Saleh Nabi</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 23 Nov 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">18 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#UxTR9Z2DW8R-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UxTR9Z2DW8R-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, partial differential equation, reduced order modeling, closure models, state prediction, state estimation, dynamic mode decomposition.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RL-ROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="V2WidtMGSRG" data-number="4516">
        <h4>
          <a href="https://openreview.net/forum?id=V2WidtMGSRG">
              Provable Identifiability of ReLU Neural Networks via Lasso Regularization
          </a>
        
          
            <a href="https://openreview.net/pdf?id=V2WidtMGSRG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Gen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gen_Li2">Gen Li</a>, <a href="https://openreview.net/profile?id=~Ganghua_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ganghua_Wang1">Ganghua Wang</a>, <a href="https://openreview.net/profile?id=~Yuantao_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuantao_Gu1">Yuantao Gu</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jie_Ding2">Jie Ding</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 05 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">7 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#V2WidtMGSRG-details-268" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V2WidtMGSRG-details-268"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Lasso, nonlinear regression, model selection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">LASSO regularization is a popular regression tool to enhance the prediction accuracy of statistical models by performing variable selection through the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="117" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> penalty, initially formulated for the linear model and its variants. In this paper, the territory of LASSO is extended to the neural network model, a fashionable and powerful nonlinear regression model. Specifically, given a neural network whose output <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="118" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container> depends only on a small subset of input <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="119" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-b mjx-i"><mjx-c class="mjx-c1D499 TEX-BI"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math></mjx-assistive-mml></mjx-container>, denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="120" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>⋆</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>, we prove that the LASSO estimator can stably reconstruct the neural network and identify <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="121" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mrow data-mjx-texclass="ORD"><mo>⋆</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> when the number of samples scales logarithmically with the input dimension. This challenging regime has been well understood for linear models while barely studied for neural networks. Our theory lies in an extended Restricted Isometry Property (RIP)-based analysis framework for two-layer ReLU neural networks, which may be of independent interest to other LASSO or neural network settings. Based on the result, we further propose a neural network-based variable selection method. Experiments on simulated and real-world datasets show the promising performance of our variable selection approach compared with classical techniques.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We theoretically show that the Lasso estimator can stably identify ReLU neural networks and then propose to use neural networks as vehicles to perform variable selection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="https://openreview.net/attachment?id=V2WidtMGSRG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
    <li class="note " data-id="Jh9VxCkrEZn" data-number="4515">
        <h4>
          <a href="https://openreview.net/forum?id=Jh9VxCkrEZn">
              Spatiotemporal Representation Learning on Time Series with Dynamic Graph ODEs
          </a>
        
          
            <a href="https://openreview.net/pdf?id=Jh9VxCkrEZn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR2022_poster_18_files/pdf_icon_blue.svg"></a>
          
          
        </h4>
        
        
        
        <div class="note-authors">
          <a href="https://openreview.net/profile?id=~Ming_Jin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming_Jin3">Ming Jin</a>, <a href="https://openreview.net/profile?id=~Yuan-Fang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yuan-Fang_Li1">Yuan-Fang Li</a>, <a href="https://openreview.net/profile?id=~Yu_Zheng5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yu_Zheng5">Yu Zheng</a>, <a href="https://openreview.net/profile?id=~Bin_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Yang4">Bin Yang</a>, <a href="https://openreview.net/profile?id=~Shirui_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shirui_Pan1">Shirui Pan</a>
        </div>
        
        <div class="note-meta-info">
          <span class="item date">29 Sept 2021 (modified: 06 Oct 2021)</span>
              <span class="item">ICLR 2022 Conference Withdrawn Submission</span>
          
            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
          
          
            <span class="item">6 Replies</span>
          
          
        </div>
        
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#Jh9VxCkrEZn-details-327" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jh9VxCkrEZn-details-327"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Spatiotemporal representation learning on multivariate time series has received tremendous attention in forecasting traffic and energy data. Recent works either rely on complicated discrete neural architectures or graph priors, hindering their effectiveness and applications in the real world. In this paper, inspired by neural ordinary differential equations and graph structure learning, we propose a fully continuous model named Dynamic Graph ODE (DyG-ODE) to capture both long-range spatial and temporal dependencies to learn expressive representations on arbitrary multivariate time series data without being restricted by rigid preconditions (e.g., graph priors). For modeling the continuous dynamics of spatiotemporal clues, we design a simple yet powerful dynamic graph ODE by coupling the proposed spatial and temporal ODEs, which not only allows the model to obtain infinite spatial and temporal receptive fields but also reduces numerical errors and model complexity significantly. Our empirical evaluations demonstrate the superior effectiveness and efficiency of DyG-ODE on a number of benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">One-sentence Summary:</strong>
              <span class="note-content-value ">We propose a fully continuous model named DyG-ODE to learn expressive spatiotemporal representations on arbitrary multivariate time series data</span>
            </li>
        </ul>
        </div></div>
        
        
        
        
    </li>
</ul>
<nav class="pagination-container text-center " aria-label="page navigation">
  <ul class="pagination">
      <li class="disabled  left-arrow" data-page-number="1">
          <span>«</span>
      </li>
      <li class="disabled  left-arrow" data-page-number="0">
          <span>‹</span>
      </li>
      <li class=" active " data-page-number="1">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">1</a>
      </li>
      <li class="  " data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">2</a>
      </li>
      <li class="  " data-page-number="3">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">3</a>
      </li>
      <li class="  " data-page-number="4">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">4</a>
      </li>
      <li class="  " data-page-number="5">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">5</a>
      </li>
      <li class="  " data-page-number="6">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">6</a>
      </li>
      <li class="  " data-page-number="7">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">7</a>
      </li>
      <li class="  " data-page-number="8">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">8</a>
      </li>
      <li class="  " data-page-number="9">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">9</a>
      </li>
      <li class="  " data-page-number="10">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">10</a>
      </li>
      <li class="  right-arrow" data-page-number="2">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">›</a>
      </li>
      <li class="  right-arrow" data-page-number="17">
          <a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#">»</a>
      </li>
  </ul>
</nav>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="recent-activity">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a href="https://openreview.net/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="https://openreview.net/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review, with legal nonprofit status through<!-- --> <a href="https://codeforscience.org/" target="_blank" rel="noopener noreferrer">Code for Science &amp; Society</a>. We gratefully acknowledge the support of the<!-- --> <a href="https://openreview.net/sponsors" target="_blank">OpenReview Sponsors</a>.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible. To submit a bug report or feature request, you can use the official OpenReview GitHub repository:<br><a href="https://github.com/openreview/openreview/issues/new/choose" target="_blank" rel="noreferrer">Report an issue</a></p><form><div class="form-group"><input type="email" id="feedback-from" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" id="feedback-subject" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea id="feedback-message" name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2022/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2022/Conference\n$(function() {\nvar args = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar group = {\"id\":\"ICLR.cc/2022/Conference\"};\nvar document = null;\nvar window = null;\n\n// TODO: remove these vars when all old webfields have been archived\nvar model = {\n  tokenPayload: function() {\n    return { user: user }\n  }\n};\nvar controller = {\n  get: Webfield.get,\n  addHandler: function(name, funcMap) {\n    Object.values(funcMap).forEach(function(func) {\n      func();\n    });\n  },\n};\n\n$('#group-container').empty();\n// START GROUP CODE\n// ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar CONFERENCE_ID = 'ICLR.cc/2022/Conference';\nvar PARENT_GROUP_ID = 'ICLR.cc/2022';\nvar SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';\nvar REVIEWERS_NAME = 'Reviewers';\nvar AREA_CHAIRS_NAME = 'Area_Chairs';\nvar AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';\nvar REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';\nvar PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';\nvar AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';\nvar HEADER = {\"title\": \"The Tenth International Conference on Learning Representations \", \"subtitle\": \"ICLR 2022\", \"location\": \"Virtual\", \"date\": \"Apr 25 2022\", \"website\": \"https://iclr.cc/Conferences/2022\", \"instructions\": \"\", \"deadline\": \"Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0\", \"contact\": \"iclr2022pc@gmail.com\"};\nvar PUBLIC = true;\n\nvar WILDCARD_INVITATION = CONFERENCE_ID + '/.*';\nvar BUFFER = 0;  // deprecated\nvar PAGE_SIZE = 50;\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true,\n  showTags: false\n};\nvar commentDisplayOptions = {\n  pdfLink: false,\n  replyCount: true,\n  showContents: false,\n  showParent: true\n};\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderSubmissionButton();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\n// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/\nfunction load() {\n\n  var spotlightNotesP = $.Deferred().resolve([]);\n  var oralNotesP = $.Deferred().resolve([]);\n  var posterNotesP = $.Deferred().resolve([]);\n  \n  \n  var activityNotesP = $.Deferred().resolve([]);\n  var authorNotesP = $.Deferred().resolve([]);\n  var userGroupsP = $.Deferred().resolve([]);\n  var withdrawnNotesP = $.Deferred().resolve([]);\n  var deskRejectedNotesP = $.Deferred().resolve([]);\n  var submittedNotesP = $.Deferred().resolve([]);\n\n  if (PUBLIC) {\n      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      //   pageSize: PAGE_SIZE,\n      //   details: 'replyCount',\n      //   includeCount: true\n      // });\n\n    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Spotlight',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Oral',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Poster',\n      details: 'replyCount',\n      includeCount: true\n    });\n  \n    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.venue': 'ICLR 2022 Submitted',\n      details: 'replyCount',\n      includeCount: true\n    });\n\n    if (WITHDRAWN_SUBMISSION_ID) {\n      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount',\n        includeCount: true\n      });\n    }\n\n    if (DESK_REJECTED_SUBMISSION_ID) {\n      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n        pageSize: PAGE_SIZE,\n        details: 'replyCount,invitation,original',\n        includeCount: true\n      });\n    }\n  }\n\n  if (user \u0026\u0026 !_.startsWith(user.id, 'guest_')) {\n    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {\n      pageSize: PAGE_SIZE,\n      details: 'forumContent,invitation,writable'\n    });\n\n    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });\n\n    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {\n      pageSize: PAGE_SIZE,\n      'content.authorids': user.profile.id\n    });\n  }\n\n  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);\n}\n\n// Render functions\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction renderSubmissionButton() {\n  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})\n    .then(function(invitation) {\n      Webfield.ui.submissionButton(invitation, user, {\n        onNoteCreated: function() {\n          // Callback funtion to be run when a paper has successfully been submitted (required)\n          if (PUBLIC) {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'A list of all submissions will be available after the deadline.');\n          } else {\n            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +\n            'The author console page for managing your submissions will be available soon.');\n          }\n\n          load().then(renderContent).then(function() {\n            $('.tabs-container a[href=\"#your-consoles\"]').click();\n          });\n        }\n      });\n    });\n}\n\nfunction renderConferenceTabs() {\n  var sections = [\n    {\n      heading: 'Your Consoles',\n      id: 'your-consoles',\n    }\n  ];\n\n  if (PUBLIC) {\n    // sections.push({\n    //   heading: 'All Submissions',\n    //   id: 'all-submissions',\n    // });\n    sections.push({\n      heading: 'Oral Presentations',\n      id: 'oral-submissions',\n    });\n    sections.push({\n      heading: 'Spotlight Presentations',\n      id: 'spotlight-submissions',\n    });\n    sections.push({\n      heading: 'Poster Presentations',\n      id: 'poster-submissions',\n    });\n    sections.push({\n      heading: 'Rejected Submissions',\n      id: 'submitted-submissions',\n    });\n    // if (WITHDRAWN_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Withdrawn Submissions',\n    //     id: 'withdrawn-submissions',\n    //   })\n    // }\n    // if (DESK_REJECTED_SUBMISSION_ID) {\n    //   sections.push({\n    //     heading: 'Desk Rejected Submissions',\n    //     id: 'desk-rejected-submissions',\n    //   })\n    // }\n    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {\n      sections.push({\n        heading: 'Desk Rejected/Withdrawn Submissions',\n        id: 'desk-rejected-withdrawn-submissions',\n      })\n    }\n  }\n\n  sections.push({\n    heading: 'Recent Activity',\n    id: 'recent-activity',\n  }\n)\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortBy(_.uniq(allGroups));\n  var links = [];\n  uniqueGroups.forEach(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n    links.push(\n      [\n        '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n        '\u003c/li\u003e'\n      ].join('')\n    );\n  });\n\n  $('#your-consoles .submissions-list').append(links);\n}\n\nfunction renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {\n\n  // Your Consoles tab\n  if (userGroups.length || authorNotes.length) {\n\n    var $container = $('#your-consoles').empty();\n    $container.append('\u003cul class=\"list-unstyled submissions-list\"\u003e');\n\n    var allConsoles = [];\n    if (authorNotes.length) {\n      allConsoles.push(AUTHORS_ID);\n    }\n    userGroups.forEach(function(group) {\n      allConsoles.push(group.id);\n    });\n\n    // Render all console links for the user\n    createConsoleLinks(allConsoles);\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n\n  // Oral Papers tab\n  var oralNotesCount = oralNotes.count || 0;\n  oralNotes = oralNotes.notes || [];\n\n  $('#oral-submissions').empty();\n\n  if (oralNotesCount) {\n    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#oral-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(oralNotes, {\n      heading: null,\n      container: '#oral-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Oral',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);\n          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: oralNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Oral',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#oral-submissions\"]').parent().hide();\n  }\n\n  // Spotlight Papers tab\n  var spotlightNotesCount = spotlightNotes.count || 0;\n  spotlightNotes = spotlightNotes.notes || [];\n\n  $('#spotlight-submissions').empty();\n\n  if (spotlightNotesCount) {\n    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#spotlight-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(spotlightNotes, {\n      heading: null,\n      container: '#spotlight-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Spotlight',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);\n          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: spotlightNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Spotlight',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#spotlight-submissions\"]').parent().hide();\n  }\n\n  // Poster Papers tab\n  var posterNotesCount = posterNotes.count || 0;\n  posterNotes = posterNotes.notes || [];\n\n  $('#poster-submissions').empty();\n\n  if (posterNotesCount) {\n    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#poster-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(posterNotes, {\n      heading: null,\n      container: '#poster-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Poster',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);\n          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: posterNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Poster',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#poster-submissions\"]').parent().hide();\n  }\n\n  // Rejected Papers tab\n  var submittedNotesCount = submittedNotes.count || 0;\n  submittedNotes = submittedNotes.notes || [];\n\n  $('#submitted-submissions').empty();\n\n  if (submittedNotesCount) {\n    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {\n      container: '#submitted-submissions',\n      autoLoad: false\n    });\n\n    Webfield.ui.submissionList(submittedNotes, {\n      heading: null,\n      container: '#submitted-submissions',\n      search: {\n        enabled: true,\n        venue: 'ICLR 2022 Submitted',\n        localSearch: false,\n        invitation: BLIND_SUBMISSION_ID,\n        onResults: function(searchResults) {\n          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);\n        },\n        onReset: function() {\n          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);\n          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));\n        }\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: submittedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {\n          'content.venue': 'ICLR 2022 Submitted',\n          details: 'replyCount',\n          pageSize: PAGE_SIZE,\n          offset: offset\n        });\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#submitted-submissions\"]').parent().hide();\n  }\n\n  // Activity Tab\n  if (activityNotes.length) {\n    var displayOptions = {\n      container: '#recent-activity',\n      user: user \u0026\u0026 user.profile,\n      showActionButtons: true\n    };\n\n    $(displayOptions.container).empty();\n\n    Webfield.ui.activityList(activityNotes, displayOptions);\n\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#recent-activity\"]').parent().hide();\n  }\n  \n  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))\n  \n  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);\n  if (removedNotesCount) {\n    $('#desk-rejected-withdrawn-submissions').empty();\n\n    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);\n    Webfield.ui.submissionList(removedNotesArray, {\n      heading: null,\n      container: '#desk-rejected-withdrawn-submissions',\n      search: {\n        enabled: false\n      },\n      displayOptions: paperDisplayOptions,\n      autoLoad: false,\n      noteCount: removedNotesCount,\n      pageSize: PAGE_SIZE,\n      onPageClick: function(offset) {\n        if (offset \u003c deskRejectedNotes.count) {\n          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset\n          });\n        } else {\n          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n            details: 'replyCount,invitation,original',\n            pageSize: PAGE_SIZE,\n            offset: offset - roundedDeskRejectedNotes\n          });\n        }\n      },\n      fadeIn: false\n    });\n  } else {\n    $('.tabs-container a[href=\"#desk-rejected-withdrawn-submissions\"]').parent().hide();\n  }\n\n  // var withdrawnNotesCount = withdrawnNotes.count || 0;\n  // if (withdrawnNotesCount) {\n  //   $('#withdrawn-submissions').empty();\n\n  //   var withdrawnNotesArray = withdrawnNotes.notes || [];\n  //   Webfield.ui.submissionList(withdrawnNotesArray, {\n  //     heading: null,\n  //     container: '#withdrawn-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: withdrawnNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#withdrawn-submissions\"]').parent().hide();\n  // }\n\n  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;\n  // if (deskRejectedNotesCount) {\n  //   $('#desk-rejected-submissions').empty();\n\n  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];\n  //   Webfield.ui.submissionList(deskRejectedNotesArray, {\n  //     heading: null,\n  //     container: '#desk-rejected-submissions',\n  //     search: {\n  //       enabled: false\n  //     },\n  //     displayOptions: paperDisplayOptions,\n  //     autoLoad: false,\n  //     noteCount: deskRejectedNotesCount,\n  //     pageSize: PAGE_SIZE,\n  //     onPageClick: function(offset) {\n  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {\n  //         details: 'replyCount,invitation,original',\n  //         pageSize: PAGE_SIZE,\n  //         offset: offset\n  //       });\n  //     },\n  //     fadeIn: false\n  //   });\n  // } else {\n  //   $('.tabs-container a[href=\"#desk-rejected-submissions\"]').parent().hide();\n  // }\n\n  $('#notes .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n// END GROUP CODE\n});\n//# sourceURL=webfieldCode.js","writable":false,"query":{"id":"ICLR.cc/2022/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2022/Conference"},"buildId":"v1.5.0-1-g2b07894","isFallback":false,"gip":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;">ICLR 2022 Conference | OpenReview</p></next-route-announcer><script>// Webfield Code for ICLR.cc/2022/Conference
$(function() {
var args = {"id":"ICLR.cc/2022/Conference"};
var group = {"id":"ICLR.cc/2022/Conference"};
var document = null;
var window = null;

// TODO: remove these vars when all old webfields have been archived
var model = {
  tokenPayload: function() {
    return { user: user }
  }
};
var controller = {
  get: Webfield.get,
  addHandler: function(name, funcMap) {
    Object.values(funcMap).forEach(function(func) {
      func();
    });
  },
};

$('#group-container').empty();
// START GROUP CODE
// ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var CONFERENCE_ID = 'ICLR.cc/2022/Conference';
var PARENT_GROUP_ID = 'ICLR.cc/2022';
var SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Submission';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2022/Conference/-/Desk_Rejected_Submission';
var REVIEWERS_NAME = 'Reviewers';
var AREA_CHAIRS_NAME = 'Area_Chairs';
var AREA_CHAIRS_ID = 'ICLR.cc/2022/Conference/Area_Chairs';
var REVIEWERS_ID = 'ICLR.cc/2022/Conference/Reviewers';
var PROGRAM_CHAIRS_ID = 'ICLR.cc/2022/Conference/Program_Chairs';
var AUTHORS_ID = 'ICLR.cc/2022/Conference/Authors';
var HEADER = {"title": "The Tenth International Conference on Learning Representations ", "subtitle": "ICLR 2022", "location": "Virtual", "date": "Apr 25 2022", "website": "https://iclr.cc/Conferences/2022", "instructions": "", "deadline": "Submission Start: Sep 14 2021 12:00AM UTC-0, Abstract Registration: Sep 28 2021 11:59PM UTC-0, End: Oct 05 2021 11:59PM UTC-0", "contact": "iclr2022pc@gmail.com"};
var PUBLIC = true;

var WILDCARD_INVITATION = CONFERENCE_ID + '/.*';
var BUFFER = 0;  // deprecated
var PAGE_SIZE = 50;

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true,
  showTags: false
};
var commentDisplayOptions = {
  pdfLink: false,
  replyCount: true,
  showContents: false,
  showParent: true
};

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }

  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderSubmissionButton();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
// It returns a jQuery deferred object: https://api.jquery.com/category/deferred-object/
function load() {

  var spotlightNotesP = $.Deferred().resolve([]);
  var oralNotesP = $.Deferred().resolve([]);
  var posterNotesP = $.Deferred().resolve([]);
  
  
  var activityNotesP = $.Deferred().resolve([]);
  var authorNotesP = $.Deferred().resolve([]);
  var userGroupsP = $.Deferred().resolve([]);
  var withdrawnNotesP = $.Deferred().resolve([]);
  var deskRejectedNotesP = $.Deferred().resolve([]);
  var submittedNotesP = $.Deferred().resolve([]);

  if (PUBLIC) {
      // notesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      //   pageSize: PAGE_SIZE,
      //   details: 'replyCount',
      //   includeCount: true
      // });

    spotlightNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Spotlight',
      details: 'replyCount',
      includeCount: true
    });
  
    oralNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Oral',
      details: 'replyCount',
      includeCount: true
    });
  
    posterNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Poster',
      details: 'replyCount',
      includeCount: true
    });
  
    submittedNotesP = Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.venue': 'ICLR 2022 Submitted',
      details: 'replyCount',
      includeCount: true
    });

    if (WITHDRAWN_SUBMISSION_ID) {
      withdrawnNotesP = Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount',
        includeCount: true
      });
    }

    if (DESK_REJECTED_SUBMISSION_ID) {
      deskRejectedNotesP = Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
        pageSize: PAGE_SIZE,
        details: 'replyCount,invitation,original',
        includeCount: true
      });
    }
  }

  if (user && !_.startsWith(user.id, 'guest_')) {
    activityNotesP = Webfield.api.getSubmissions(WILDCARD_INVITATION, {
      pageSize: PAGE_SIZE,
      details: 'forumContent,invitation,writable'
    });

    userGroupsP = Webfield.getAll('/groups', { regex: CONFERENCE_ID + '/.*', member: user.id, web: true });

    authorNotesP = Webfield.api.getSubmissions(SUBMISSION_ID, {
      pageSize: PAGE_SIZE,
      'content.authorids': user.profile.id
    });
  }

  return $.when(spotlightNotesP, oralNotesP, posterNotesP, submittedNotesP, userGroupsP, activityNotesP, authorNotesP, withdrawnNotesP, deskRejectedNotesP);
}

// Render functions
function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);

  Webfield.ui.spinner('#notes', { inline: true });
}

function renderSubmissionButton() {
  Webfield.api.getSubmissionInvitation(SUBMISSION_ID, {deadlineBuffer: BUFFER})
    .then(function(invitation) {
      Webfield.ui.submissionButton(invitation, user, {
        onNoteCreated: function() {
          // Callback funtion to be run when a paper has successfully been submitted (required)
          if (PUBLIC) {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'A list of all submissions will be available after the deadline.');
          } else {
            promptMessage('Your submission is complete. Check your inbox for a confirmation email. ' +
            'The author console page for managing your submissions will be available soon.');
          }

          load().then(renderContent).then(function() {
            $('.tabs-container a[href="#your-consoles"]').click();
          });
        }
      });
    });
}

function renderConferenceTabs() {
  var sections = [
    {
      heading: 'Your Consoles',
      id: 'your-consoles',
    }
  ];

  if (PUBLIC) {
    // sections.push({
    //   heading: 'All Submissions',
    //   id: 'all-submissions',
    // });
    sections.push({
      heading: 'Oral Presentations',
      id: 'oral-submissions',
    });
    sections.push({
      heading: 'Spotlight Presentations',
      id: 'spotlight-submissions',
    });
    sections.push({
      heading: 'Poster Presentations',
      id: 'poster-submissions',
    });
    sections.push({
      heading: 'Rejected Submissions',
      id: 'submitted-submissions',
    });
    // if (WITHDRAWN_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Withdrawn Submissions',
    //     id: 'withdrawn-submissions',
    //   })
    // }
    // if (DESK_REJECTED_SUBMISSION_ID) {
    //   sections.push({
    //     heading: 'Desk Rejected Submissions',
    //     id: 'desk-rejected-submissions',
    //   })
    // }
    if (WITHDRAWN_SUBMISSION_ID || DESK_REJECTED_SUBMISSION_ID) {
      sections.push({
        heading: 'Desk Rejected/Withdrawn Submissions',
        id: 'desk-rejected-withdrawn-submissions',
      })
    }
  }

  sections.push({
    heading: 'Recent Activity',
    id: 'recent-activity',
  }
)

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortBy(_.uniq(allGroups));
  var links = [];
  uniqueGroups.forEach(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }
    links.push(
      [
        '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
        '</li>'
      ].join('')
    );
  });

  $('#your-consoles .submissions-list').append(links);
}

function renderContent(spotlightNotes, oralNotes, posterNotes, submittedNotes, userGroups, activityNotes, authorNotes, withdrawnNotes, deskRejectedNotes) {

  // Your Consoles tab
  if (userGroups.length || authorNotes.length) {

    var $container = $('#your-consoles').empty();
    $container.append('<ul class="list-unstyled submissions-list">');

    var allConsoles = [];
    if (authorNotes.length) {
      allConsoles.push(AUTHORS_ID);
    }
    userGroups.forEach(function(group) {
      allConsoles.push(group.id);
    });

    // Render all console links for the user
    createConsoleLinks(allConsoles);

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }


  // Oral Papers tab
  var oralNotesCount = oralNotes.count || 0;
  oralNotes = oralNotes.notes || [];

  $('#oral-submissions').empty();

  if (oralNotesCount) {
    var oralSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#oral-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(oralNotes, {
      heading: null,
      container: '#oral-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Oral',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, oralSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(oralNotes, oralSearchResultsListOptions);
          $('#oral-submissions').append(view.paginationLinks(oralNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: oralNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Oral',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#oral-submissions"]').parent().hide();
  }

  // Spotlight Papers tab
  var spotlightNotesCount = spotlightNotes.count || 0;
  spotlightNotes = spotlightNotes.notes || [];

  $('#spotlight-submissions').empty();

  if (spotlightNotesCount) {
    var spotlightSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#spotlight-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(spotlightNotes, {
      heading: null,
      container: '#spotlight-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Spotlight',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, spotlightSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(spotlightNotes, spotlightSearchResultsListOptions);
          $('#spotlight-submissions').append(view.paginationLinks(spotlightNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: spotlightNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Spotlight',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#spotlight-submissions"]').parent().hide();
  }

  // Poster Papers tab
  var posterNotesCount = posterNotes.count || 0;
  posterNotes = posterNotes.notes || [];

  $('#poster-submissions').empty();

  if (posterNotesCount) {
    var poterSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#poster-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(posterNotes, {
      heading: null,
      container: '#poster-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Poster',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, poterSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(posterNotes, poterSearchResultsListOptions);
          $('#poster-submissions').append(view.paginationLinks(posterNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: posterNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Poster',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#poster-submissions"]').parent().hide();
  }

  // Rejected Papers tab
  var submittedNotesCount = submittedNotes.count || 0;
  submittedNotes = submittedNotes.notes || [];

  $('#submitted-submissions').empty();

  if (submittedNotesCount) {
    var submittedSearchResultsListOptions = _.assign({}, paperDisplayOptions, {
      container: '#submitted-submissions',
      autoLoad: false
    });

    Webfield.ui.submissionList(submittedNotes, {
      heading: null,
      container: '#submitted-submissions',
      search: {
        enabled: true,
        venue: 'ICLR 2022 Submitted',
        localSearch: false,
        invitation: BLIND_SUBMISSION_ID,
        onResults: function(searchResults) {
          Webfield.ui.searchResults(searchResults, submittedSearchResultsListOptions);
        },
        onReset: function() {
          Webfield.ui.searchResults(submittedNotes, submittedSearchResultsListOptions);
          $('#submitted-submissions').append(view.paginationLinks(submittedNotesCount, PAGE_SIZE, 1));
        }
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: submittedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        return Webfield.api.getSubmissions(BLIND_SUBMISSION_ID, {
          'content.venue': 'ICLR 2022 Submitted',
          details: 'replyCount',
          pageSize: PAGE_SIZE,
          offset: offset
        });
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#submitted-submissions"]').parent().hide();
  }

  // Activity Tab
  if (activityNotes.length) {
    var displayOptions = {
      container: '#recent-activity',
      user: user && user.profile,
      showActionButtons: true
    };

    $(displayOptions.container).empty();

    Webfield.ui.activityList(activityNotes, displayOptions);

    $('.tabs-container a[href="#recent-activity"]').parent().show();
  } else {
    $('.tabs-container a[href="#recent-activity"]').parent().hide();
  }
  
  var roundedDeskRejectedNotes = !deskRejectedNotes.count ? 0 : (deskRejectedNotes.count + (PAGE_SIZE - (deskRejectedNotes.count % PAGE_SIZE)))
  
  var removedNotesCount = roundedDeskRejectedNotes + (withdrawnNotes.count || 0);
  if (removedNotesCount) {
    $('#desk-rejected-withdrawn-submissions').empty();

    var removedNotesArray = _.concat(deskRejectedNotes.notes || [], withdrawnNotes.notes || []);
    Webfield.ui.submissionList(removedNotesArray, {
      heading: null,
      container: '#desk-rejected-withdrawn-submissions',
      search: {
        enabled: false
      },
      displayOptions: paperDisplayOptions,
      autoLoad: false,
      noteCount: removedNotesCount,
      pageSize: PAGE_SIZE,
      onPageClick: function(offset) {
        if (offset < deskRejectedNotes.count) {
          return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset
          });
        } else {
          return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
            details: 'replyCount,invitation,original',
            pageSize: PAGE_SIZE,
            offset: offset - roundedDeskRejectedNotes
          });
        }
      },
      fadeIn: false
    });
  } else {
    $('.tabs-container a[href="#desk-rejected-withdrawn-submissions"]').parent().hide();
  }

  // var withdrawnNotesCount = withdrawnNotes.count || 0;
  // if (withdrawnNotesCount) {
  //   $('#withdrawn-submissions').empty();

  //   var withdrawnNotesArray = withdrawnNotes.notes || [];
  //   Webfield.ui.submissionList(withdrawnNotesArray, {
  //     heading: null,
  //     container: '#withdrawn-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: withdrawnNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(WITHDRAWN_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#withdrawn-submissions"]').parent().hide();
  // }

  // var deskRejectedNotesCount = deskRejectedNotes.count || 0;
  // if (deskRejectedNotesCount) {
  //   $('#desk-rejected-submissions').empty();

  //   var deskRejectedNotesArray = deskRejectedNotes.notes || [];
  //   Webfield.ui.submissionList(deskRejectedNotesArray, {
  //     heading: null,
  //     container: '#desk-rejected-submissions',
  //     search: {
  //       enabled: false
  //     },
  //     displayOptions: paperDisplayOptions,
  //     autoLoad: false,
  //     noteCount: deskRejectedNotesCount,
  //     pageSize: PAGE_SIZE,
  //     onPageClick: function(offset) {
  //       return Webfield.api.getSubmissions(DESK_REJECTED_SUBMISSION_ID, {
  //         details: 'replyCount,invitation,original',
  //         pageSize: PAGE_SIZE,
  //         offset: offset
  //       });
  //     },
  //     fadeIn: false
  //   });
  // } else {
  //   $('.tabs-container a[href="#desk-rejected-submissions"]').parent().hide();
  // }

  $('#notes .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();
// END GROUP CODE
});
//# sourceURL=webfieldCode.js</script><script src="./ICLR2022_poster_18_files/index-55b7d6149a41ddec.js.download"></script><script src="./ICLR2022_poster_18_files/about-77a61c48f23a1315.js.download"></script><script src="./ICLR2022_poster_18_files/venues-458c67c21955226a.js.download"></script><script src="./ICLR2022_poster_18_files/contact-2c775b351d0a5421.js.download"></script><script src="./ICLR2022_poster_18_files/sponsors-987fb223230996d5.js.download"></script><script src="./ICLR2022_poster_18_files/terms-e2f16e0dce69665f.js.download"></script><script src="./ICLR2022_poster_18_files/privacy-276f217f8a2a3840.js.download"></script><script src="./ICLR2022_poster_18_files/login-6d19e702f9bd1dcc.js.download"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>